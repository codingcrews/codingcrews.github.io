<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>코딩크루</title>
    <link>https://codingcrews.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>코딩이 어렵나요? 코딩크루와 함께 경험을 쌓아보세요.</description>
    <pubDate>Fri, 01 Feb 2019 23:14:14 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>딥러닝으로 개와 고양이를 분류해보자 - 전이학습과 미세조정</title>
      <link>https://codingcrews.github.io/2019/01/22/transfer-learning-and-find-tuning/</link>
      <guid>https://codingcrews.github.io/2019/01/22/transfer-learning-and-find-tuning/</guid>
      <pubDate>Tue, 22 Jan 2019 09:29:03 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Transfer-Learning-amp-Find-Tuning&quot;&gt;&lt;a href=&quot;#Transfer-Learning-amp-Find-Tuning&quot; class=&quot;headerlink&quot; title=&quot;Transfer Learning &amp;amp; Fi
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Transfer-Learning-amp-Find-Tuning"><a href="#Transfer-Learning-amp-Find-Tuning" class="headerlink" title="Transfer Learning &amp; Find Tuning"></a>Transfer Learning &amp; Find Tuning</h1><p>안녕하세요.<br>이번 시간은 학습된 모델을 우리의 도메인에 맞춰 사용할 수 있도록 변경하는법을 알아봅시다.<br>저번 포스팅에서 개와 고양이를 분류하는 CNN 모델을 만들어보고 학습을 진행해봤는데요.<br>ResNet50 모델의 경우 모델이 무거운데다 저희의 학습 데이터량도 많지 않고(증식을 이용하긴 했으나, 과적합될 가능성이 존재),<br>학습시간도 모자랐었습니다. (32에폭 도는데 대략 40분정도의 시간이 소요됨. 대회에 나간 모델의 경우 2~3주동안 학습을 진행했다고 함.)  </p><p>그럼 매번 모델을 만들때마다 새로 학습을 진행해야 할까요 ?<br>개와 고양이의 분류 모델을 만드는데 2주,<br>개, 고양이와 거북이를 분류하는 모델을 다시 만드는데 2주… 이렇게 시간이 소요될 수 있습니다.  </p><p>저희가 사용한 모델들은 CNN 레이어는 이미지의 특징들을 뽑아내주고,<br>마지막 Fully Connected 레이어에서 그 해당 특징들을 기반으로 분류를 하는 형태를 띄는데요.<br>그럼 기존에 학습된 모델을 가지고 특징을 추출하여 FC레이어만 새로 재학습을 할수는 없을까요 ?  </p><p>오늘은 InceptionV3 모델을 이용하여 Feature Extraction, Transfer Learning을 진행해보도록 하겠습니다.<br>(이번에 사용하는 InceptionV3 모델은 마지막 레이어가 FC 레이어가 아닌 GAP를 통해 분류를 하고 있지만, Transfer learning의 기본적인 부분은 동일합니다.)</p><h1 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h1><p>어파인 레이어(Fully Connected Layer)를 거치기 바로 직전의 추상화된 레이어의 특징들을 보틀넥 피쳐라 말합니다.<br>저희는 기존 imagenet 데이터셋을 기반으로 학습된 모델을 사용하여, 이미지의 특징을 추출하고,<br>이 특징을 기반으로 FC레이어를 분류시킬 수 있는 모델을 만들어 보도록 하겠습니다.<br>이 과정을 <strong>Feature Extraction</strong> 혹은 <strong>Transfer Learning</strong>이라고 말하기도 합니다.  </p><p>이 예제는 마지막 분류 레이어만 학습을 진행해도 되기 때문에 CPU로도 빠른 속도로 학습이 가능합니다.<br>이 뒤의 Fine Tuning시에는 GPU가 아닐경우 많은 시간이 소요되니 GPU 환경을 권장합니다.<br><del>(피쳐 추출 부분에서 시간이 많이 소요되는건 비밀..)</del>  </p><h2 id="데이터셋-준비"><a href="#데이터셋-준비" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h2><p>데이터셋은 이전 포스팅에서 이용한 개, 고양이 데이터셋을 이용합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위노그라드 알고리즘 설정 (GPU 사용시 conv 연산이 빨라짐)</span></span><br><span class="line">os.environ[<span class="string">'TF_ENABLE_WINOGRAD_NONFUSED'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">rootPath = <span class="string">'./datasets/cat-and-dog'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미지의 사이즈는 논문에서는 (224, 224, 3)을 사용하여, 빠른 학습을 위해 사이즈를 조정</span></span><br><span class="line">IMG_SIZE = (<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)  </span><br><span class="line"></span><br><span class="line">imageGenerator = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">20</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    brightness_range=[<span class="number">.2</span>, <span class="number">.2</span>],</span><br><span class="line">    horizontal_flip=<span class="keyword">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=IMG_SIZE[:<span class="number">2</span>],</span><br><span class="line">    batch_size=<span class="number">20</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">testGen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">).flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'test_set'</span>),</span><br><span class="line">    target_size=IMG_SIZE[:<span class="number">2</span>],</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Found 8005 images belonging to 2 classes.Found 2023 images belonging to 2 classes.</code></pre><h2 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h2><p>모델은 피쳐추출기로 사용할 InceptionV3 모델과 이를 분류할 분류기 두개를 만들겠습니다.<br>이미지의 특징을 추출해주니 꼭 신경망이 아니더라도 다른 분류기를(예를 들면 SVM 등) 사용하셔도 무방합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications <span class="keyword">import</span> InceptionV3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fc 레이어를 포함하지 않고, imagenet기반으로 학습된 가중치를 로드한 뒤 GAP 레이어를 추가</span></span><br><span class="line">extractor = Sequential()</span><br><span class="line">extractor.add(InceptionV3(include_top=<span class="keyword">False</span>, weights=<span class="string">'imagenet'</span>, input_shape=IMG_SIZE))</span><br><span class="line">extractor.add(layers.GlobalAveragePooling2D())</span><br><span class="line"></span><br><span class="line">extractor_output_shape = extractor.get_output_shape_at(<span class="number">0</span>)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.InputLayer(input_shape=extractor_output_shape))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense (Dense)                (None, 2)                 4098      =================================================================Total params: 4,098Trainable params: 4,098Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line">optimizer = Adam(lr=<span class="number">0.001</span>)</span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>, </span><br><span class="line">    metrics=[<span class="string">'acc'</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">extractor.compile(</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>, </span><br><span class="line">    metrics=[<span class="string">'acc'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="특징-추출기-생성"><a href="#특징-추출기-생성" class="headerlink" title="특징 추출기 생성"></a>특징 추출기 생성</h2><p>이미지 제너레이터에서 특징을 추출하기 위한 펑션을 하나 만들어 줍니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_features</span><span class="params">(extractor, gen, cnt)</span>:</span></span><br><span class="line">    bs = gen.batch_size</span><br><span class="line">    ds = cnt</span><br><span class="line">    extractor_shapes = list(extractor.get_output_shape_at(<span class="number">0</span>)[<span class="number">1</span>:])</span><br><span class="line">    </span><br><span class="line">    features = np.empty([<span class="number">0</span>] + extractor_shapes)</span><br><span class="line">    labels = np.empty((<span class="number">0</span>, <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, (trainX, trainY) <span class="keyword">in</span> enumerate(gen):</span><br><span class="line">        features = np.append(features, extractor.predict(trainX), axis=<span class="number">0</span>)</span><br><span class="line">        labels = np.append(labels, trainY, axis=<span class="number">0</span>)</span><br><span class="line">        print(<span class="string">'batch index: &#123;&#125;/&#123;&#125;'</span>.format(i * bs, ds), end=<span class="string">'\r'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> bs * i &gt;= cnt:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure><h3 id="특징추출"><a href="#특징추출" class="headerlink" title="특징추출"></a>특징추출</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainX, trainY = get_features(extractor, trainGen, <span class="number">3000</span>)</span><br><span class="line">testX, testY = get_features(extractor, testGen, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><pre><code>batch index: 3000/3000batch index: 1000/1000</code></pre><h2 id="학습-시작"><a href="#학습-시작" class="headerlink" title="학습 시작"></a>학습 시작</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">    trainX, </span><br><span class="line">    trainY,</span><br><span class="line">    epochs=epochs,</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 2718 samples, validate on 302 samplesEpoch 1/322718/2718 [==============================] - 2s 575us/step - loss: 0.3848 - acc: 0.8214 - val_loss: 0.2531 - val_acc: 0.8791Epoch 2/322718/2718 [==============================] - 0s 95us/step - loss: 0.2236 - acc: 0.9031 - val_loss: 0.2380 - val_acc: 0.8891Epoch 3/322718/2718 [==============================] - 0s 88us/step - loss: 0.1946 - acc: 0.9187 - val_loss: 0.2312 - val_acc: 0.8891Epoch 4/322718/2718 [==============================] - 0s 88us/step - loss: 0.1733 - acc: 0.9284 - val_loss: 0.2322 - val_acc: 0.8907Epoch 5/322718/2718 [==============================] - 0s 90us/step - loss: 0.1541 - acc: 0.9391 - val_loss: 0.2334 - val_acc: 0.8940Epoch 6/322718/2718 [==============================] - 0s 93us/step - loss: 0.1441 - acc: 0.9430 - val_loss: 0.2444 - val_acc: 0.8957Epoch 7/322718/2718 [==============================] - 0s 86us/step - loss: 0.1337 - acc: 0.9513 - val_loss: 0.2425 - val_acc: 0.8825Epoch 8/322718/2718 [==============================] - 0s 88us/step - loss: 0.1295 - acc: 0.9507 - val_loss: 0.2432 - val_acc: 0.8940Epoch 9/322718/2718 [==============================] - 0s 87us/step - loss: 0.1211 - acc: 0.9533 - val_loss: 0.2524 - val_acc: 0.8974Epoch 10/322718/2718 [==============================] - 0s 90us/step - loss: 0.1143 - acc: 0.9573 - val_loss: 0.2539 - val_acc: 0.8924Epoch 11/322718/2718 [==============================] - 0s 89us/step - loss: 0.1123 - acc: 0.9562 - val_loss: 0.2560 - val_acc: 0.8874Epoch 12/322718/2718 [==============================] - 0s 85us/step - loss: 0.1009 - acc: 0.9639 - val_loss: 0.2659 - val_acc: 0.8940Epoch 13/322718/2718 [==============================] - 0s 85us/step - loss: 0.0955 - acc: 0.9682 - val_loss: 0.2550 - val_acc: 0.8924Epoch 14/322718/2718 [==============================] - 0s 91us/step - loss: 0.0901 - acc: 0.9673 - val_loss: 0.2738 - val_acc: 0.8940Epoch 15/322718/2718 [==============================] - 0s 90us/step - loss: 0.0897 - acc: 0.9678 - val_loss: 0.2909 - val_acc: 0.8907Epoch 16/322718/2718 [==============================] - 0s 87us/step - loss: 0.0843 - acc: 0.9722 - val_loss: 0.2811 - val_acc: 0.8891Epoch 17/322718/2718 [==============================] - 0s 87us/step - loss: 0.0812 - acc: 0.9731 - val_loss: 0.2788 - val_acc: 0.8924Epoch 18/322718/2718 [==============================] - 0s 90us/step - loss: 0.0800 - acc: 0.9737 - val_loss: 0.3198 - val_acc: 0.8891Epoch 19/322718/2718 [==============================] - 0s 89us/step - loss: 0.0746 - acc: 0.9748 - val_loss: 0.2781 - val_acc: 0.8940Epoch 20/322718/2718 [==============================] - 0s 85us/step - loss: 0.0759 - acc: 0.9748 - val_loss: 0.2898 - val_acc: 0.8924Epoch 21/322718/2718 [==============================] - 0s 87us/step - loss: 0.0704 - acc: 0.9774 - val_loss: 0.2810 - val_acc: 0.8990Epoch 22/322718/2718 [==============================] - 0s 88us/step - loss: 0.0660 - acc: 0.9792 - val_loss: 0.2831 - val_acc: 0.8940Epoch 23/322718/2718 [==============================] - 0s 88us/step - loss: 0.0631 - acc: 0.9825 - val_loss: 0.2886 - val_acc: 0.8974Epoch 24/322718/2718 [==============================] - 0s 89us/step - loss: 0.0611 - acc: 0.9827 - val_loss: 0.2841 - val_acc: 0.8990Epoch 25/322718/2718 [==============================] - 0s 89us/step - loss: 0.0599 - acc: 0.9829 - val_loss: 0.2893 - val_acc: 0.8974Epoch 26/322718/2718 [==============================] - 0s 89us/step - loss: 0.0575 - acc: 0.9845 - val_loss: 0.2965 - val_acc: 0.8990Epoch 27/322718/2718 [==============================] - 0s 89us/step - loss: 0.0556 - acc: 0.9875 - val_loss: 0.2971 - val_acc: 0.9007Epoch 28/322718/2718 [==============================] - 0s 90us/step - loss: 0.0531 - acc: 0.9860 - val_loss: 0.3044 - val_acc: 0.9007Epoch 29/322718/2718 [==============================] - 0s 88us/step - loss: 0.0512 - acc: 0.9875 - val_loss: 0.3063 - val_acc: 0.8974Epoch 30/322718/2718 [==============================] - 0s 87us/step - loss: 0.0498 - acc: 0.9893 - val_loss: 0.3097 - val_acc: 0.8990Epoch 31/322718/2718 [==============================] - 0s 91us/step - loss: 0.0499 - acc: 0.9880 - val_loss: 0.3155 - val_acc: 0.8924Epoch 32/322718/2718 [==============================] - 0s 94us/step - loss: 0.0463 - acc: 0.9904 - val_loss: 0.3197 - val_acc: 0.8957</code></pre><h2 id="결과-시각화"><a href="#결과-시각화" class="headerlink" title="결과 시각화"></a>결과 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history_dict)</span>:</span></span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">.8</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="transfer-learning-and-find-tuning_16_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">smooth_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key, val <span class="keyword">in</span> history.history.items():</span><br><span class="line">    smooth_data[key] = smooth_curve(val[:])</span><br><span class="line">show_graph(smooth_data)</span><br></pre></td></tr></table></figure><p><img src="transfer-learning-and-find-tuning_17_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(testX, testY)</span><br></pre></td></tr></table></figure><pre><code>1020/1020 [==============================] - 0s 59us/step[0.16686982696547228, 0.9480392149850434]</code></pre><p>imagenet으로 학습한 모델을 이용했을 때 이미지의 분류를 위한 특징을 잘 뽑아내도록 필터가 학습되어 있으므로,<br>첫 에폭에서부터 높은 결과를 보여주는걸 확인할 수 있습니다.  </p><h1 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h1><p>Find Tuning이란 미세조정인데요.<br>아무래도 기존 학습된 네트워크는 학습한 데이터셋에 잘맞도록 학습이 되어 있겠죠.<br>그렇다고 우리 데이터셋으로 밑바닥부터 학습시키기엔 데이터를 모으기가 힘든 경우가 대다수입니다.<br>이런 경우에 학습된 네트워크를 미세조정하여 우리의 데이터셋에 조금 더 잘 맞도록 모델의 파라메터를 조정하는 과정을 거치는데요.<br>이 과정을 Fine Tuning이라고 합니다.  </p><p>하지만 연산량이 많아지기 때문에 위의 Transfer Learning보다 상대적으로 시간이 조금 더 필요하게 됩니다.<br>Fine Tuning은 CPU로 하기엔 연산량이 많아 <strong>GPU 사용을 권장</strong>합니다.</p><p><strong>미세조정의 순서는 대략 아래와 같습니다.</strong></p><ol><li>분류기를 학습한다. (이때 분류기를 제외한 모델의 모든 레이어는 가중치를 업데이트 되지 않도록 <strong>동결(Freeze)</strong> 시킵니다)  </li><li>모델의 마지막 레이어들을 미세조정을 실시한다.  </li></ol><p>이렇게 먼저 분류기를 학습하는 이유는 저희가 추가한 분류기의 가중치는 랜덤하게 초기화가 되어 있습니다.<br>이러한 분류기를 통해 그대로 보틀넥 레이어를 재학습 할 경우, Loss가 높아 기존의 레이어가 망가질 우려가 있습니다.<br>기존 학습된 모델의 가중치를 최대한 미세하게 조정하기 위하여 분류기를 선학습 한 이후, 보틀넥 레이어를 다시 재학습 하도록 합니다.</p><p>가장 중요한 부분은 모델의 가중치 동결인데요.<br>기존 학습된 모델의 경우 앞의 레이어들은 이미지를 직관적으로 이해할 수 있도록 이미지 자체의 엣지 디텍터등의 역할을 수행합니다.<br>레이어가 뒤로 향할수록 필터에 대한 데이터를 추상적으로 표현하게 되는데, 앞단의 레이어들의 가중치를 동결함으로써 학습 시 필터의 역할이 망가지지 않도록 하는 역할을 합니다.</p><h2 id="데이터셋-준비-1"><a href="#데이터셋-준비-1" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h2><p>위에서 사용한 제너레이터를 조금 변경하여 사용하도록 하겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">rootPath = <span class="string">'./datasets/cat-and-dog'</span></span><br><span class="line"></span><br><span class="line">IMG_SIZE = (<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)  </span><br><span class="line">imageGenerator = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">20</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    brightness_range=[<span class="number">.2</span>, <span class="number">.2</span>],</span><br><span class="line">    horizontal_flip=<span class="keyword">True</span>,</span><br><span class="line">    validation_split=<span class="number">0.1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=IMG_SIZE[:<span class="number">2</span>],</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    subset=<span class="string">'training'</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">validationGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=IMG_SIZE[:<span class="number">2</span>],</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    subset=<span class="string">'validation'</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">testGen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">).flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'test_set'</span>),</span><br><span class="line">    target_size=IMG_SIZE[:<span class="number">2</span>],</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Found 7205 images belonging to 2 classes.Found 800 images belonging to 2 classes.Found 2023 images belonging to 2 classes.</code></pre><h2 id="모델-구성-1"><a href="#모델-구성-1" class="headerlink" title="모델 구성"></a>모델 구성</h2><p>위에서 사용한 InceptionV3 모델을 이용해도 되지만, 저희는 새로운 모델을 만들어 진행해보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">inception = InceptionV3(include_top=<span class="keyword">False</span>, weights=<span class="string">'imagenet'</span>, input_shape=IMG_SIZE)</span><br><span class="line">gap = layers.GlobalAveragePooling2D()(inception.output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 마지막 31번째 뒤 레이어들을 제외한 모든 레이어를 가중치 동결합니다.</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> inception.layers[:<span class="number">-31</span>]:</span><br><span class="line">    l.trainable = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 위의 Transfer Learning에서 훈련시킨 분류기를 이용합니다.</span></span><br><span class="line">classifier = model</span><br><span class="line"></span><br><span class="line">model = Model(inception.input, classifier(gap))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_3 (InputLayer)            (None, 150, 150, 3)  0                                            __________________________________________________________________________________________________conv2d_94 (Conv2D)              (None, 74, 74, 32)   864         input_3[0][0]                    __________________________________________________________________________________________________batch_normalization_94 (BatchNo (None, 74, 74, 32)   96          conv2d_94[0][0]                  __________________________________________________________________________________________________activation_94 (Activation)      (None, 74, 74, 32)   0           batch_normalization_94[0][0]     __________________________________________________________________________________________________conv2d_95 (Conv2D)              (None, 72, 72, 32)   9216        activation_94[0][0]                                                    중간 생략 ...__________________________________________________________________________________________________concatenate_3 (Concatenate)     (None, 3, 3, 768)    0           activation_185[0][0]                                                                              activation_186[0][0]             __________________________________________________________________________________________________activation_187 (Activation)     (None, 3, 3, 192)    0           batch_normalization_187[0][0]    __________________________________________________________________________________________________mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_179[0][0]                                                                              mixed9_1[0][0]                                                                                    concatenate_3[0][0]                                                                               activation_187[0][0]             __________________________________________________________________________________________________global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    __________________________________________________________________________________________________sequential_1 (Sequential)       (None, 2)            4098        global_average_pooling2d_1[0][0] ==================================================================================================Total params: 21,806,882Trainable params: 6,077,634Non-trainable params: 15,729,248__________________________________________________________________________________________________</code></pre><p>모델의 구성을 잘 보시면 총 파라메터의 개수는 약 2,200만개의 파라메터를 가지고 있지만,<br>훈련이 가능한 파라메터의 수는 약 600만개 입니다.<br>나머지 1,500만개의 파라메터는 저희가 설정한대로 가중치 업데이트가 동결 되었습니다.</p><h2 id="모델-컴파일-1"><a href="#모델-컴파일-1" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>모델의 미세조정을 위해 러닝레이트를 줄여 학습을 진행합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer = Adam(lr=<span class="number">0.0001</span>)</span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>, </span><br><span class="line">    metrics=[<span class="string">'acc'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="학습-시작-1"><a href="#학습-시작-1" class="headerlink" title="학습 시작"></a>학습 시작</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">32</span></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    generator=trainGen,</span><br><span class="line">    epochs=epochs,</span><br><span class="line">    steps_per_epoch=trainGen.n / trainGen.batch_size,</span><br><span class="line">    validation_data=validationGen,</span><br><span class="line">    validation_steps=validationGen.n / validationGen.batch_size,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/32226/225 [==============================] - 57s 253ms/step - loss: 0.2900 - acc: 0.8724 - val_loss: 0.3690 - val_acc: 0.9169Epoch 2/32226/225 [==============================] - 55s 245ms/step - loss: 0.2683 - acc: 0.8868 - val_loss: 0.4129 - val_acc: 0.9081Epoch 3/32226/225 [==============================] - 56s 246ms/step - loss: 0.2487 - acc: 0.8915 - val_loss: 0.4223 - val_acc: 0.9081Epoch 4/32226/225 [==============================] - 56s 246ms/step - loss: 0.2156 - acc: 0.9079 - val_loss: 0.4608 - val_acc: 0.9131Epoch 5/32226/225 [==============================] - 55s 244ms/step - loss: 0.2214 - acc: 0.9061 - val_loss: 0.3838 - val_acc: 0.9144Epoch 6/32226/225 [==============================] - 55s 243ms/step - loss: 0.1986 - acc: 0.9154 - val_loss: 0.4782 - val_acc: 0.9019Epoch 7/32226/225 [==============================] - 55s 245ms/step - loss: 0.1875 - acc: 0.9258 - val_loss: 0.3910 - val_acc: 0.9313Epoch 8/32226/225 [==============================] - 55s 243ms/step - loss: 0.1663 - acc: 0.9322 - val_loss: 0.4091 - val_acc: 0.9250Epoch 9/32226/225 [==============================] - 55s 241ms/step - loss: 0.1813 - acc: 0.9256 - val_loss: 0.5089 - val_acc: 0.9163Epoch 10/32226/225 [==============================] - 55s 244ms/step - loss: 0.1742 - acc: 0.9275 - val_loss: 0.4822 - val_acc: 0.9094Epoch 11/32226/225 [==============================] - 55s 242ms/step - loss: 0.1580 - acc: 0.9341 - val_loss: 0.4449 - val_acc: 0.9137Epoch 12/32226/225 [==============================] - 56s 247ms/step - loss: 0.1556 - acc: 0.9375 - val_loss: 0.5008 - val_acc: 0.9163Epoch 13/32226/225 [==============================] - 55s 243ms/step - loss: 0.1676 - acc: 0.9314 - val_loss: 0.4137 - val_acc: 0.9087Epoch 14/32226/225 [==============================] - 55s 244ms/step - loss: 0.1466 - acc: 0.9375 - val_loss: 0.6155 - val_acc: 0.8925Epoch 15/32226/225 [==============================] - 55s 244ms/step - loss: 0.1364 - acc: 0.9459 - val_loss: 0.5151 - val_acc: 0.9125Epoch 16/32226/225 [==============================] - 55s 243ms/step - loss: 0.1325 - acc: 0.9458 - val_loss: 0.5785 - val_acc: 0.9025Epoch 17/32226/225 [==============================] - 56s 246ms/step - loss: 0.1303 - acc: 0.9496 - val_loss: 0.5260 - val_acc: 0.9081Epoch 18/32226/225 [==============================] - 55s 242ms/step - loss: 0.1337 - acc: 0.9463 - val_loss: 0.5409 - val_acc: 0.9106Epoch 19/32226/225 [==============================] - 56s 247ms/step - loss: 0.1189 - acc: 0.9550 - val_loss: 0.4270 - val_acc: 0.9181Epoch 20/32226/225 [==============================] - 55s 245ms/step - loss: 0.1166 - acc: 0.9532 - val_loss: 0.5129 - val_acc: 0.9250Epoch 21/32226/225 [==============================] - 55s 244ms/step - loss: 0.1117 - acc: 0.9556 - val_loss: 0.7072 - val_acc: 0.8981Epoch 22/32226/225 [==============================] - 56s 247ms/step - loss: 0.1155 - acc: 0.9562 - val_loss: 0.4914 - val_acc: 0.9187Epoch 23/32226/225 [==============================] - 56s 247ms/step - loss: 0.1055 - acc: 0.9602 - val_loss: 0.6032 - val_acc: 0.9006Epoch 24/32226/225 [==============================] - 55s 246ms/step - loss: 0.1140 - acc: 0.9549 - val_loss: 0.4621 - val_acc: 0.9044Epoch 25/32226/225 [==============================] - 55s 244ms/step - loss: 0.1158 - acc: 0.9532 - val_loss: 0.5440 - val_acc: 0.9144Epoch 26/32226/225 [==============================] - 55s 241ms/step - loss: 0.1042 - acc: 0.9628 - val_loss: 0.5088 - val_acc: 0.9100Epoch 27/32226/225 [==============================] - 55s 245ms/step - loss: 0.0982 - acc: 0.9605 - val_loss: 0.5012 - val_acc: 0.9200Epoch 28/32226/225 [==============================] - 55s 242ms/step - loss: 0.1063 - acc: 0.9577 - val_loss: 0.5262 - val_acc: 0.9056Epoch 29/32226/225 [==============================] - 55s 241ms/step - loss: 0.0933 - acc: 0.9637 - val_loss: 0.4498 - val_acc: 0.9294Epoch 30/32226/225 [==============================] - 54s 241ms/step - loss: 0.0916 - acc: 0.9658 - val_loss: 0.5562 - val_acc: 0.9044Epoch 31/32226/225 [==============================] - 55s 244ms/step - loss: 0.0870 - acc: 0.9666 - val_loss: 0.4454 - val_acc: 0.9169Epoch 32/32226/225 [==============================] - 55s 245ms/step - loss: 0.0857 - acc: 0.9660 - val_loss: 0.5616 - val_acc: 0.9119</code></pre><h2 id="모델-평가-및-시각화"><a href="#모델-평가-및-시각화" class="headerlink" title="모델 평가 및 시각화"></a>모델 평가 및 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate_generator(testGen)</span><br></pre></td></tr></table></figure><pre><code>[0.2855920347539756, 0.95551161632288]</code></pre><p>테스트셋에 위의 분류기보다 조금 더 높은 정확도를 확인할 수 있습니다.<br>정학도가 높아질수록 정확도 1% 올리는게 더더욱 힘들어지죠</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="transfer-learning-and-find-tuning_33_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">smooth_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key, val <span class="keyword">in</span> history.history.items():</span><br><span class="line">    smooth_data[key] = smooth_curve(val[:])</span><br><span class="line">show_graph(smooth_data)</span><br></pre></td></tr></table></figure><p><img src="transfer-learning-and-find-tuning_34_0.png" alt="png"></p><p>5~10에폭에서부터 과대적합이 시작되는것처럼 보이는데요.<br>이 예제에서는 분류기를 이전에 학습한걸 그대로 사용하였으나, 여러가지 방법으로 테스트를 진행해보세요.</p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/22/transfer-learning-and-find-tuning/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝으로 개와 고양이를 분류하는 모델을 만들어보자 - ResNet50</title>
      <link>https://codingcrews.github.io/2019/01/19/cat-dog-resnet/</link>
      <guid>https://codingcrews.github.io/2019/01/19/cat-dog-resnet/</guid>
      <pubDate>Fri, 18 Jan 2019 18:21:43 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;사진으로-개-고양이-분류하기-Ver-ResNet&quot;&gt;&lt;a href=&quot;#사진으로-개-고양이-분류하기-Ver-ResNet&quot; class=&quot;headerlink&quot; title=&quot;사진으로 개 고양이 분류하기 - Ver. ResNet&quot;&gt;&lt;/a&gt;사진으로 
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="사진으로-개-고양이-분류하기-Ver-ResNet"><a href="#사진으로-개-고양이-분류하기-Ver-ResNet" class="headerlink" title="사진으로 개 고양이 분류하기 - Ver. ResNet"></a>사진으로 개 고양이 분류하기 - Ver. ResNet</h1><p>안녕하세요.<br>저번 포스팅에서는 일반적인 CNN을 이용하여 네트워크를 구성하고, 개와 고양이 사진을 분류시키는 모델을 만들어 봤는데요.<br>이번 시간에는 대회에서 사람보다 적은 에러율을 보였던 ResNet을 이용하여 모델을 만들어 보겠습니다.  </p><p>토이 프로젝트의 튜토리얼이지만 모델 자체가 굉장히 무겁다보니 CPU로는 극악의 시간이 소요됩니다.<br>꼭 GPU 환경을 구축하시고 진행하세요.  </p><p>데스크톱으로 GPU 학습환경 구성이 어려우시다면, <a href="/2019/01/15/deeplearning-gpu/">AWS로 GPU기반의 딥러닝 학습환경 구축하기</a> 포스팅을 참고하세요.  </p><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>ResNet 모델을 사용하기 전에 간단하게만 알아봅시다.<br>참고한 논문은 <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">여기</a>에서 확인하실 수 있고, 가장 흥미로운 부분은 Residual Network 입니다.  </p><p>기존의 CNN 학습 방법으로 VGG의 구현을 살짝 보겠습니다.<br><img src="residual.png" alt="VGG-19"><br>위 이미지에서 보면 좌측의 네트워크는 저희가 지금까지 만든 각 레이어를 쭉 일자로 연결한 형태이고,<br>우측의 Residual을 보면 이전 레이어에서 나온 데이터를 한단계를 건너뛰어 재사용하는걸 볼 수 있습니다.<br>좌측의 평평한 레이어에서 하나의 지름길을 만들어 한단계를 넘어 다음 레이어에 연결시킨 부분이 인상적입니다.<br>이러한 부분으로 인해 VGG모델에 비해 깊은 레이어를 가지고 있지만, 실제 사용시에는 굉장히 빠른 속도를 보여줍니다.<br>다만 학습시간이 오래걸리는건 당연하겠죠 ?</p><p>어떤 강의에서는 이 Residual network를 Fast network라고 표현한답니다.  </p><p>좀 더 자세한 설명은 해당 논문을 참고하시기 바랍니다.</p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>데이터셋은 이전 포스팅과 마찬가지로 동일한 데이터셋인 kaggle에서 제공하는 데이터셋을 이용할 예정입니다.  </p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><p>직접 데이터셋을 배포하진 않고 있습니다.<br>데이터셋은 <a href="https://www.kaggle.com/tongpython/cat-and-dog" target="_blank" rel="noopener">여기</a>에서 직접 다운로드 받으실 수 있고, kaggle api를 통하여 받으실 수 있습니다.  </p><p>저희는 저번 포스팅과 동일하게 datasets이란 폴더에 데이터셋을 넣어두고 사용하도록 하겠습니다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">datasets</span><br><span class="line">└── cat-and-dog</span><br><span class="line">    ├── test_set</span><br><span class="line">    │   ├── cats</span><br><span class="line">    │   │   └── datas...</span><br><span class="line">    │   └── dogs</span><br><span class="line">    │       └── datas...</span><br><span class="line">    └── training_set</span><br><span class="line">        ├── cats</span><br><span class="line">        │   └── datas...</span><br><span class="line">        └── dogs</span><br><span class="line">            └── datas...</span><br></pre></td></tr></table></figure><p>이와 같은 형태의 구성이 되어 있다고 가정합니다.</p><h2 id="데이터-로드"><a href="#데이터-로드" class="headerlink" title="데이터 로드"></a>데이터 로드</h2><p>마찬가지로 저번 포스팅과 동일하게 데이터 증식을 위한 제너레이터로 데이터를 읽어옵니다.<br>관련된 설명은 이전 포스팅을 참고해주세요.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위노그라드 알고리즘 설정</span></span><br><span class="line">os.environ[<span class="string">'TF_ENABLE_WINOGRAD_NONFUSED'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">rootPath = <span class="string">'./datasets/cat-and-dog'</span></span><br><span class="line"></span><br><span class="line">imageGenerator = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">20</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    brightness_range=[<span class="number">.2</span>, <span class="number">.2</span>],</span><br><span class="line">    horizontal_flip=<span class="keyword">True</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    subset=<span class="string">'training'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">validationGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    subset=<span class="string">'validation'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Found 7205 images belonging to 2 classes.Found 800 images belonging to 2 classes.</code></pre><h1 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h1><p>텐서플로 케라스에서 ResNet 모델을 제공해주고 있습니다.<br>저희는 해당 모델의 구성을 그대로 가져다 사용하도록 하겠습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.applications <span class="keyword">import</span> ResNet50</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(ResNet50(include_top=<span class="keyword">True</span>, weights=<span class="keyword">None</span>, input_shape=(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================resnet50 (Model)             (None, 2)                 23591810  =================================================================Total params: 23,591,810Trainable params: 23,538,690Non-trainable params: 53,120_________________________________________________________________</code></pre><p><strong>include_top</strong><br>    모델의 최상위 Fully Connected 레이어를 추가할것인지 추가하지 않을것인지에 대한 토글입니다.  </p><p><strong>weights</strong><br>    기존 학습된 가중치 데이터를 읽어들일 가중치 경로입니다. imagenet이라 입력하면 imagenet 데이터셋을 기반으로 기존 학습된 가중치 데이터를 불러옵니다.  </p><p><strong>input_shape</strong><br>    입력값의 형태를 지정합니다. 3D 텐서를 입력해야합니다.  </p><p><strong>classes</strong><br>    모델의 마지막 출력 차원을 지정합니다.  </p><p>저희는 ResNet50 모델을 이용하여 개와 고양이를 학습하는 모델을 학습시켜보도록 하겠습니다.  </p><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>모델의 구성이 완료된 이후에는 항상 컴파일이 필요합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>, </span><br><span class="line">    metrics=[<span class="string">'acc'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="학습-시작"><a href="#학습-시작" class="headerlink" title="학습 시작"></a>학습 시작</h2><p>ResNet 모델의 학습이 시작됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">32</span></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    trainGen, </span><br><span class="line">    epochs=epochs,</span><br><span class="line">    steps_per_epoch=trainGen.samples / epochs, </span><br><span class="line">    validation_data=validationGen,</span><br><span class="line">    validation_steps=trainGen.samples / epochs,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/32226/225 [==============================] - 100s 441ms/step - loss: 1.0603 - acc: 0.5530 - val_loss: 0.9258 - val_acc: 0.5022Epoch 2/32226/225 [==============================] - 80s 355ms/step - loss: 0.9658 - acc: 0.6102 - val_loss: 0.8094 - val_acc: 0.5333Epoch 3/32226/225 [==============================] - 80s 354ms/step - loss: 0.9366 - acc: 0.6113 - val_loss: 4.5736 - val_acc: 0.5488Epoch 4/32226/225 [==============================] - 80s 354ms/step - loss: 0.9635 - acc: 0.5666 - val_loss: 4.1998 - val_acc: 0.5292Epoch 5/32226/225 [==============================] - 80s 354ms/step - loss: 0.8955 - acc: 0.6076 - val_loss: 2.3943 - val_acc: 0.5353Epoch 6/32226/225 [==============================] - 80s 353ms/step - loss: 0.8526 - acc: 0.6103 - val_loss: 1.5856 - val_acc: 0.6340Epoch 7/32226/225 [==============================] - 80s 354ms/step - loss: 0.8663 - acc: 0.6193 - val_loss: 0.6723 - val_acc: 0.6033Epoch 8/32226/225 [==============================] - 80s 353ms/step - loss: 0.8656 - acc: 0.5956 - val_loss: 0.9622 - val_acc: 0.6312Epoch 9/32226/225 [==============================] - 80s 354ms/step - loss: 0.7594 - acc: 0.6465 - val_loss: 0.6115 - val_acc: 0.6655Epoch 10/32226/225 [==============================] - 80s 354ms/step - loss: 0.8273 - acc: 0.6457 - val_loss: 5.6990 - val_acc: 0.5010Epoch 11/32226/225 [==============================] - 80s 355ms/step - loss: 0.8979 - acc: 0.5210 - val_loss: 0.7881 - val_acc: 0.5429Epoch 12/32226/225 [==============================] - 80s 355ms/step - loss: 0.7378 - acc: 0.5575 - val_loss: 0.7642 - val_acc: 0.5741Epoch 13/32226/225 [==============================] - 80s 354ms/step - loss: 0.6924 - acc: 0.5781 - val_loss: 0.7743 - val_acc: 0.5873Epoch 14/32226/225 [==============================] - 80s 355ms/step - loss: 0.6797 - acc: 0.5925 - val_loss: 0.6813 - val_acc: 0.5864Epoch 15/32226/225 [==============================] - 80s 353ms/step - loss: 0.6758 - acc: 0.6071 - val_loss: 0.6564 - val_acc: 0.6355Epoch 16/32226/225 [==============================] - 80s 352ms/step - loss: 0.6618 - acc: 0.6306 - val_loss: 0.6620 - val_acc: 0.6398Epoch 17/32226/225 [==============================] - 80s 354ms/step - loss: 0.6850 - acc: 0.5840 - val_loss: 0.6861 - val_acc: 0.5525Epoch 18/32226/225 [==============================] - 80s 355ms/step - loss: 0.6746 - acc: 0.5918 - val_loss: 0.6706 - val_acc: 0.5904Epoch 19/32226/225 [==============================] - 81s 357ms/step - loss: 0.6513 - acc: 0.6269 - val_loss: 0.6519 - val_acc: 0.6323Epoch 20/32226/225 [==============================] - 80s 353ms/step - loss: 0.6425 - acc: 0.6363 - val_loss: 0.6568 - val_acc: 0.6289Epoch 21/32226/225 [==============================] - 80s 353ms/step - loss: 0.6306 - acc: 0.6528 - val_loss: 0.6282 - val_acc: 0.6532Epoch 22/32226/225 [==============================] - 80s 356ms/step - loss: 0.6204 - acc: 0.6574 - val_loss: 0.6147 - val_acc: 0.6574Epoch 23/32226/225 [==============================] - 80s 352ms/step - loss: 0.5968 - acc: 0.6823 - val_loss: 0.6732 - val_acc: 0.6366Epoch 24/32226/225 [==============================] - 80s 353ms/step - loss: 0.5842 - acc: 0.6910 - val_loss: 0.5618 - val_acc: 0.7125Epoch 25/32226/225 [==============================] - 80s 352ms/step - loss: 0.5762 - acc: 0.6996 - val_loss: 0.9354 - val_acc: 0.5436Epoch 26/32226/225 [==============================] - 80s 353ms/step - loss: 0.5724 - acc: 0.7059 - val_loss: 4.0694 - val_acc: 0.4996Epoch 27/32226/225 [==============================] - 80s 354ms/step - loss: 0.5702 - acc: 0.7041 - val_loss: 0.5508 - val_acc: 0.7060Epoch 28/32226/225 [==============================] - 80s 353ms/step - loss: 0.5459 - acc: 0.7235 - val_loss: 0.5939 - val_acc: 0.6792Epoch 29/32226/225 [==============================] - 80s 352ms/step - loss: 0.5353 - acc: 0.7347 - val_loss: 0.6381 - val_acc: 0.6692Epoch 30/32226/225 [==============================] - 80s 354ms/step - loss: 0.5242 - acc: 0.7404 - val_loss: 0.7704 - val_acc: 0.6610Epoch 31/32226/225 [==============================] - 80s 355ms/step - loss: 0.5159 - acc: 0.7481 - val_loss: 1.4030 - val_acc: 0.5737Epoch 32/32226/225 [==============================] - 80s 352ms/step - loss: 0.4922 - acc: 0.7576 - val_loss: 0.5993 - val_acc: 0.7081</code></pre><h2 id="학습결과-시각화"><a href="#학습결과-시각화" class="headerlink" title="학습결과 시각화"></a>학습결과 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history_dict)</span>:</span></span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy and loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="cat-dog-resnet_15_0.png" alt="png"></p><p>그래프의 변동이 심하니 지수 이동 평균값을 구하여 그래프를 다시 그립니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">.8</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">smooth_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key, val <span class="keyword">in</span> history.history.items():</span><br><span class="line">    smooth_data[key] = smooth_curve(val)</span><br><span class="line">show_graph(smooth_data)</span><br></pre></td></tr></table></figure><p><img src="cat-dog-resnet_18_0.png" alt="png"></p><p>현재 Tesla K80으로 약 40분을 훈련시켰는데, 그래프로만 봐서는 과소적합일 가능성이 보입니다.<br>조금 더 학습을 시켜보면 알수 있겠네요.</p><p>모델의 평가를 해본 이후에 32에폭을 더 돌려보도록 하겠습니다.</p><h3 id="모델-중간-평가"><a href="#모델-중간-평가" class="headerlink" title="모델 중간 평가"></a>모델 중간 평가</h3><p>현재 모델을 기준으로 테스트셋의 정확도와 손실율을 구해보겠습니다.<br>테스트셋의 경우 rescale만 적용하여 원본 이미지 그대로 사용합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">testGenerator = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">testGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'test_set'</span>),</span><br><span class="line">    target_size=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.evaluate_generator(testGen)</span><br></pre></td></tr></table></figure><pre><code>Found 2023 images belonging to 2 classes.[0.6065594666792624, 0.7162629758080102]</code></pre><h2 id="모델-추가-학습"><a href="#모델-추가-학습" class="headerlink" title="모델 추가 학습"></a>모델 추가 학습</h2><p>이전의 그래프로는 과소적합일 가능성이 있기 때문에 32에폭만 추가적으로 더 학습을 진행해보겠습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">32</span></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    trainGen, </span><br><span class="line">    epochs=epochs,</span><br><span class="line">    steps_per_epoch=trainGen.samples / epochs, </span><br><span class="line">    validation_data=validationGen,</span><br><span class="line">    validation_steps=trainGen.samples / epochs,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/32226/225 [==============================] - 80s 355ms/step - loss: 0.4832 - acc: 0.7692 - val_loss: 0.5355 - val_acc: 0.7609Epoch 2/32226/225 [==============================] - 80s 353ms/step - loss: 0.4713 - acc: 0.7720 - val_loss: 0.4927 - val_acc: 0.7584Epoch 3/32226/225 [==============================] - 80s 356ms/step - loss: 0.4615 - acc: 0.7808 - val_loss: 0.5980 - val_acc: 0.7175Epoch 4/32226/225 [==============================] - 80s 352ms/step - loss: 0.4611 - acc: 0.7821 - val_loss: 1.0948 - val_acc: 0.5885Epoch 5/32226/225 [==============================] - 80s 352ms/step - loss: 0.4445 - acc: 0.7929 - val_loss: 0.5592 - val_acc: 0.7476Epoch 6/32226/225 [==============================] - 80s 353ms/step - loss: 0.4443 - acc: 0.7986 - val_loss: 0.5090 - val_acc: 0.7360Epoch 7/32226/225 [==============================] - 80s 355ms/step - loss: 0.4169 - acc: 0.8060 - val_loss: 0.5854 - val_acc: 0.7501Epoch 8/32226/225 [==============================] - 80s 352ms/step - loss: 0.4104 - acc: 0.8143 - val_loss: 0.9060 - val_acc: 0.7382Epoch 9/32226/225 [==============================] - 80s 352ms/step - loss: 0.4023 - acc: 0.8198 - val_loss: 0.4205 - val_acc: 0.8031Epoch 10/32226/225 [==============================] - 80s 352ms/step - loss: 0.3985 - acc: 0.8197 - val_loss: 1.0545 - val_acc: 0.6121Epoch 11/32226/225 [==============================] - 81s 356ms/step - loss: 0.3883 - acc: 0.8228 - val_loss: 0.4979 - val_acc: 0.7987Epoch 12/32226/225 [==============================] - 80s 352ms/step - loss: 0.3755 - acc: 0.8326 - val_loss: 0.6414 - val_acc: 0.6928Epoch 13/32226/225 [==============================] - 80s 352ms/step - loss: 0.3692 - acc: 0.8364 - val_loss: 0.5251 - val_acc: 0.7304Epoch 14/32226/225 [==============================] - 80s 352ms/step - loss: 0.3950 - acc: 0.8204 - val_loss: 3.7220 - val_acc: 0.5317Epoch 15/32226/225 [==============================] - 80s 354ms/step - loss: 0.3995 - acc: 0.8192 - val_loss: 0.4228 - val_acc: 0.7976Epoch 16/32226/225 [==============================] - 80s 353ms/step - loss: 0.3514 - acc: 0.8469 - val_loss: 0.5189 - val_acc: 0.7638Epoch 17/32226/225 [==============================] - 80s 352ms/step - loss: 0.3363 - acc: 0.8533 - val_loss: 0.3654 - val_acc: 0.8330Epoch 18/32226/225 [==============================] - 80s 355ms/step - loss: 0.3307 - acc: 0.8577 - val_loss: 0.4184 - val_acc: 0.8128Epoch 19/32226/225 [==============================] - 80s 354ms/step - loss: 0.3644 - acc: 0.8398 - val_loss: 0.5159 - val_acc: 0.7530Epoch 20/32226/225 [==============================] - 80s 353ms/step - loss: 0.3657 - acc: 0.8429 - val_loss: 0.5468 - val_acc: 0.7775Epoch 21/32226/225 [==============================] - 80s 355ms/step - loss: 0.3313 - acc: 0.8565 - val_loss: 1.4114 - val_acc: 0.5933Epoch 22/32226/225 [==============================] - 80s 353ms/step - loss: 0.3172 - acc: 0.8613 - val_loss: 1.1401 - val_acc: 0.6374Epoch 23/32226/225 [==============================] - 80s 353ms/step - loss: 0.3278 - acc: 0.8590 - val_loss: 0.5699 - val_acc: 0.7566Epoch 24/32226/225 [==============================] - 80s 354ms/step - loss: 0.3031 - acc: 0.8703 - val_loss: 0.3731 - val_acc: 0.8324Epoch 25/32226/225 [==============================] - 80s 353ms/step - loss: 0.2799 - acc: 0.8809 - val_loss: 0.4147 - val_acc: 0.8131Epoch 26/32226/225 [==============================] - 80s 355ms/step - loss: 0.2879 - acc: 0.8765 - val_loss: 0.3705 - val_acc: 0.8410Epoch 27/32226/225 [==============================] - 80s 354ms/step - loss: 0.2953 - acc: 0.8763 - val_loss: 0.4153 - val_acc: 0.8234Epoch 28/32226/225 [==============================] - 80s 352ms/step - loss: 0.2705 - acc: 0.8829 - val_loss: 0.3329 - val_acc: 0.8583Epoch 29/32226/225 [==============================] - 80s 355ms/step - loss: 0.2615 - acc: 0.8906 - val_loss: 0.3774 - val_acc: 0.8317Epoch 30/32226/225 [==============================] - 81s 356ms/step - loss: 0.2550 - acc: 0.8910 - val_loss: 0.4223 - val_acc: 0.8317Epoch 31/32226/225 [==============================] - 79s 351ms/step - loss: 0.2552 - acc: 0.8906 - val_loss: 0.4189 - val_acc: 0.8092Epoch 32/32226/225 [==============================] - 80s 352ms/step - loss: 0.2475 - acc: 0.8917 - val_loss: 0.3132 - val_acc: 0.8566</code></pre><h3 id="학습결과-시각화-1"><a href="#학습결과-시각화-1" class="headerlink" title="학습결과 시각화"></a>학습결과 시각화</h3><p>마찬가지로 지수이동평균을 적용하여 그래프를 그려보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">smooth_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key, val <span class="keyword">in</span> history.history.items():</span><br><span class="line">    smooth_data[key] = smooth_curve(val)</span><br><span class="line">show_graph(smooth_data)</span><br></pre></td></tr></table></figure><p><img src="cat-dog-resnet_25_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate_generator(testGen)</span><br></pre></td></tr></table></figure><pre><code>[0.33355356405939846, 0.8581314879482004]</code></pre><p>여전히 과소적합인 것 같네요.<br>테스트셋에 대한 정확도가 85%까지 올라왔는데 아마 학습을 더 진행하게 된다면 더 높은 성능을 내줄거 같습니다.  </p><p>아무래도 모델을 처음부터 모델을 학습시키다보니 시간이 오래 걸리네요.<br>대회에 나갈때에는 모델을 2~3주동안 학습시켰답니다.<br>여러분도 학습을 추가적으로 진행시켜보세요 !</p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/19/cat-dog-resnet/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝으로 개와 고양이를 분류하는 모델을 만들어 보자.</title>
      <link>https://codingcrews.github.io/2019/01/17/cat-dog/</link>
      <guid>https://codingcrews.github.io/2019/01/17/cat-dog/</guid>
      <pubDate>Thu, 17 Jan 2019 14:25:11 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;개와-고양이-분류하기&quot;&gt;&lt;a href=&quot;#개와-고양이-분류하기&quot; class=&quot;headerlink&quot; title=&quot;개와 고양이 분류하기&quot;&gt;&lt;/a&gt;개와 고양이 분류하기&lt;/h1&gt;&lt;p&gt;이번에는 CNN(Convolutional Neural Netw
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="개와-고양이-분류하기"><a href="#개와-고양이-분류하기" class="headerlink" title="개와 고양이 분류하기"></a>개와 고양이 분류하기</h1><p>이번에는 CNN(Convolutional Neural Network)를 이용하여 개와 고양이 사진을 분류하는 모델을 만들어 봅시다.  </p><p>사람은 개와 고양이를 분류하기가 굉장히 쉽죠. 딱 보면 아니까요.<br>하지만 컴퓨터 공학에서는 이걸 분류하는 문제가 해결하기 굉장히 어려운 문제였습니다.<br>컴퓨터는 일일히 룰을 지정해가며 개와 고양이를 분류하도록 만들어야 하니까요.  </p><p>그래서 최근에는 딥러닝이 이러한 문제들을 해결하면서 최근 비전 관련 문제들을 딥러닝으로 풀어내려는 시도가 많습니다.<br>오늘 소개할 CNN은 이미지 인식 등에서 기존의 전통적인 비전 프로세싱에 비해 높은 성능을 보여줍니다.  </p><p>오늘 시도하는 튜토리얼은 CPU로 학습시키기엔 꽤 오랜 시간이 걸리므로 인내심을 가지고 학습을 진행하시거나,<br><a href="/2019/01/15/deeplearning-gpu/" target="_blank">AWS로 GPU 딥러닝 환경 구축하기</a> 포스팅을 참고하여 환경을 세팅한 뒤 진행해보세요.</p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>이번 데이터셋은 kaggle에서 제공하는 데이터셋을 이용할 예정입니다.<br>MNIST 데이터셋은 흑백의 이미지였지만, 이번에 사용할 이미지는 컬러를 가지고 있습니다.  </p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><p>직접 데이터셋을 배포하진 않고 있습니다.<br>데이터셋은 <a href="https://www.kaggle.com/tongpython/cat-and-dog" target="_blank" rel="noopener">여기</a>에서 직접 다운로드 받으실 수 있고, kaggle api를 통하여 받으실 수 있습니다.<br>로그인을 해야하니 만약 가입하지 않으셨다면 이번 기회에 가입해보시는걸 추천합니다.  </p><p>저희는 이 노트와 동일한 위치에 datasets이란 폴더에 데이터셋을 넣어두고 사용하도록 하겠습니다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">datasets</span><br><span class="line">└── cat-and-dog</span><br><span class="line">    ├── test_set</span><br><span class="line">    │   ├── cats</span><br><span class="line">    │   │   └── datas...</span><br><span class="line">    │   └── dogs</span><br><span class="line">    │       └── datas...</span><br><span class="line">    └── training_set</span><br><span class="line">        ├── cats</span><br><span class="line">        │   └── datas...</span><br><span class="line">        └── dogs</span><br><span class="line">            └── datas...</span><br></pre></td></tr></table></figure><p>이와 같은 형태의 구성이 되어 있다고 가정합니다.</p><h2 id="데이터-증식"><a href="#데이터-증식" class="headerlink" title="데이터 증식"></a>데이터 증식</h2><p>데이터 증식(Data argumentaion)을 위해 케라스에서 제공하는 이미지 제너레이터를 사용합니다.<br>이미지의 위치를 조금 옮긴다거나, 회전, 좌우반전등을 했을 때 컴퓨터가 받아들이는 이미지는 전혀 다른것이 됩니다.<br>이러한 변형을 줌으로써 학습 데이터를 늘리고, 이러한 변조에 강하게 모델을 학습시킬 수 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">rootPath = <span class="string">'./datasets/cat-and-dog'</span></span><br><span class="line"></span><br><span class="line">imageGenerator = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">20</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.1</span>,</span><br><span class="line">    brightness_range=[<span class="number">.2</span>, <span class="number">.2</span>],</span><br><span class="line">    horizontal_flip=<span class="keyword">True</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    subset=<span class="string">'training'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">validationGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'training_set'</span>),</span><br><span class="line">    target_size=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    subset=<span class="string">'validation'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Found 7205 images belonging to 2 classes.Found 800 images belonging to 2 classes.</code></pre><p>rescale은 이미지의 nomalization을 위해 사용합니다. 각 이미지별로 255로 나눈 값으로 데이터가 변형됩니다.<br>rotation_range는 이미지의 최대 회전각을 지정합니다. 최대 20도까지 회전합니다.<br>width,height shift_range는 이미지의 이동을 말합니다. 좌우, 위아래로 이미지의 이동하는 백분율을 지정합니다. (0.1은 10%)<br>brightness_range는 이미지 밝기에 대한 내용입니다.<br>horizontal_flip은 이미지의 수평 반전을 시켜줍니다. 이 옵션의 경우 <strong>데이터셋의 이해가 필요</strong>합니다. 예를 들면 MNIST 데이터셋의 경우 손글씨 데이터이기 때문에 수평 반전이 일어나면 안됩니다.<br>validation_split은 검증세트의 비율을 지정해줍니다.  </p><p>설정한 이미지 제너레이터를 통해 특정 디렉터리의 데이터들을 손쉽게 불러올 수 있습니다.<br>현재 데이터셋 구성에 맞춰 폴더이름은 레이블명, 폴더안의 데이터는 해당 레이블의 데이터셋이 됩니다.<br>target_size는 이미지를 해당 형태로 변형시켜주는데, 저희는 64x64 사이즈로 데이터로 읽어오겠습니다.</p><h1 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h1><p>간단한 CNN 모델을 구성하여 학습을 진행해봅니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.InputLayer(input_shape=(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>), <span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Dropout(rate=<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>), <span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Dropout(rate=<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>), <span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Dropout(rate=<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_3 (Conv2D)            (None, 64, 64, 16)        448       _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 32, 32, 16)        0         _________________________________________________________________dropout_3 (Dropout)          (None, 32, 32, 16)        0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 32, 32, 32)        4640      _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         _________________________________________________________________dropout_4 (Dropout)          (None, 16, 16, 32)        0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     _________________________________________________________________max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         _________________________________________________________________dropout_5 (Dropout)          (None, 8, 8, 64)          0         _________________________________________________________________flatten_1 (Flatten)          (None, 4096)              0         _________________________________________________________________dense_3 (Dense)              (None, 512)               2097664   _________________________________________________________________dense_4 (Dense)              (None, 256)               131328    _________________________________________________________________dense_5 (Dense)              (None, 2)                 514       =================================================================Total params: 2,253,090Trainable params: 2,253,090Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainGen.samples</span><br></pre></td></tr></table></figure><pre><code>7205</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>, </span><br><span class="line">    metrics=[<span class="string">'acc'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">32</span></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    trainGen, </span><br><span class="line">    epochs=epochs,</span><br><span class="line">    steps_per_epoch=trainGen.samples / epochs, </span><br><span class="line">    validation_data=validationGen,</span><br><span class="line">    validation_steps=trainGen.samples / epochs,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/32226/225 [==============================] - 37s 164ms/step - loss: 0.6921 - acc: 0.5143 - val_loss: 0.6901 - val_acc: 0.5551Epoch 2/32226/225 [==============================] - 36s 159ms/step - loss: 0.6824 - acc: 0.5672 - val_loss: 0.6680 - val_acc: 0.5807Epoch 3/32226/225 [==============================] - 36s 158ms/step - loss: 0.6727 - acc: 0.5781 - val_loss: 0.6682 - val_acc: 0.5857Epoch 4/32226/225 [==============================] - 36s 159ms/step - loss: 0.6614 - acc: 0.6063 - val_loss: 0.6381 - val_acc: 0.6231Epoch 5/32226/225 [==============================] - 36s 159ms/step - loss: 0.6451 - acc: 0.6323 - val_loss: 0.6168 - val_acc: 0.6573Epoch 6/32226/225 [==============================] - 36s 159ms/step - loss: 0.6329 - acc: 0.6482 - val_loss: 0.6262 - val_acc: 0.6653Epoch 7/32226/225 [==============================] - 36s 158ms/step - loss: 0.6093 - acc: 0.6664 - val_loss: 0.5814 - val_acc: 0.6937Epoch 8/32226/225 [==============================] - 37s 165ms/step - loss: 0.5997 - acc: 0.6765 - val_loss: 0.6226 - val_acc: 0.6498Epoch 9/32226/225 [==============================] - 36s 158ms/step - loss: 0.6055 - acc: 0.6683 - val_loss: 0.5609 - val_acc: 0.7178Epoch 10/32226/225 [==============================] - 36s 159ms/step - loss: 0.5880 - acc: 0.6837 - val_loss: 0.5585 - val_acc: 0.7047Epoch 11/32226/225 [==============================] - 36s 158ms/step - loss: 0.5769 - acc: 0.6972 - val_loss: 0.5701 - val_acc: 0.7006Epoch 12/32226/225 [==============================] - 36s 159ms/step - loss: 0.5727 - acc: 0.7003 - val_loss: 0.5507 - val_acc: 0.7168Epoch 13/32226/225 [==============================] - 36s 158ms/step - loss: 0.5673 - acc: 0.7059 - val_loss: 0.5697 - val_acc: 0.7028Epoch 14/32226/225 [==============================] - 36s 158ms/step - loss: 0.5588 - acc: 0.7132 - val_loss: 0.5845 - val_acc: 0.6786Epoch 15/32226/225 [==============================] - 36s 160ms/step - loss: 0.5522 - acc: 0.7187 - val_loss: 0.5251 - val_acc: 0.7267Epoch 16/32226/225 [==============================] - 37s 163ms/step - loss: 0.5570 - acc: 0.7141 - val_loss: 0.5353 - val_acc: 0.7317Epoch 17/32226/225 [==============================] - 37s 162ms/step - loss: 0.5441 - acc: 0.7182 - val_loss: 0.5237 - val_acc: 0.7369Epoch 18/32226/225 [==============================] - 36s 158ms/step - loss: 0.5399 - acc: 0.7227 - val_loss: 0.5142 - val_acc: 0.7402Epoch 19/32226/225 [==============================] - 36s 158ms/step - loss: 0.5343 - acc: 0.7278 - val_loss: 0.5283 - val_acc: 0.7217Epoch 20/32226/225 [==============================] - 36s 158ms/step - loss: 0.5352 - acc: 0.7262 - val_loss: 0.5031 - val_acc: 0.7476Epoch 21/32226/225 [==============================] - 36s 158ms/step - loss: 0.5278 - acc: 0.7312 - val_loss: 0.5222 - val_acc: 0.7347Epoch 22/32226/225 [==============================] - 36s 158ms/step - loss: 0.5251 - acc: 0.7374 - val_loss: 0.4975 - val_acc: 0.7667Epoch 23/32226/225 [==============================] - 36s 158ms/step - loss: 0.5186 - acc: 0.7448 - val_loss: 0.4979 - val_acc: 0.7543Epoch 24/32226/225 [==============================] - 36s 158ms/step - loss: 0.5091 - acc: 0.7504 - val_loss: 0.5470 - val_acc: 0.7277Epoch 25/32226/225 [==============================] - 38s 168ms/step - loss: 0.5108 - acc: 0.7416 - val_loss: 0.5053 - val_acc: 0.7491Epoch 26/32226/225 [==============================] - 36s 158ms/step - loss: 0.5028 - acc: 0.7507 - val_loss: 0.5248 - val_acc: 0.7302Epoch 27/32226/225 [==============================] - 36s 159ms/step - loss: 0.5008 - acc: 0.7523 - val_loss: 0.4948 - val_acc: 0.7599Epoch 28/32226/225 [==============================] - 36s 158ms/step - loss: 0.5063 - acc: 0.7456 - val_loss: 0.5401 - val_acc: 0.7167Epoch 29/32226/225 [==============================] - 36s 157ms/step - loss: 0.4870 - acc: 0.7671 - val_loss: 0.5423 - val_acc: 0.7310Epoch 30/32226/225 [==============================] - 36s 158ms/step - loss: 0.4976 - acc: 0.7625 - val_loss: 0.5083 - val_acc: 0.7450Epoch 31/32226/225 [==============================] - 36s 160ms/step - loss: 0.4917 - acc: 0.7613 - val_loss: 0.5162 - val_acc: 0.7344Epoch 32/32226/225 [==============================] - 36s 158ms/step - loss: 0.4840 - acc: 0.7648 - val_loss: 0.5015 - val_acc: 0.7436</code></pre><h2 id="학습결과-시각화-및-평가"><a href="#학습결과-시각화-및-평가" class="headerlink" title="학습결과 시각화 및 평가"></a>학습결과 시각화 및 평가</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history_dict)</span>:</span></span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy and loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="cat-dog_13_0.png" alt="png"></p><p>32 에폭의 학습을 마친 그래프입니다.<br>GPU를 GTX 750TI 모델을 사용중인데 에폭당 36초씩 약 20분 가량 학습 시간이 소요되었습니다.<br>그래프를 보면 과소적합 된것처럼 보입니다.</p><h3 id="모델-중간-평가"><a href="#모델-중간-평가" class="headerlink" title="모델 중간 평가"></a>모델 중간 평가</h3><p>현재 모델을 기준으로 테스트셋의 정확도와 손실율을 구해보겠습니다.<br>테스트셋의 경우 rescale만 적용하여 원본 이미지 그대로 넣습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">testGenerator = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">testGen = imageGenerator.flow_from_directory(</span><br><span class="line">    os.path.join(rootPath, <span class="string">'test_set'</span>),</span><br><span class="line">    target_size=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.evaluate_generator(testGen)</span><br></pre></td></tr></table></figure><pre><code>Found 2023 images belonging to 2 classes.[0.5174207690176962, 0.7412259022139466]</code></pre><p>74.1% 의 정확도를 보여줍니다.<br>과소적합이 의심되니 32에폭을 더 돌려보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">32</span></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    trainGen, </span><br><span class="line">    epochs=epochs,</span><br><span class="line">    steps_per_epoch=trainGen.samples / epochs, </span><br><span class="line">    validation_data=validationGen,</span><br><span class="line">    validation_steps=trainGen.samples / epochs,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/32226/225 [==============================] - 37s 162ms/step - loss: 0.4809 - acc: 0.7647 - val_loss: 0.4936 - val_acc: 0.7552Epoch 2/32226/225 [==============================] - 38s 167ms/step - loss: 0.4797 - acc: 0.7714 - val_loss: 0.5129 - val_acc: 0.7468Epoch 3/32226/225 [==============================] - 36s 158ms/step - loss: 0.4706 - acc: 0.7766 - val_loss: 0.4703 - val_acc: 0.7754Epoch 4/32226/225 [==============================] - 36s 158ms/step - loss: 0.4771 - acc: 0.7682 - val_loss: 0.4822 - val_acc: 0.7671Epoch 5/32226/225 [==============================] - 36s 158ms/step - loss: 0.4721 - acc: 0.7637 - val_loss: 0.4953 - val_acc: 0.7539Epoch 6/32226/225 [==============================] - 36s 158ms/step - loss: 0.4714 - acc: 0.7736 - val_loss: 0.4742 - val_acc: 0.7727Epoch 7/32226/225 [==============================] - 36s 158ms/step - loss: 0.4698 - acc: 0.7747 - val_loss: 0.5087 - val_acc: 0.7595Epoch 8/32226/225 [==============================] - 36s 158ms/step - loss: 0.4577 - acc: 0.7805 - val_loss: 0.4736 - val_acc: 0.7674Epoch 9/32226/225 [==============================] - 36s 158ms/step - loss: 0.4615 - acc: 0.7771 - val_loss: 0.5172 - val_acc: 0.7386Epoch 10/32226/225 [==============================] - 36s 158ms/step - loss: 0.4628 - acc: 0.7766 - val_loss: 0.5027 - val_acc: 0.7499Epoch 11/32226/225 [==============================] - 37s 165ms/step - loss: 0.4598 - acc: 0.7785 - val_loss: 0.4804 - val_acc: 0.7623Epoch 12/32226/225 [==============================] - 36s 158ms/step - loss: 0.4515 - acc: 0.7859 - val_loss: 0.4893 - val_acc: 0.7549Epoch 13/32226/225 [==============================] - 36s 158ms/step - loss: 0.4569 - acc: 0.7862 - val_loss: 0.4973 - val_acc: 0.7631Epoch 14/32226/225 [==============================] - 36s 158ms/step - loss: 0.4350 - acc: 0.7974 - val_loss: 0.4770 - val_acc: 0.7730Epoch 15/32226/225 [==============================] - 36s 158ms/step - loss: 0.4376 - acc: 0.8010 - val_loss: 0.5099 - val_acc: 0.7562Epoch 16/32226/225 [==============================] - 36s 158ms/step - loss: 0.4416 - acc: 0.7891 - val_loss: 0.4770 - val_acc: 0.7665Epoch 17/32226/225 [==============================] - 36s 158ms/step - loss: 0.4342 - acc: 0.7915 - val_loss: 0.4721 - val_acc: 0.7779Epoch 18/32226/225 [==============================] - 36s 159ms/step - loss: 0.4388 - acc: 0.7926 - val_loss: 0.4733 - val_acc: 0.7666Epoch 19/32226/225 [==============================] - 37s 163ms/step - loss: 0.4189 - acc: 0.7991 - val_loss: 0.4757 - val_acc: 0.7711Epoch 20/32226/225 [==============================] - 36s 161ms/step - loss: 0.4194 - acc: 0.7991 - val_loss: 0.4706 - val_acc: 0.7801Epoch 21/32226/225 [==============================] - 36s 159ms/step - loss: 0.4334 - acc: 0.7942 - val_loss: 0.4563 - val_acc: 0.7807Epoch 22/32226/225 [==============================] - 36s 158ms/step - loss: 0.4205 - acc: 0.8017 - val_loss: 0.4695 - val_acc: 0.7667Epoch 23/32226/225 [==============================] - 36s 158ms/step - loss: 0.4246 - acc: 0.8006 - val_loss: 0.4741 - val_acc: 0.7678Epoch 24/32226/225 [==============================] - 36s 159ms/step - loss: 0.4097 - acc: 0.8103 - val_loss: 0.5746 - val_acc: 0.7368Epoch 25/32226/225 [==============================] - 36s 158ms/step - loss: 0.4137 - acc: 0.8051 - val_loss: 0.4951 - val_acc: 0.7663Epoch 26/32226/225 [==============================] - 39s 173ms/step - loss: 0.4066 - acc: 0.8077 - val_loss: 0.4999 - val_acc: 0.7667Epoch 27/32226/225 [==============================] - 36s 159ms/step - loss: 0.4121 - acc: 0.8110 - val_loss: 0.4716 - val_acc: 0.7789Epoch 28/32226/225 [==============================] - 38s 167ms/step - loss: 0.3985 - acc: 0.8153 - val_loss: 0.5083 - val_acc: 0.7613Epoch 29/32226/225 [==============================] - 36s 158ms/step - loss: 0.3980 - acc: 0.8153 - val_loss: 0.4996 - val_acc: 0.7672Epoch 30/32226/225 [==============================] - 36s 159ms/step - loss: 0.4105 - acc: 0.8083 - val_loss: 0.4759 - val_acc: 0.7745Epoch 31/32226/225 [==============================] - 36s 159ms/step - loss: 0.4027 - acc: 0.8112 - val_loss: 0.4851 - val_acc: 0.7696Epoch 32/32226/225 [==============================] - 36s 158ms/step - loss: 0.3988 - acc: 0.8205 - val_loss: 0.4962 - val_acc: 0.7692</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="cat-dog_19_0.png" alt="png"></p><p>그래프를 보니 과대적합이 되어가고 있는것처럼 보입니다.<br>모델이 문제에 비해 간단한 모델인 것 같네요.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate_generator(testGen)</span><br></pre></td></tr></table></figure><pre><code>[0.5502711092547738, 0.736529906139006]</code></pre><p>테스트셋의 정확도를 봤을 때 73%로 아까보다 오히려 떨어진 결과를 보여줍니다.<br>예상대로 과대적합이 되어가고 있던 모양입니다.  </p><h2 id="모델-예측"><a href="#모델-예측" class="headerlink" title="모델 예측"></a>모델 예측</h2><p>모델 학습시켰으니 개와 고양이를 예측시키는걸 한번 해보도록 하겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> array_to_img</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cls_index = [<span class="string">'고양이'</span>, <span class="string">'개'</span>]</span><br><span class="line"></span><br><span class="line">imgs = testGen.next()</span><br><span class="line">arr = imgs[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">img = array_to_img(arr).resize((<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">plt.imshow(img)</span><br><span class="line">result = model.predict_classes(arr.reshape(<span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">print(<span class="string">'예측: &#123;&#125;'</span>.format(cls_index[result[<span class="number">0</span>]]))</span><br><span class="line">print(<span class="string">'정답: &#123;&#125;'</span>.format(cls_index[np.argmax(imgs[<span class="number">1</span>][<span class="number">0</span>])]))</span><br></pre></td></tr></table></figure><pre><code>예측: 고양이정답: 고양이</code></pre><p><img src="cat-dog_25_1.png" alt="png"></p><p>사이즈를 줄인 이미지를 강제로 키웠더니 이미지 품질이 많이 떨어졌네요.<br>모델이 예측한 답과 원본 답이 일치하는걸 볼 수 있습니다.<br>물론 정확도가 많이 낮아 여러번 하면 틀린답도 자주 나옵니다.</p><p>다음에는 이미지넷에서 큰 성과를 이뤘던 모델들을 가지고 정확도를 더 높일 수 있도록 학습을 진행해보겠습니다.  </p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/17/cat-dog/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝으로 손글씨를 인식하는 모델을 만들어보자.</title>
      <link>https://codingcrews.github.io/2019/01/17/mnist/</link>
      <guid>https://codingcrews.github.io/2019/01/17/mnist/</guid>
      <pubDate>Thu, 17 Jan 2019 09:52:53 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Mnist-데이터셋을-이용한-손글씨-분류하기-DNN-CNN&quot;&gt;&lt;a href=&quot;#Mnist-데이터셋을-이용한-손글씨-분류하기-DNN-CNN&quot; class=&quot;headerlink&quot; title=&quot;Mnist 데이터셋을 이용한 손글씨 분류하기 : D
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Mnist-데이터셋을-이용한-손글씨-분류하기-DNN-CNN"><a href="#Mnist-데이터셋을-이용한-손글씨-분류하기-DNN-CNN" class="headerlink" title="Mnist 데이터셋을 이용한 손글씨 분류하기 : DNN, CNN"></a>Mnist 데이터셋을 이용한 손글씨 분류하기 : DNN, CNN</h1><p>지금까지 진행한 포스팅을 기반으로 딥러닝 튜토리얼 시 가장 흔하게 접할 수 있는 손글씨 분류하기를 해보겠습니다.<br>MNIST 문제는 다중 분류 문제로써 0~9까지의 손글씨를 분류하는 문제입니다.<br>이전 포스팅에서 사용한 선형 레이어를 이용하여 0~9의 숫자를 분류해보고, 이후에는 CNN을 이용해 정확도를 개선해보도록 하겠습니다.  </p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>사용할 MNIST 데이터 세트는 텐서플로 패키지에서 다운로드까지 진행해주는 코드를 포함시켜두었습니다.<br>우리가 사용할 데이터셋은 텐서플로 패키지에서 제공하는 데이터셋을 이용할건데요.<br>다른곳에서 MNIST 데이터셋을 이용하여 진행해봐도 무방합니다.  </p><p>우리가 사용할 MNIST 데이터는 손글씨 데이터로써 흑백 Gray Scale로 된 데이터셋을 말합니다.<br>28x28 사이즈의 단일 색상채널을 가지고 있으며, 트레이닝셋과 테스트셋이 각각 60,000개와 10,000개로 구성되어 있습니다.  </p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train),(x_test, y_test) = mnist.load_data()</span><br></pre></td></tr></table></figure><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">display(x_train.shape)</span><br><span class="line">display(y_train.shape)</span><br><span class="line">display(x_test.shape)</span><br><span class="line">display(y_test.shape)</span><br><span class="line"></span><br><span class="line">display(y_test)</span><br></pre></td></tr></table></figure><pre><code>(60000, 28, 28)(60000,)(10000, 28, 28)(10000,)array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)</code></pre><p>데이터는 60,000개의 트레이닝셋과 10,000개의 테스트셋을 확인할 수 있고,<br>테스트 데이터는 28x28 사이즈의 이미지 데이터임을 확인할 수 있습니다. (색상 채널은 단일 채널이라 따로 표기되지 않아요.)<br>레이블 데이터는 0~9까지의 10개 클래스로 구성되어 있습니다.</p><h3 id="데이터-시각화"><a href="#데이터-시각화" class="headerlink" title="데이터 시각화"></a>데이터 시각화</h3><p>우리가 다룰 Mnist 데이터는 이미지 데이터 기반입니다.<br>해당 이미지를 시각화하여 확인해보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(arr)</span>:</span></span><br><span class="line">    plt.imshow(arr, cmap=plt.cm.binary)</span><br><span class="line">    </span><br><span class="line">    reshape_data = arr.reshape(<span class="number">-1</span>, )</span><br><span class="line">    <span class="keyword">for</span> index, data <span class="keyword">in</span> enumerate(reshape_data):</span><br><span class="line">        print(<span class="string">'&#123;:3d&#125;'</span>.format(data), end=<span class="string">''</span>)</span><br><span class="line">        <span class="keyword">if</span> index % <span class="number">28</span> == <span class="number">27</span>:</span><br><span class="line">            print()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_data(x_train[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51159253159 50  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0 48238252252252237  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0 54227253252239233252 57  6  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0 10 60224252253252202 84252253122  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0163252252252253252252 96189253167  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0 51238253253190114253228 47 79255168  0  0  0  0  0  00  0  0  0  0  0  0  0  0 48238252252179 12 75121 21  0  0253243 50  0  0  0  0  00  0  0  0  0  0  0  0 38165253233208 84  0  0  0  0  0  0253252165  0  0  0  0  00  0  0  0  0  0  0  7178252240 71 19 28  0  0  0  0  0  0253252195  0  0  0  0  00  0  0  0  0  0  0 57252252 63  0  0  0  0  0  0  0  0  0253252195  0  0  0  0  00  0  0  0  0  0  0198253190  0  0  0  0  0  0  0  0  0  0255253196  0  0  0  0  00  0  0  0  0  0 76246252112  0  0  0  0  0  0  0  0  0  0253252148  0  0  0  0  00  0  0  0  0  0 85252230 25  0  0  0  0  0  0  0  0  7135253186 12  0  0  0  0  00  0  0  0  0  0 85252223  0  0  0  0  0  0  0  0  7131252225 71  0  0  0  0  0  00  0  0  0  0  0 85252145  0  0  0  0  0  0  0 48165252173  0  0  0  0  0  0  0  00  0  0  0  0  0 86253225  0  0  0  0  0  0114238253162  0  0  0  0  0  0  0  0  00  0  0  0  0  0 85252249146 48 29 85178225253223167 56  0  0  0  0  0  0  0  0  00  0  0  0  0  0 85252252252229215252252252196130  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0 28199252252253252252233145  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0 25128252253252141 37  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</code></pre><p><img src="mnist_9_1.png" alt="png"></p><p>위는 해당 데이터를 시각화 한 결과입니다.<br>28x28의 데이터는 각 픽셀별로 해당 픽셀이 표시해야 할 색상값을 표시하는데,<br>이걸 28x28 사이즈에 맞춰 해당 데이터를 출력한 결과입니다.  </p><p>이미지는 matplotlib을 이용하여 해당 데이터를 시각화 한 결과입니다.</p><h2 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h2><p>이미지 데이터를 확인했을 때 0~255까지의 값을 가지고 있는걸 확인할 수 있습니다.<br>이걸 모델에 효율적으로 학습시키기 위해선 0~1 사이의 값으로 일반화(Nomalization)시키는 과정을 거쳐야 합니다.<br>그 이외에도 2D컨볼루셔널 레이어를 사용하기 위해선 채널에 대한 값도 명시되어 있어야하기 때문에<br>(넓이, 높이)의 형태를 가진 데이터를 (넓이,높이,채널)의 형태로 변환시켜보겠습니다.<br>구조는 어떤식으로든 상관없으며 (채널, 넓이, 높이) 순으로 정렬을 시켜도 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reshape_x_train = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">reshape_x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h1><p>이전 포스팅에서 다뤘던 Dense 레이어를 이용해 기본적인 신경망으로 학습을 진행해봅시다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.InputLayer(input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================flatten (Flatten)            (None, 784)               0         _________________________________________________________________dense (Dense)                (None, 64)                50240     _________________________________________________________________dense_1 (Dense)              (None, 16)                1040      _________________________________________________________________dense_2 (Dense)              (None, 10)                170       =================================================================Total params: 51,450Trainable params: 51,450Non-trainable params: 0_________________________________________________________________</code></pre><p>총 3개의 레이어로 구성되어 있으며, MNIST 데이터셋에 맞춰 (28, 28, 1)의 형태에 맞춰 인풋 데이터를 받도록 구성했습니다.<br>마지막층은 손글씨 데이터의 10개 클래스(0~9까지의 레이블)를 예측하기 위해 출력을 10으로, 활성함수는 softmax를 사용합니다.  </p><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h2><p><del>CPU로 학습 시 대략 25분 정도의 시간이 소요됩니다. (맥북프로 15인치 2017 CTO 기준)<br>그래서 저는 GPU 환경을 세팅하고 학습을 진행했습니다.<br>GPU 환경을 구축하기 위한 데스크탑을 만들기 어려우신 분들은 <a href="/2019/01/15/deeplearning-gpu/">AWS로 GPU환경 세팅하기</a>포스팅을 참고하세요.</del></p><p>모델의 구성을 변경했습니다.<br>CPU에서도 에폭당 1초의 결과를 보여줍니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    epochs=<span class="number">50</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 54000 samples, validate on 6000 samplesEpoch 1/5054000/54000 [==============================] - 1s 20us/step - loss: 6.2426 - acc: 0.5904 - val_loss: 4.6204 - val_acc: 0.6893Epoch 2/5054000/54000 [==============================] - 1s 14us/step - loss: 3.7938 - acc: 0.7398 - val_loss: 1.9817 - val_acc: 0.8462Epoch 3/5054000/54000 [==============================] - 1s 15us/step - loss: 1.2180 - acc: 0.8636 - val_loss: 0.6667 - val_acc: 0.8982Epoch 4/5054000/54000 [==============================] - 1s 12us/step - loss: 0.6308 - acc: 0.8874 - val_loss: 0.4537 - val_acc: 0.9087Epoch 5/5054000/54000 [==============================] - 1s 12us/step - loss: 0.4778 - acc: 0.9019 - val_loss: 0.3766 - val_acc: 0.9243Epoch 6/5054000/54000 [==============================] - 1s 13us/step - loss: 0.3996 - acc: 0.9122 - val_loss: 0.3421 - val_acc: 0.9307Epoch 7/5054000/54000 [==============================] - 1s 13us/step - loss: 0.3400 - acc: 0.9253 - val_loss: 0.2921 - val_acc: 0.9385Epoch 8/5054000/54000 [==============================] - 1s 13us/step - loss: 0.2954 - acc: 0.9321 - val_loss: 0.2361 - val_acc: 0.9437Epoch 9/5054000/54000 [==============================] - 1s 13us/step - loss: 0.2549 - acc: 0.9403 - val_loss: 0.2291 - val_acc: 0.9452Epoch 10/5054000/54000 [==============================] - 1s 13us/step - loss: 0.2292 - acc: 0.9447 - val_loss: 0.2212 - val_acc: 0.9495Epoch 11/5054000/54000 [==============================] - 1s 13us/step - loss: 0.2008 - acc: 0.9490 - val_loss: 0.2298 - val_acc: 0.9518Epoch 12/5054000/54000 [==============================] - 1s 13us/step - loss: 0.1871 - acc: 0.9531 - val_loss: 0.1879 - val_acc: 0.9533Epoch 13/5054000/54000 [==============================] - 1s 12us/step - loss: 0.1681 - acc: 0.9563 - val_loss: 0.1907 - val_acc: 0.9565Epoch 14/5054000/54000 [==============================] - 1s 14us/step - loss: 0.1610 - acc: 0.9572 - val_loss: 0.2007 - val_acc: 0.9540Epoch 15/5054000/54000 [==============================] - 1s 12us/step - loss: 0.1477 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9582Epoch 16/5054000/54000 [==============================] - 1s 12us/step - loss: 0.1330 - acc: 0.9646 - val_loss: 0.1695 - val_acc: 0.9598Epoch 17/5054000/54000 [==============================] - 1s 12us/step - loss: 0.1262 - acc: 0.9661 - val_loss: 0.1596 - val_acc: 0.9627Epoch 18/5054000/54000 [==============================] - 1s 13us/step - loss: 0.1226 - acc: 0.9669 - val_loss: 0.1563 - val_acc: 0.9642Epoch 19/5054000/54000 [==============================] - 1s 12us/step - loss: 0.1081 - acc: 0.9708 - val_loss: 0.1706 - val_acc: 0.9580Epoch 20/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0998 - acc: 0.9729 - val_loss: 0.1407 - val_acc: 0.9658Epoch 21/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0970 - acc: 0.9733 - val_loss: 0.1428 - val_acc: 0.9653Epoch 22/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0829 - acc: 0.9774 - val_loss: 0.1584 - val_acc: 0.9637Epoch 23/5054000/54000 [==============================] - 1s 15us/step - loss: 0.0851 - acc: 0.9763 - val_loss: 0.1609 - val_acc: 0.9645Epoch 24/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0833 - acc: 0.9766 - val_loss: 0.1462 - val_acc: 0.9682Epoch 25/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0730 - acc: 0.9794 - val_loss: 0.1344 - val_acc: 0.9658Epoch 26/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0781 - acc: 0.9788 - val_loss: 0.1881 - val_acc: 0.9623Epoch 27/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0766 - acc: 0.9786 - val_loss: 0.1654 - val_acc: 0.9615Epoch 28/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0664 - acc: 0.9809 - val_loss: 0.1617 - val_acc: 0.9663Epoch 29/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0661 - acc: 0.9819 - val_loss: 0.1393 - val_acc: 0.9670Epoch 30/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0649 - acc: 0.9814 - val_loss: 0.1492 - val_acc: 0.9655Epoch 31/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0552 - acc: 0.9842 - val_loss: 0.1669 - val_acc: 0.9647Epoch 32/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0523 - acc: 0.9849 - val_loss: 0.1880 - val_acc: 0.9648Epoch 33/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0534 - acc: 0.9852 - val_loss: 0.1748 - val_acc: 0.9652Epoch 34/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0533 - acc: 0.9847 - val_loss: 0.1453 - val_acc: 0.9708Epoch 35/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0528 - acc: 0.9850 - val_loss: 0.1709 - val_acc: 0.9673Epoch 36/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0498 - acc: 0.9856 - val_loss: 0.1492 - val_acc: 0.9653Epoch 37/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0523 - acc: 0.9850 - val_loss: 0.1639 - val_acc: 0.9663Epoch 38/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0467 - acc: 0.9863 - val_loss: 0.1682 - val_acc: 0.9660Epoch 39/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0430 - acc: 0.9876 - val_loss: 0.1630 - val_acc: 0.9657Epoch 40/5054000/54000 [==============================] - 1s 13us/step - loss: 0.0397 - acc: 0.9890 - val_loss: 0.1517 - val_acc: 0.9688Epoch 41/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0450 - acc: 0.9871 - val_loss: 0.1916 - val_acc: 0.9662Epoch 42/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0416 - acc: 0.9877 - val_loss: 0.1888 - val_acc: 0.9640Epoch 43/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0457 - acc: 0.9867 - val_loss: 0.1827 - val_acc: 0.9670Epoch 44/5054000/54000 [==============================] - 1s 15us/step - loss: 0.0375 - acc: 0.9892 - val_loss: 0.1546 - val_acc: 0.9708Epoch 45/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0357 - acc: 0.9897 - val_loss: 0.1752 - val_acc: 0.9673Epoch 46/5054000/54000 [==============================] - 1s 15us/step - loss: 0.0391 - acc: 0.9894 - val_loss: 0.1772 - val_acc: 0.9672Epoch 47/5054000/54000 [==============================] - 1s 15us/step - loss: 0.0386 - acc: 0.9893 - val_loss: 0.1684 - val_acc: 0.9700Epoch 48/5054000/54000 [==============================] - 1s 15us/step - loss: 0.0392 - acc: 0.9886 - val_loss: 0.1530 - val_acc: 0.9723Epoch 49/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0336 - acc: 0.9905 - val_loss: 0.1777 - val_acc: 0.9667Epoch 50/5054000/54000 [==============================] - 1s 14us/step - loss: 0.0315 - acc: 0.9907 - val_loss: 0.1884 - val_acc: 0.9678</code></pre><p><del>현재 저의 GPU는 GTX 750TI를 사용중입니다.<br>에폭당 8초정도의 시간을 보여주고 있네요.</del></p><p>CPU로도 빠른속도를 보일 수 있도록 모델을 변경했습니다.<br>변경된 모델로 학습에 약 1분정도 소요됩니다.  </p><h2 id="정확도-손실-시각화"><a href="#정확도-손실-시각화" class="headerlink" title="정확도, 손실 시각화"></a>정확도, 손실 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history_dict)</span>:</span></span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy and loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="mnist_23_0.png" alt="png"></p><p>첫번째 에폭의 결과가 너무 편차가 심하여 그래프를 알아보기 힘드니 10번째 에폭의 결과부터 그래프로 그려보도록 하겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">10</span>:]), history.history.items())</span><br><span class="line">not_noise_history = dict(list(m))</span><br><span class="line">show_graph(not_noise_history)</span><br></pre></td></tr></table></figure><p><img src="mnist_25_0.png" alt="png"></p><p>대략 20 에폭부터 과대적합이 시작되는걸 확인할 수 있습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(reshape_x_test, y_test)</span><br></pre></td></tr></table></figure><pre><code>10000/10000 [==============================] - 0s 14us/step[0.21370663271014564, 0.9631]</code></pre><p>96%의 정확도를 보여주고 있습니다.<br>레이어를 더 깊고 넓게 구성을 하여 높은 정확도를 만들어보세요.  </p><h1 id="모델-구성-CNN"><a href="#모델-구성-CNN" class="headerlink" title="모델 구성 - CNN"></a>모델 구성 - CNN</h1><p>지금까지 포스팅한 간단한 DNN으로도 MNIST 데이터셋의 분류를 높은 정확도로 만들 수 있습니다.<br>하지만 CNN을 이용하면 적은 파라메터로 좀더 정확한 모델을 구성할 수 있습니다.<br>비전 인식에서 딥러닝이 크게 활약한 이유가 이런곳에 있습니다.<br>간단한 CNN 모델을 구성해보겠습니다.  </p><h2 id="네트워크-구성"><a href="#네트워크-구성" class="headerlink" title="네트워크 구성"></a>네트워크 구성</h2><p>Kernel Initializer로는 유명한 Xavier Initializer를 사용합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># model.add(layers.InputLayer())</span></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>),</span><br><span class="line">    filters=<span class="number">64</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">128</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span></span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 28, 28, 64)        320       _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         _________________________________________________________________conv2d_1 (Conv2D)            (None, 14, 14, 128)       32896     _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         _________________________________________________________________flatten_1 (Flatten)          (None, 6272)              0         _________________________________________________________________dense_3 (Dense)              (None, 10)                62730     =================================================================Total params: 95,946Trainable params: 95,946Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="모델-컴파일-1"><a href="#모델-컴파일-1" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>저희는 레이블 데이터를 원-핫 인코딩을 진행하지 않았기 때문에 Loss 펑션에 sparse_categorical_crossentropy를 지정합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="모델-학습-1"><a href="#모델-학습-1" class="headerlink" title="모델 학습"></a>모델 학습</h2><p>CPU로 돌리시는 경우 시간이 조금 걸릴 수 있으니 커피한잔 하고 오세요~  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 54000 samples, validate on 6000 samplesEpoch 1/2054000/54000 [==============================] - 15s 278us/step - loss: 4.6634 - acc: 0.6814 - val_loss: 0.0709 - val_acc: 0.9788Epoch 2/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0665 - acc: 0.9799 - val_loss: 0.0560 - val_acc: 0.9840Epoch 3/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0472 - acc: 0.9854 - val_loss: 0.0516 - val_acc: 0.985570 - acc: 0Epoch 4/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0372 - acc: 0.9880 - val_loss: 0.0491 - val_acc: 0.9863 loss: 0.0353 - ac - ETAEpoch 5/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0449 - val_acc: 0.9882Epoch 6/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0235 - acc: 0.9919 - val_loss: 0.0557 - val_acc: 0.9848Epoch 7/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0222 - acc: 0.9928 - val_loss: 0.0687 - val_acc: 0.9840Epoch 8/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.0524 - val_acc: 0.9882Epoch 9/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0798 - val_acc: 0.9818Epoch 10/2054000/54000 [==============================] - 9s 161us/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0748 - val_acc: 0.9853Epoch 11/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0734 - val_acc: 0.9860Epoch 12/2054000/54000 [==============================] - 9s 161us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0706 - val_acc: 0.9860Epoch 13/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0928 - val_acc: 0.9842Epoch 14/2054000/54000 [==============================] - 9s 161us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0892 - val_acc: 0.9847Epoch 15/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0949 - val_acc: 0.9847Epoch 16/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0868 - val_acc: 0.9855Epoch 17/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0988 - val_acc: 0.9838Epoch 18/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.1250 - val_acc: 0.9805Epoch 19/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0855 - val_acc: 0.9860Epoch 20/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0942 - val_acc: 0.9865</code></pre><p>제 노트북 CPU를 기준으로 모델을 학습하는데 에폭당 48초정도 걸리네요.<br>20 에폭을 돌렸으니 대략 16분 정도 걸릴거라 보시면 됩니다.  </p><p>저는 GPU를 사용하겠습니다.</p><h2 id="정확도-및-손실-시각화"><a href="#정확도-및-손실-시각화" class="headerlink" title="정확도 및 손실 시각화"></a>정확도 및 손실 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="mnist_38_0.png" alt="png"></p><p>첫 에폭의 결과 때문에 편차가 심하여 <strong>첫 에폭을 제외</strong>하고 보도록 하겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">1</span>:]), history.history.items())</span><br><span class="line">not_noise_history = dict(list(m))</span><br><span class="line">show_graph(not_noise_history)</span><br></pre></td></tr></table></figure><p><img src="mnist_40_0.png" alt="png"></p><p>검증세트의 손실을 보니 8번째 에폭부터 과대적합이 된 것 같군요.<br>과대적합을 해결할 수 있다면 정확도를 높일 수 있을까요 ?</p><h1 id="과대적합-해결"><a href="#과대적합-해결" class="headerlink" title="과대적합 해결"></a>과대적합 해결</h1><p>위와 같은 과대적합을 피하는 방법으로는 여러가지 방법이 있는데요.<br>L1, L2 규제와 같은 정규화를 이용하여 가중치를 감쇠(Weight Decay)시키는 방법과 드랍아웃, 모델의 하이퍼파라메터 튜닝 등으로 과대적합을 피하려 할 수 있습니다.  </p><h2 id="모델의-재구성"><a href="#모델의-재구성" class="headerlink" title="모델의 재구성"></a>모델의 재구성</h2><p>기존 모델은 2단의 컨볼루션 레이어와 풀링 레이어 사용했습니다.<br>모델이 정보를 더 가질 수 있도록 레이어를 더 구성시키고, 드랍아웃과 L2 규제를 걸어 더 복잡한 모델이지만 과대적합은 피할 수 있도록 재구성해보죠.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>),</span><br><span class="line">    filters=<span class="number">32</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">64</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">128</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">256</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>, kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_2 (Conv2D)            (None, 28, 28, 32)        160       _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         _________________________________________________________________dropout (Dropout)            (None, 14, 14, 32)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 14, 14, 64)        8256      _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         _________________________________________________________________dropout_1 (Dropout)          (None, 7, 7, 64)          0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 7, 7, 128)         32896     _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         _________________________________________________________________dropout_2 (Dropout)          (None, 3, 3, 128)         0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 3, 3, 256)         131328    _________________________________________________________________max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         _________________________________________________________________dropout_3 (Dropout)          (None, 1, 1, 256)         0         _________________________________________________________________flatten_2 (Flatten)          (None, 256)               0         _________________________________________________________________dense_4 (Dense)              (None, 10)                2570      =================================================================Total params: 175,210Trainable params: 175,210Non-trainable params: 0_________________________________________________________________</code></pre><p>아까보다 두개의 레이어를 더 추가했고, 파라메터 수가 2배 가까이 늘어난게 보입니다.<br>중간중간 드랍아웃이 적용되었고, 레이어별로 L2 정규화를 추가했습니다.<br>이러한 정규화 기법을 추가하지 않고 학습을 했다면 이전보다 더 빠르게 과대적합이 일어나겠죠?<br>그럼 드랍아웃과 L2규제를 추가한 이후로 모델이 과대적합을 어떻게 견뎌내는지 확인해봅시다.</p><h2 id="모델-컴파일-2"><a href="#모델-컴파일-2" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="학습-시작"><a href="#학습-시작" class="headerlink" title="학습 시작"></a>학습 시작</h2><p>Dropout과 Weight Decay를 추가함으로써 과대적합을 막았고, 뉴런의 학습에 제약을 두었으니, 학습이 더뎌질 수 밖에 없습니다.<br>충분히 학습할 수 있도록 에폭수를 조금 더 늘려 학습을 진행하도록 합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 54000 samples, validate on 6000 samplesEpoch 1/3054000/54000 [==============================] - 11s 199us/step - loss: 16.5374 - acc: 0.0995 - val_loss: 15.8382 - val_acc: 0.0952Epoch 2/3054000/54000 [==============================] - 9s 166us/step - loss: 15.3383 - acc: 0.0990 - val_loss: 15.0753 - val_acc: 0.0952 -Epoch 3/3054000/54000 [==============================] - 9s 165us/step - loss: 11.6865 - acc: 0.1130 - val_loss: 2.4193 - val_acc: 0.3727Epoch 4/3054000/54000 [==============================] - 9s 165us/step - loss: 1.6351 - acc: 0.5584 - val_loss: 0.6810 - val_acc: 0.9327Epoch 5/3054000/54000 [==============================] - 9s 165us/step - loss: 0.9334 - acc: 0.8091 - val_loss: 0.4898 - val_acc: 0.9630 - accEpoch 6/3054000/54000 [==============================] - 9s 164us/step - loss: 0.7393 - acc: 0.8710 - val_loss: 0.4268 - val_acc: 0.9718Epoch 7/3054000/54000 [==============================] - 9s 164us/step - loss: 0.6583 - acc: 0.8908 - val_loss: 0.4011 - val_acc: 0.9752Epoch 8/3054000/54000 [==============================] - 9s 164us/step - loss: 0.6127 - acc: 0.9045 - val_loss: 0.3935 - val_acc: 0.9735Epoch 9/3054000/54000 [==============================] - 9s 164us/step - loss: 0.6006 - acc: 0.9076 - val_loss: 0.3735 - val_acc: 0.9792: 0s - loss: 0.6030 -Epoch 10/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5846 - acc: 0.9099 - val_loss: 0.3665 - val_acc: 0.9803Epoch 11/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5774 - acc: 0.9119 - val_loss: 0.3618 - val_acc: 0.9782Epoch 12/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5714 - acc: 0.9148 - val_loss: 0.3764 - val_acc: 0.9780Epoch 13/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5682 - acc: 0.9154 - val_loss: 0.3707 - val_acc: 0.9763Epoch 14/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5633 - acc: 0.9163 - val_loss: 0.3611 - val_acc: 0.9813Epoch 15/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5628 - acc: 0.9177 - val_loss: 0.3728 - val_acc: 0.9792Epoch 16/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5665 - acc: 0.9181 - val_loss: 0.3630 - val_acc: 0.9820Epoch 17/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5650 - acc: 0.9168 - val_loss: 0.3526 - val_acc: 0.9835 - ETA: 3s - losEpoch 18/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5632 - acc: 0.9172 - val_loss: 0.3584 - val_acc: 0.9823Epoch 19/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5597 - acc: 0.9176 - val_loss: 0.3576 - val_acc: 0.9818Epoch 20/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5548 - acc: 0.9193 - val_loss: 0.3577 - val_acc: 0.9813Epoch 21/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5558 - acc: 0.9202 - val_loss: 0.3812 - val_acc: 0.9758Epoch 22/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5601 - acc: 0.9179 - val_loss: 0.3601 - val_acc: 0.9830Epoch 23/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5571 - acc: 0.9189 - val_loss: 0.3596 - val_acc: 0.9818Epoch 24/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5495 - acc: 0.9200 - val_loss: 0.3568 - val_acc: 0.9817Epoch 25/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5490 - acc: 0.9202 - val_loss: 0.3602 - val_acc: 0.9820Epoch 26/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5529 - acc: 0.9205 - val_loss: 0.3563 - val_acc: 0.9822Epoch 27/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5538 - acc: 0.9198 - val_loss: 0.3534 - val_acc: 0.9840Epoch 28/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5492 - acc: 0.9204 - val_loss: 0.3573 - val_acc: 0.9812Epoch 29/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5530 - acc: 0.9198 - val_loss: 0.3508 - val_acc: 0.9848Epoch 30/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5521 - acc: 0.9198 - val_loss: 0.3648 - val_acc: 0.9792</code></pre><h2 id="모델-평가"><a href="#모델-평가" class="headerlink" title="모델 평가"></a>모델 평가</h2><p>정규화 기법을 적용한 학습 결과가 어떻게 변했는지 확인 해봅시다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="mnist_51_0.png" alt="png"></p><p>가중치의 급격환 변화를 L2 규제를 통해 막아두어 처음 2에폭까지는 큰 변화가 없다가 3~5 에폭이 지나서 모델이 높은 성능에 근접해지는걸 확인할 수 있습니다.<br>그럼 조금 더 자세하게 볼 수 있도록 첫 5에폭까지의 데이터는 제외하고 이후의 데이터를 그래프로 확인해보겠습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">5</span>:]), history.history.items())</span><br><span class="line">not_noise_history = dict(list(m))</span><br><span class="line">show_graph(not_noise_history)</span><br></pre></td></tr></table></figure><p><img src="mnist_53_0.png" alt="png"></p><p>트레이닝 세트의 정확도가 낮게 측정되는건 드랍아웃 때문입니다.<br>학습마다 몇몇의 특정 뉴런을 비활성화 시키기 때문에 정확도가 낮아지게 됩니다.<br>훈련을 마친 이후 예측시에는 드랍아웃 시키는 뉴런의 수를 0%로 맞추고 사용합니다.<br>(모든 뉴런을 사용)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>60000/60000 [==============================] - 6s 103us/step[0.38104470246632893, 0.9750666666666666]</code></pre><p>드랍아웃을 해제한 후 훈련세트의 손실값과 정확도입니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(</span><br><span class="line">    reshape_x_test,</span><br><span class="line">    y_test</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>10000/10000 [==============================] - 1s 98us/step[0.3756561163902283, 0.9753]</code></pre><p>테스트 세트의 손실과 정확도입니다.<br>98%의 정확도를 보여주고 있습니다.<br>모델의 네트워크를 변경하여 99% 정확도에 도전해보세요.</p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/17/mnist/#disqus_thread</comments>
    </item>
    
    <item>
      <title>AWS로 GPU기반의 딥러닝 학습환경 구축하기</title>
      <link>https://codingcrews.github.io/2019/01/15/deeplearning-gpu/</link>
      <guid>https://codingcrews.github.io/2019/01/15/deeplearning-gpu/</guid>
      <pubDate>Tue, 15 Jan 2019 07:42:51 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기&quot;&gt;&lt;a href=&quot;#Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기&quot; class=&quot;headerlink&quot; title=&quot;Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기&quot;&gt;&lt;/a
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기"><a href="#Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기" class="headerlink" title="Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기"></a>Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기</h1><p>안녕하세요. 오늘은 아마존 웹 서비스(이하 AWS)를 이용하여 GPU 인스턴스를 이용한 딥러닝 학습환경 만들기에 대해 알아봅시다.  </p><p>딥러닝이란 최근 핫해진 뉴럴넷 기반의 기계학습 기법을 말하는데요.<br>학습 시 굉장히 많은 연산을 필요로 하여 학습에 소요되는 시간이 많이 필요합니다.<br>단순한 연산을 많이 하기 때문에 병렬처리에 특화된 GPU를 사용하여 학습시간을 단축시킬 수 있는데요.<br>당연히 좋은 GPU를 사용할수록 짧은 학습시간이 필요하겠지요.<br>하지만 문제는 이 GPU 가격이 개인이 사서 사용하기엔 너무나도 비싸다고 느낄 수 있습니다.  </p><p><img src="v100.png" alt="Tesla V100 가격">  </p><p><img src="k80.png" alt="Tesla K80 가격"></p><p>위 사진은 GPU 연산에 특화되어 나온 모델들입니다.<br>저희가 사용할 AWS EC2 P2, P3 인스턴스에 제공되는 GPU 모델들입니다.  </p><p>포스팅 날짜를 기준으로 최소 200만원 이상의 가격을 보여주고 있는데요.<br>개인이라도 사지 못할 가격은 아닙니다만, 딥러닝이 무엇인지 이제 알아가는 시점에 입문자가 구매하기엔 굉장히 벅찬 금액이죠.<br>하지만 AWS에서 제공해주는 GPU인스턴스를 빌려서 시간제로 비용을 낼 수 있다면, 비싼 GPU를 구매하지 않고도 필요할 때에만 성능이 좋은 GPU를 이용할 수 있겠죠 ?  </p><h1 id="Aws-EC2"><a href="#Aws-EC2" class="headerlink" title="Aws EC2"></a>Aws EC2</h1><p>EC2에 관한 설명은 생략하도록 하고, <a href="https://aws.amazon.com/ko/ec2/" target="_blank" rel="noopener">여기</a>에서 자세하게 확인 가능합니다. </p><h2 id="인스턴스-종류-및-가격"><a href="#인스턴스-종류-및-가격" class="headerlink" title="인스턴스 종류 및 가격"></a>인스턴스 종류 및 가격</h2><p>저희는 GPU 인스턴스를 사용할 예정이기 때문에 P, G 두가지 계열을 확인하면 됩니다.<br><a href="https://aws.amazon.com/ko/ec2/instance-types/" target="_blank" rel="noopener">인스턴스 스펙</a>으로 이동하여 자세한 스펙을 확인 가능합니다.<br>각 인스턴스의 가격은 시간에 비례하여 비용이 청구되는 방식이며, 각 리전마다 사용가격의 편차가 있습니다.  </p><p>서울리전은 해외리전보다 비싼편이니 (약 2배가량 비쌈), 연구목적으로 사용하실 때에는 해외리전을 이용하시는걸 추천드립니다.  </p><p><img src="ohio.png" alt="오하이오 리전의 가격"><br><img src="seoul.png" alt="서울 리전의 가격">  </p><p>위 사진은 오하이오 리전의 가격이며, 아래 사진은 서울리전의 가격입니다.<br>같은 p2.xlarge의 가격을 보더라도 오하이오 리전은 시간당 \$0.9, 서울리전은 시간당 \$1.465의 가격을 보여줍니다.<br>그리고, 서울리전은 g3계열의 인스턴스가 제공되지 않고 있습니다.  </p><p>오하이오 리전의 p2.xlarge 가격을 보면 대략 시간당 1,000원 정도의 가격을 보여줍니다.<br>요새 PC방 가격도 시간당 1,000원이 넘는걸 생각하면 그리 비싸다고 생각하진 않을 수 있습니다.  </p><h2 id="스팟-인스턴스"><a href="#스팟-인스턴스" class="headerlink" title="스팟 인스턴스"></a>스팟 인스턴스</h2><p>하지만 위의 가격은 온디맨드 계약의 가격이며, AWS에서 더 할인을 받을 수 있는 방법이 존재합니다.<br>그건 바로 R.I(예약 인스턴스) 혹은 Spot인스턴스를 이용하는 방법입니다.<br>R.I의 경우는 최소 계약단위가 1년 단위이기 때문에 개인이 하기엔 부담스럽고, 오히려 서비스를 운영중인 회사에서 GPU인스턴스가 필요할 때 사용할 수 있겠습니다.  </p><p>스팟 인스턴스의 경우는 인스턴스를 경매입찰방식으로 할당받는 개념인데, 최대 90%까지 가격이 할인될 수 있습니다.  </p><p><img src="ohio-spot.png" alt="오하이오 리전의 스팟 인스턴스 가격"><br><img src="seoul-spot.png" alt="서울 리전의 스팟 인스턴스 가격"></p><p>온디맨드의 가격보다 더 저렴한 가격을 확인할 수 있습니다.<br>이 스팟 인스턴스를 이용해보도록 하겠습니다.  </p><h2 id="스팟-인스턴스-생성"><a href="#스팟-인스턴스-생성" class="headerlink" title="스팟 인스턴스 생성"></a>스팟 인스턴스 생성</h2><p><img src="aws-1.png" alt="스팟인스턴스 생성 - 1"></p><ol><li>AWS 관리 콘솔에서 스팟 요청을 눌러줍니다.</li></ol><p><img src="aws-2.png" alt="스팟인스턴스 생성 - 2"></p><ol start="2"><li>자주 쓰이는 형태의 패턴들이 이미 만들어져 있으나, 명시한 시간동안 인스턴스를 사용할 수 있도록 시간을 지정하여 사용합니다.  <blockquote><p>(최소 1시간에서 최대 6시간까지 사용 가능합니다.)</p></blockquote></li></ol><p><img src="aws-3.png" alt="스팟인스턴스 생성 - 3"></p><ol start="3"><li>인스턴스 타입 변경을 눌러 사용할 인스턴스의 타입을 지정합니다.<blockquote><p>저는 GPU compute에서 p2.xlarge를 이용하겠습니다.<br>GPU instance에서 조금 더 저렴한 G3 계열의 인스턴스도 사용 가능합니다.  </p></blockquote></li></ol><p><img src="aws-4.png" alt="스팟인스턴스 생성 - 4"></p><ol start="4"><li>AMI 검색을 눌러 사용할 기본 AMI를 지정합니다.  <blockquote><p>딥러닝 프레임워크를 사용하여 GPU를 사용하기 위해선 GPU 설정이 많이 필요한데, 이러한 사전 작업을 마친 템플릿을 AWS에서 공식 AMI로 제공합니다.<br>꼭 소유자를 확인하여 아마존 공식 이미지가 맞는지 확인합니다.<br>(일반 유저도 AMI를 만들어 배포할 수 있어 마이닝 프로그램을 설치해둔 템플릿들이 다수 존재하니 주의합시다.)  </p></blockquote></li></ol><p><img src="aws-5.png" alt="스팟인스턴스 생성 - 5"></p><ol start="5"><li>Additional configurations를 눌러 시큐리티 그룹 등 인스턴스에 필요한 설정을 마쳐줍시다.  </li></ol><p><img src="aws-6.png" alt="스팟인스턴스 생성 - 6"></p><ol start="6"><li>스팟 인스턴스가 정상적으로 생성되었습니다.<blockquote><p>간혹 해외리전의 스팟 인스턴스 제한이 걸려있어 생성되지 않는 경우가 존재합니다.<br>이때는 AWS에 케이스를 오픈하여 리밋제한을 상향요청 할 수 있습니다.</p></blockquote></li></ol><p><img src="aws-7.png" alt="스팟인스턴스 생성 - 7"></p><ol start="7"><li>생성된 인스턴스 확인<blockquote><p>생성된 인스턴스의 유형과 최대 가격을 볼 수 있습니다.<br>최대가격이란 명시된 시간동안 다른 인스턴스들이 높은 입찰가를 제시할 때 인스턴스가 종료 될 수 있으니 최대 가격이 온디맨드 가격으로 자동적으로 조절됩니다.  </p></blockquote></li></ol><h2 id="영구적으로-사용할-볼륨-생성하기"><a href="#영구적으로-사용할-볼륨-생성하기" class="headerlink" title="영구적으로 사용할 볼륨 생성하기"></a>영구적으로 사용할 볼륨 생성하기</h2><p>스팟 인스턴스가 종료될 시 75GB로 할당된 AMI 볼륨이 자동적으로 삭제됩니다.<br>인스턴스 생성 시 해당 볼륨을 삭제하지 않을 수 있으나, 75GB의 볼륨을 갖고 있기는 부담스럽기 때문에 우리가 필요한 데이터들만 저장할 수 있는 작은 크기의 볼륨을 따로 생성하여 인스턴스의 종료와 무관하게 사용할 수 있도록 새로운 볼륨을 만들어 추가해봅시다.  </p><p><img src="aws-8.png" alt="스팟인스턴스 생성 - 8"></p><ol start="8"><li>데이터 영구보존 볼륨 생성하기<blockquote><p>좌측의 볼륨 메뉴를 눌러 현재 인스턴스의 가용영역을 확인 한 이후,<br>볼륨 생성을 눌러줍니다.  </p></blockquote></li></ol><p><img src="aws-9.png" alt="스팟인스턴스 생성 - 9"></p><ol start="9"><li>바로 이전에 확인한 가용영역을 맞춰 새로운 볼륨을 생성합니다.<blockquote><p>가용영역이 다르다면 볼륨이 인스턴스에 사용할 수 없으니 꼭 가용영역을 맞춰 생성합니다.  </p></blockquote></li></ol><p><img src="aws-10.png" alt="스팟인스턴스 생성 - 10"></p><ol start="10"><li>생성된 볼륨을 확인할 수 있습니다. </li></ol><p><img src="aws-11.png" alt="스팟인스턴스 생성 - 11"></p><ol start="11"><li>우클릭을 이용해 볼륨 연결을 눌러줍니다.  </li></ol><p><img src="aws-12.png" alt="스팟인스턴스 생성 - 12"></p><ol start="12"><li>볼륨을 연결합니다.  <blockquote><p>인스턴스를 눌러주면 자동적으로 같은 가용영역에 있는 사용중인 인스턴스 목록이 나옵니다.<br>디바이스는 해당 인스턴스에 사용될 디바이스명입니다.  </p></blockquote></li></ol><p><strong>여기까지가 콘솔에서 설정 가능한 스팟 인스턴스에 대한 인스턴스 생성과 설정입니다.</strong></p><p>이후는 AWS 인스턴스에 ssh접속을 한 이후 설정하는 부분입니다.</p><h2 id="SSH-접속"><a href="#SSH-접속" class="headerlink" title="SSH 접속"></a>SSH 접속</h2><p>SSH 로 생성한 스팟 인스턴스에 접근하면 초기 화면이 이렇게 보이는걸 확인할 수 있습니다.<br>각각의 가상환경에 진입할 수 있는 명령어와 사용가능한 환경에 대한 설명이 보입니다.  </p><pre><code>=============================================================================       __|  __|_  )       _|  (     /   Deep Learning AMI (Amazon Linux) Version 20.0      ___|\___|___|=============================================================================Please use one of the following commands to start the required environment with the framework of your choice:for MXNet(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p36for MXNet(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p27for MXNet(+Amazon Elastic Inference) with Python3 _______________________________________ source activate amazonei_mxnet_p36for MXNet(+Amazon Elastic Inference) with Python2 _______________________________________ source activate amazonei_mxnet_p27for TensorFlow(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p36for TensorFlow(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p27for Tensorflow(+Amazon Elastic Inference) with Python2 _____________________________ source activate amazonei_tensorflow_p27for Theano(+Keras2) with Python3 (CUDA 9.0) _____________________________________________________ source activate theano_p36for Theano(+Keras2) with Python2 (CUDA 9.0) _____________________________________________________ source activate theano_p27for PyTorch with Python3 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p36for PyTorch with Python2 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p27for CNTK(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p36for CNTK(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p27for Caffe2 with Python2 (CUDA 9.0) ______________________________________________________________ source activate caffe2_p27for Caffe with Python2 (CUDA 8.0) ________________________________________________________________ source activate caffe_p27for Caffe with Python3 (CUDA 8.0) ________________________________________________________________ source activate caffe_p35for Chainer with Python2 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p27for Chainer with Python3 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p36for base Python2 (CUDA 9.0) ________________________________________________________________________ source activate python2for base Python3 (CUDA 9.0) ________________________________________________________________________ source activate python3Official Conda User Guide: https://conda.io/docs/user-guide/index.htmlAWS Deep Learning AMI Homepage: https://aws.amazon.com/machine-learning/amis/Developer Guide and Release Notes: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.htmlSupport: https://forums.aws.amazon.com/forum.jspa?forumID=263For a fully managed experience, check out Amazon SageMaker at https://aws.amazon.com/sagemaker=============================================================================</code></pre><h3 id="파이썬-가상환경-진입"><a href="#파이썬-가상환경-진입" class="headerlink" title="파이썬 가상환경 진입"></a>파이썬 가상환경 진입</h3><p>저희는 Python 3.6기반의 Tensorflow 사용할 수 있는 가상환경으로 진입해보겠습니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ec2-user@ip-172-31-15-213 ~]$ <span class="built_in">source</span> activate tensorflow_p36</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파이썬 가상환경으로 진입하면 쉘이 아래와 같이 변합니다.  </span></span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$</span><br></pre></td></tr></table></figure></p><h2 id="연결한-볼륨-확인"><a href="#연결한-볼륨-확인" class="headerlink" title="연결한 볼륨 확인"></a>연결한 볼륨 확인</h2><p>생성하고 나서 연결한 볼륨에 대한 정보를 확인합니다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ lsblk</span><br></pre></td></tr></table></figure><pre><code>NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINTxvda    202:0    0  75G  0 disk└─xvda1 202:1    0  75G  0 part /xvdf    202:80   0   8G  0 disk</code></pre><p>아까 연결한 8GB의 볼륨이 xvdf로 연결되어있는걸 확인할 수 있습니다.</p><h3 id="볼륨의-포맷-확인"><a href="#볼륨의-포맷-확인" class="headerlink" title="볼륨의 포맷 확인"></a>볼륨의 포맷 확인</h3><p>연결된 xvdf 디바이스를 통해 볼륨의 포맷을 확인합니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf</span><br></pre></td></tr></table></figure></p><pre><code>/dev/xvdf: data</code></pre><p>갓 생성한 볼륨의 경우 data로 나오게 될 것이고, 이 볼륨은 사용하고 있는 os 파일 시스템에 맞춰 파일 포맷을 진행해줘야 합니다.</p><h3 id="볼륨-ext4-포맷"><a href="#볼륨-ext4-포맷" class="headerlink" title="볼륨 ext4 포맷"></a>볼륨 ext4 포맷</h3><p>볼륨의 파일시스템을 ext4 포맷으로 변경합니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mkfs -t ext4 /dev/xvdf</span><br></pre></td></tr></table></figure></p><pre><code>mke2fs 1.43.5 (04-Aug-2017)Creating filesystem with 2097152 4k blocks and 524288 inodesFilesystem UUID: 6ce22348-395a-4ac1-8df7-e180aaadaadfSuperblock backups stored on blocks:  32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632Allocating group tables: doneWriting inode tables: doneCreating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done</code></pre><p>​    </p><h3 id="볼륨-포맷-재확인"><a href="#볼륨-포맷-재확인" class="headerlink" title="볼륨 포맷 재확인"></a>볼륨 포맷 재확인</h3><p>파일 시스템 포맷이 완료 된 후 ext4 포맷으로 변경됬는지 확인합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf</span><br></pre></td></tr></table></figure><pre><code>/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf (extents) (64bit) (large files) (huge files)</code></pre><blockquote><p>결과에 나오는 <strong>UUID</strong>를 잘 기억하세요.</p></blockquote><h2 id="볼륨-마운트"><a href="#볼륨-마운트" class="headerlink" title="볼륨 마운트"></a>볼륨 마운트</h2><p>포맷 완료 된 볼륨을 사용하기 위해서는 마운트 과정이 필요합니다.<br>현재 Home 디렉터리에 새로운 디렉터리를 만들어 마운트를 진행합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ mkdir mount_dir</span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount /dev/xvdf mount_dir/</span><br></pre></td></tr></table></figure><blockquote><p>정상적으로 마운트 되었다면 에러 메세지 없이 완료됩니다.</p></blockquote><h3 id="볼륨-자동-마운트"><a href="#볼륨-자동-마운트" class="headerlink" title="볼륨 자동 마운트"></a>볼륨 자동 마운트</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /home/ubuntu ubutnu</span><br><span class="line">mount /dev/xvdf ~/smount</span><br></pre></td></tr></table></figure><p>위와 같이 수동으로 마운트 시켜 사용하는 경우 인스턴스가 재시동 되는 경우 마운트를 다시 해줘야 합니다.<br>/etc/fstab 이라는 폴더는 부팅 시 마운트 해야할 목록을 가지고 있어 자동으로 마운트가 진행되도록 합니다.<br>fstab 파일이 잘못 작성 되어 있으면 부팅이 안되는 경우도 발생하니 백업을 만들고, 신중하게 파일을 수정하도록 합시다.  </p><p><strong>아래 명령어는 복붙하지마시고 본인의 UUID와 마운트 할 디렉터리에 맞춰 변경하여 사용하세요.</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo cp /etc/fstab /etc/fstab.orig  </span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ <span class="built_in">echo</span> <span class="string">"UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf       /home/ec2-user/mount_dir   ext4    defaults,nofail        0       2"</span> | sudo tee --apend /etc/fstab</span><br><span class="line"></span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount -a</span><br></pre></td></tr></table></figure></p><blockquote><p>마지막 mount -a 옵션을 꼭 실행해보시고, 정상적이라면 아무 메시지 없이 완료됩니다.<br>에러 메세지가 발생한 경우 fstab 파일에 문제가 있을 수 있으니 꼭 확인하세요.</p></blockquote><h2 id="Jupyter-Notebook-실행"><a href="#Jupyter-Notebook-실행" class="headerlink" title="Jupyter Notebook 실행"></a>Jupyter Notebook 실행</h2><p>원격에서 접속하기 위해 IP 대역을 전부 풀어줍니다.<br>Port 번호는 기본적으로 8888 포트를 사용하니 시큐리티 그룹에서 해당 포트를 허용시켜주시고, 다른 포트번호로 사용하셔도 무방합니다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ jupyter notebook --ip 0.0.0.0</span><br></pre></td></tr></table></figure><pre><code>[I 06:43:24.447 NotebookApp] Using EnvironmentKernelSpecManager...[I 06:43:24.448 NotebookApp] Started periodic updates of the kernel list (every 3 minutes).[I 06:43:24.552 NotebookApp] Writing notebook server cookie secret to /home/ec2-user/.local/share/jupyter/runtime/notebook_cookie_secret[I 06:43:27.661 NotebookApp] Loading IPython parallel extension[I 06:43:27.784 NotebookApp] JupyterLab beta preview extension loaded from /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/jupyterlab[I 06:43:27.784 NotebookApp] JupyterLab application directory is /home/ec2-user/anaconda3/envs/tensorflow_p36/share/jupyter/lab[I 06:43:28.405 NotebookApp] [nb_conda] enabled[I 06:43:28.408 NotebookApp] Serving notebooks from local directory: /home/ec2-user[I 06:43:28.408 NotebookApp] 0 active kernels[I 06:43:28.408 NotebookApp] The Jupyter Notebook is running at:[I 06:43:28.408 NotebookApp] http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d[I 06:43:28.408 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 06:43:28.408 NotebookApp] No web browser found: could not locate runnable browser.[C 06:43:28.408 NotebookApp]  Copy/paste this URL into your browser when you connect for the first time,  to login with a token:      http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d&amp;token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d[I 06:43:28.409 NotebookApp] Starting initial scan of virtual environments...</code></pre><blockquote><p>ec2 인스턴스의 public ip 주소가 만약 1.10.10.10 이라고 가정하면,<br><a href="http://1.10.10.10:8888" target="_blank" rel="noopener">http://1.10.10.10:8888</a> 로 접속하시면 위에 출력된 해당 토큰을 요청합니다.<br>토큰을 입력해주면 쥬피터 노트북을 사용하실 수 있습니다.</p></blockquote>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/15/deeplearning-gpu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝으로 보스턴의 집값을 예측해보자.</title>
      <link>https://codingcrews.github.io/2019/01/15/boston_linear_regression/</link>
      <guid>https://codingcrews.github.io/2019/01/15/boston_linear_regression/</guid>
      <pubDate>Mon, 14 Jan 2019 18:06:54 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;선형-회귀-문제-Linear-Regression&quot;&gt;&lt;a href=&quot;#선형-회귀-문제-Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;선형 회귀 문제 : Linear Regression&quot;&gt;&lt;/a&gt;선형 회귀 
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="선형-회귀-문제-Linear-Regression"><a href="#선형-회귀-문제-Linear-Regression" class="headerlink" title="선형 회귀 문제 : Linear Regression"></a>선형 회귀 문제 : Linear Regression</h1><p>머신러닝으로 해결할 수 있는 문제 중 분류 문제들을 이전 포스팅에서 다뤄보았고,<br>이번에는 연속된 데이터를 예측해야하는 회귀 문제를 보겠습니다.  </p><p>이전의 포스팅은 데이터가 주어졌을 때 리뷰의 긍정/부정 분류, 뉴스 기사의 토픽 분류 등의 문제를 다뤘었고,<br>회귀 문제란 데이터가 주어졌을 때 주택 가격을 예측하는것처럼 연속된 값을 예측하는걸 회귀 문제라고 합니다.  </p><p>오늘은 선형 회귀 모델을 이용하여 1970년대 보스턴의 집값을 예측하는 모델을 만들어 보겠습니다.  </p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>사용하는 데이터셋은 카네기 멜런 대학교에서 제공하는 데이터셋으로 데이터의 설명은 <a href="http://lib.stat.cmu.edu/datasets/boston" target="_blank" rel="noopener">여기</a>에서 확인할 수 있습니다.</p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br></pre></td></tr></table></figure><h2 id="Dataset-shape-확인"><a href="#Dataset-shape-확인" class="headerlink" title="Dataset shape 확인"></a>Dataset shape 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train Datas Shape : &#123;&#125;"</span>.format(train_data.shape))</span><br><span class="line">print(<span class="string">"Train Labels Shape : &#123;&#125;"</span>.format(train_targets.shape))</span><br></pre></td></tr></table></figure><pre><code>Train Datas Shape : (404, 13)Train Labels Shape : (404,)</code></pre><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><p>각 컬럼별로 해당하는 데이터들을 확인할 수 있고, 레이블에는 해당 주택의 가격값이 들어가있는걸 확인할 수 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display(train_data[<span class="number">0</span>])</span><br><span class="line">display(train_targets[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,        18.72   ])array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4])</code></pre><h2 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h2><p>신경망 학습 시 차이가 큰 값을 신경망에 주입하면 학습이 잘 되지 않습니다.<br>(0.01, 0.1)과 (1, 10)을 보면 비율대로만 보게되면 10배의 차이를 가지는걸 볼 수 있지만,<br>실수 체계에서 값의 차이는 어마어마하게 큰 차이로 볼 수 있습니다.<br>(1, 10)과 같은 데이터를 Sparse 하다라고 표현하며,<br>(0.01, 0.1)과 같은 밀도가 높은 데이터를 Dense 데이터라고 표현합니다.  </p><p>이렇게 Dense 데이터로 변경하는 과정이 필요한데, 이 부분을 Scaling 혹은 일반화 한다고 합니다.  </p><p>데이터의 일반화를 위해서 각 특성별 평균값을 뺀 이후에 표준편차로 나눠주게 되면,<br>특성의 중앙이 0으로, 표준편차는 1인 정규분포가 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">test_data -= mean</span><br><span class="line">test_data /= std</span><br></pre></td></tr></table></figure><h1 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h1><p>데이터셋이 작으므로 간단한 모델을 구성하면 과대적합을 피할 수 있습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_network</span><span class="params">(input_shape=<span class="params">(<span class="number">0</span>,)</span>)</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=input_shape))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'mse'</span>, metrics=[<span class="string">'mae'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>모델 구성의 마지막 Dense layer를 보면 활성함수가 지정 되어있지 않은걸 확인할 수 있는데,<br>우리가 필요한 데이터는 선형 데이터를 원하기 때문에 활성함수를 지정하지 않으면 스칼라 회귀를 위한 구성이 완성됩니다.<br>반대로 sigmoid 같은 활성화 함수를 적용하면 네트워크가 0~1 사이의 값을 만들도록 만듭니다.  </p><p>우리가 필요한 데이터는 주택가격을 예측하기 위함으로 활성함수를 지정하지 않고 스칼라 회귀 값을 예측하도록 구성합니다.  </p><h2 id="K-Fold-Validation"><a href="#K-Fold-Validation" class="headerlink" title="K-Fold Validation"></a>K-Fold Validation</h2><p>데이터셋의 크기가 매우 작기 때문에 검증셋(Validation)의 크기도 매우 작아지게 되는데,<br>이때 데이터셋이 적어지면서 훈련세트 중 어느 한 특정 부분이 훈련세트로 사용되는지에 따라 모델의 정확도가 크게 달라질 수 있습니다.<br>이 이유는 훈련세트와 검증세트로 나눴을 때 각 값들의 분포도가 고르게 되지 못할 경우가 생기기 때문입니다.  </p><p>이런 상황에서 가장 좋은 방법은 K-겹 교차 검증(K-Fold Cross Validation)을 실시하는것인데,<br>데이터를 K개의 분할로 나누고 각각 K개의 모델을 만들어 K-1 개의 분할에서 훈련하고 나머지 분할된 데이터에서 평가를 하게됩니다.<br>이 점수는 각 검증 데이터셋의 평균으로 모델을 평가하게 됩니다.  </p><p>즉, 여러 폴드의 데이터셋으로 나누어 교차검증을 함으로써 데이터 분포에 신경쓰지 않고 훈련세트의 모든 부분을 사용해 모델을 평가할 수 있는 장점이 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line">num_epochs = <span class="number">150</span></span><br><span class="line">all_scores = []</span><br><span class="line">all_history = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'폴드 번호 #&#123;&#125;'</span>.format(i))</span><br><span class="line">    fold_start_index = i * num_val_samples</span><br><span class="line">    fold_end_index = (i + <span class="number">1</span>) * num_val_samples</span><br><span class="line">    </span><br><span class="line">    val_data = train_data[fold_start_index : fold_end_index]</span><br><span class="line">    val_targets = train_targets[fold_start_index : fold_end_index]</span><br><span class="line">    </span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:fold_start_index], train_data[fold_end_index:]], </span><br><span class="line">        axis=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:fold_start_index], train_targets[fold_end_index:]],</span><br><span class="line">        axis=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    model = build_network((partial_train_data.shape[<span class="number">1</span>], ))</span><br><span class="line">    history = model.fit(</span><br><span class="line">        partial_train_data,</span><br><span class="line">        partial_train_targets,</span><br><span class="line">        epochs=num_epochs, </span><br><span class="line">        validation_data=(val_data, val_targets),</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        verbose=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="number">0</span>)</span><br><span class="line">    all_scores.append(val_mse)</span><br><span class="line">    all_history.append(history.history)</span><br></pre></td></tr></table></figure><pre><code>폴드 번호 #0폴드 번호 #1폴드 번호 #2폴드 번호 #3</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val_mae_lst = map(<span class="keyword">lambda</span> x: x[<span class="string">'val_mean_absolute_error'</span>], all_history)</span><br><span class="line">val_mae_lst = np.array(list(val_mae_lst))</span><br><span class="line">avg_mae = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> val_mae_lst]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h2 id="검증-데이터-시각화"><a href="#검증-데이터-시각화" class="headerlink" title="검증 데이터 시각화"></a>검증 데이터 시각화</h2><p>검증 데이터 시각화를 위해 history 데이터를 에폭별 평균 데이터로 치환시킵니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val_mae_lst = map(<span class="keyword">lambda</span> x: x[<span class="string">'val_mean_absolute_error'</span>], all_history)</span><br><span class="line">val_mae_lst = np.array(list(val_mae_lst))</span><br><span class="line">avg_mae = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> val_mae_lst]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>그래프 변동이 심하지 않도록 이전 포인트의 지수 이동 평균값으로 변경시켜주는 함수를 만듭니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">.9</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(data)</span>:</span></span><br><span class="line">    smooth_data = smooth_curve(data)</span><br><span class="line">    plt.plot(range(<span class="number">1</span>, len(smooth_data) + <span class="number">1</span>), smooth_data)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(avg_mae[<span class="number">10</span>:])</span><br></pre></td></tr></table></figure><p><img src="boston_linear_regression_21_0.png" alt="png"></p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/15/boston_linear_regression/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝으로 뉴스기사의 토픽을 분류해보자.</title>
      <link>https://codingcrews.github.io/2019/01/14/reuter_multi_classification/</link>
      <guid>https://codingcrews.github.io/2019/01/14/reuter_multi_classification/</guid>
      <pubDate>Mon, 14 Jan 2019 14:27:59 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;다중-분류-문제-Multiple-class-classification&quot;&gt;&lt;a href=&quot;#다중-분류-문제-Multiple-class-classification&quot; class=&quot;headerlink&quot; title=&quot;다중 분류 문제 : Multi
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="다중-분류-문제-Multiple-class-classification"><a href="#다중-분류-문제-Multiple-class-classification" class="headerlink" title="다중 분류 문제 : Multiple class classification"></a>다중 분류 문제 : Multiple class classification</h1><p>이전 포스팅에서 다룬 이진 분류 모델의 경우 예측할 수 있는 결과값이 긍정/부정 두가지 였습니다.<br>하지만 뉴스의 토픽(예를들어 시사/인터넷/연예/스포츠 등)을 예측하기 위해선 이진 분류로는 토픽을 나눌 수 없습니다.  </p><p>이번 포스팅에서는 2개 이상의 클래스를 가진 경우 사용할 수 있는 다중 분류 문제를 해결해보도록 하겠습니다.  </p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>저희는 로이터 데이터셋을 사용하도록 합니다.<br>해당 데이터셋은 46개의 토픽을 갖고 있으며 각 뉴스기사마다 하나의 토픽이 정해져있습니다.<br>즉, 단일 레이블 다중분류 문제로 볼 수 있습니다.  </p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="Dataset-shape-확인"><a href="#Dataset-shape-확인" class="headerlink" title="Dataset shape 확인"></a>Dataset shape 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train Datas Shape : &#123;&#125;"</span>.format(train_data.shape))</span><br><span class="line">print(<span class="string">"Train Labels Shape : &#123;&#125;"</span>.format(train_labels.shape))</span><br></pre></td></tr></table></figure><pre><code>Train Datas Shape : (8982,)Train Labels Shape : (8982,)</code></pre><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><p>확인한대로 8982개의 훈련 데이터셋이 존재하며,<br>각 인덱스는 단어 인덱스의 리스트를 뜻 합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">display(train_data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line">display(train_labels)</span><br><span class="line"></span><br><span class="line">display(test_labels)</span><br></pre></td></tr></table></figure><pre><code>[1, 2, 2, 8, 43, 10, 447, 5, 25, 207]array([ 3,  4,  3, ..., 25,  3, 25])array([ 3, 10,  1, ...,  3,  3, 24])</code></pre><h3 id="단어를-텍스트로-디코딩"><a href="#단어를-텍스트로-디코딩" class="headerlink" title="단어를 텍스트로 디코딩"></a>단어를 텍스트로 디코딩</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_index = reuters.get_word_index()</span><br><span class="line">reverse_word_index = dict([(val, key) <span class="keyword">for</span> (key, val) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_news = <span class="string">' '</span>.join([reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</span><br><span class="line">print(decoded_news)</span><br></pre></td></tr></table></figure><pre><code>? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3</code></pre><h3 id="라벨-확인"><a href="#라벨-확인" class="headerlink" title="라벨 확인"></a>라벨 확인</h3><p>샘플의 라벨은 토픽의 인덱스로써 0~45의 값을 가집니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>3</code></pre><h3 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h3><p>학습에 용이하도록 뉴스 기사와 라벨 데이터를 벡터로 변환시킵니다.<br>학습에 사용되는 데이터셋의 인풋 데이터는 해당 뉴스 기사의 들어가있는 단어 인덱스를 1.0 으로 변경시킵니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension)) <span class="comment"># 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br><span class="line"></span><br><span class="line">display(x_train.shape)</span><br><span class="line">display(x_test.shape)</span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br><span class="line"></span><br><span class="line">display(one_hot_train_labels.shape)</span><br><span class="line">display(one_hot_test_labels.shape)</span><br></pre></td></tr></table></figure><pre><code>(8982, 10000)(2246, 10000)(8982, 46)(2246, 46)</code></pre><h1 id="신경망-구성"><a href="#신경망-구성" class="headerlink" title="신경망 구성"></a>신경망 구성</h1><p>단일 레이블 다중 분류를 위한 모델을 작성 해보도록 하겠습니다.<br>다만 레이어 구축 시 참고해야 할 부분이 있는데,<br>각 레이어를 통과 할 때 유닛의 수가 레이블 보다 적다면 가지고 있어야 할 정보가 많이 사라질 수 있습니다.  </p><p>학습 시 파라미터를 줄이기 위해서나 노이즈를 줄이기 위해서 유닛을 줄이는 경우도 있지만,<br>데이터가 가지고 있어야 할 필수 데이터를 잃어버릴 수 있다는 점도 참고하여 네트워크를 구성해야 합니다.</p><h2 id="신경망-네트워크-구축"><a href="#신경망-네트워크-구축" class="headerlink" title="신경망 네트워크 구축"></a>신경망 네트워크 구축</h2><p>로이터 데이터셋의 단어수를 10,000개로 제한해두었으니,<br>신경망의 입력 차원수도 10,000으로 설정합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_3 (Dense)              (None, 64)                640064    _________________________________________________________________dense_4 (Dense)              (None, 64)                4160      _________________________________________________________________dense_5 (Dense)              (None, 46)                2990      =================================================================Total params: 647,214Trainable params: 647,214Non-trainable params: 0_________________________________________________________________</code></pre><p>이진분류와 비슷하게 마지막 Dense 레이어의 아웃풋 벡터 개수는(46개) 예측하기 위한 클래스의 개수와 동일합니다.<br>다른점은 이진 분류에서는 활성함수로 sigmoid를 사용한 반면 여기서는 softmax를 이용했는데,<br>softmax는 각 클래스별로 해당 클래스 일 확률을 표시하도록 만들어집니다.   </p><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>다중 분류에서 손실함수로써는 categorical_crossentropy를 주로 사용합니다.<br>옵티마이저는 가장 빠르고 효과가 좋다고 알려진 adam 옵티마이저를 사용하도록 설정합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'accuracy'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="검증-데이터셋-구성"><a href="#검증-데이터셋-구성" class="headerlink" title="검증 데이터셋 구성"></a>검증 데이터셋 구성</h2><p>훈련용 데이터셋에서 Validation으로 사용할 데이터셋을 분리시킵니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure><h1 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train, </span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 1s 152us/step - loss: 3.3032 - acc: 0.4328 - val_loss: 2.6156 - val_acc: 0.5320Epoch 2/207982/7982 [==============================] - 1s 78us/step - loss: 2.0615 - acc: 0.6076 - val_loss: 1.6695 - val_acc: 0.6460Epoch 3/207982/7982 [==============================] - 1s 78us/step - loss: 1.3844 - acc: 0.7051 - val_loss: 1.3219 - val_acc: 0.7100Epoch 4/207982/7982 [==============================] - 1s 77us/step - loss: 1.0796 - acc: 0.7669 - val_loss: 1.1773 - val_acc: 0.7520Epoch 5/207982/7982 [==============================] - 1s 79us/step - loss: 0.8673 - acc: 0.8132 - val_loss: 1.0768 - val_acc: 0.7790Epoch 6/207982/7982 [==============================] - 1s 78us/step - loss: 0.6909 - acc: 0.8515 - val_loss: 0.9995 - val_acc: 0.7910Epoch 7/207982/7982 [==============================] - 1s 78us/step - loss: 0.5410 - acc: 0.8880 - val_loss: 0.9467 - val_acc: 0.8010Epoch 8/207982/7982 [==============================] - 1s 78us/step - loss: 0.4177 - acc: 0.9137 - val_loss: 0.9069 - val_acc: 0.8190Epoch 9/207982/7982 [==============================] - 1s 78us/step - loss: 0.3253 - acc: 0.9300 - val_loss: 0.8855 - val_acc: 0.8160Epoch 10/207982/7982 [==============================] - 1s 79us/step - loss: 0.2593 - acc: 0.9432 - val_loss: 0.8973 - val_acc: 0.8090Epoch 11/207982/7982 [==============================] - 1s 80us/step - loss: 0.2123 - acc: 0.9503 - val_loss: 0.8912 - val_acc: 0.8210Epoch 12/207982/7982 [==============================] - 1s 78us/step - loss: 0.1792 - acc: 0.9549 - val_loss: 0.9012 - val_acc: 0.8230Epoch 13/207982/7982 [==============================] - 1s 78us/step - loss: 0.1594 - acc: 0.9565 - val_loss: 0.9194 - val_acc: 0.8170Epoch 14/207982/7982 [==============================] - 1s 78us/step - loss: 0.1410 - acc: 0.9573 - val_loss: 0.9531 - val_acc: 0.8150Epoch 15/207982/7982 [==============================] - 1s 79us/step - loss: 0.1294 - acc: 0.9578 - val_loss: 0.9501 - val_acc: 0.8160Epoch 16/207982/7982 [==============================] - 1s 78us/step - loss: 0.1192 - acc: 0.9603 - val_loss: 0.9757 - val_acc: 0.8130Epoch 17/207982/7982 [==============================] - 1s 77us/step - loss: 0.1146 - acc: 0.9592 - val_loss: 0.9701 - val_acc: 0.8100Epoch 18/207982/7982 [==============================] - 1s 79us/step - loss: 0.1058 - acc: 0.9600 - val_loss: 0.9883 - val_acc: 0.8080Epoch 19/207982/7982 [==============================] - 1s 80us/step - loss: 0.1055 - acc: 0.9624 - val_loss: 1.0214 - val_acc: 0.8130Epoch 20/207982/7982 [==============================] - 1s 81us/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.9970 - val_acc: 0.8150</code></pre><h2 id="훈련의-정확도와-손실-시각화"><a href="#훈련의-정확도와-손실-시각화" class="headerlink" title="훈련의 정확도와 손실 시각화"></a>훈련의 정확도와 손실 시각화</h2><p>이전 이진분류 포스팅에서 사용했던 함수를 그대로 끌어와 사용하도록 합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history)</span>:</span></span><br><span class="line">    history_dict = history.history</span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="reuter_multi_classification_27_0.png" alt="png"></p><p>그래프를 보면 대략 8~9번째 에폭부터 과대적합이 시작되는걸 알 수 있습니다.  </p><h2 id="새로운-데이터-예측해보기"><a href="#새로운-데이터-예측해보기" class="headerlink" title="새로운 데이터 예측해보기"></a>새로운 데이터 예측해보기</h2><p>모델을 이용해 각 뉴스기사에 대한 토픽을 예측해보도록 합니다.<br>softmax를 사용하여 각 뉴스별로 46개의 토픽에 해당하는 확률을 출력합니다.<br>각 클래스 별 확률을 모두 더하면 1.0(100%)가 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br><span class="line">display(predictions.shape)</span><br></pre></td></tr></table></figure><pre><code>(2246, 46)</code></pre><p>predictions는 테스트 데이터셋의 개수에 맞게 2246개의 결과가 들어있습니다.<br>각 결과안에는 46개 클래스에 해당하는 확률값이 들어가있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>1.0</code></pre><p>각 46개의 모든 원소의 값을 모두 더하면 1.0(100%)가 된걸 확인할 수 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display(np.argmax(predictions[<span class="number">0</span>]))</span><br><span class="line">display(predictions[<span class="number">0</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure><pre><code>30.96262544</code></pre><p>predictions[0]의 3번째 인덱스가 가장 큰 값을 가진걸 확인하였고,<br>해당 인덱스의 값을 확인하였더니 0.96262544(96.262544%)로 모델이 예측한걸 확인할 수 있습니다.</p><h2 id="데이터-레이블-인코딩-방식-변경하여-학습하기"><a href="#데이터-레이블-인코딩-방식-변경하여-학습하기" class="headerlink" title="데이터 레이블 인코딩 방식 변경하여 학습하기"></a>데이터 레이블 인코딩 방식 변경하여 학습하기</h2><p>one hot 인코딩이 아닌 정수형으로 토픽을 예측하도록 레이블 인코딩을 사용하도록 변경해봅니다.<br>손실함수를 변경해주면 되는데,<br>categorical_crossentropy는 범주형 인코딩일 시 사용하는 손실함수이고,<br>정수형을 사용할 때에는 sparse_categorical_crossentropy를 사용합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.array(train_labels)</span><br><span class="line">y_test = np.array(test_labels)</span><br><span class="line"></span><br><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = y_train[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_6 (Dense)              (None, 64)                640064    _________________________________________________________________dense_7 (Dense)              (None, 64)                4160      _________________________________________________________________dense_8 (Dense)              (None, 46)                2990      =================================================================Total params: 647,214Trainable params: 647,214Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train, </span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">9</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/97982/7982 [==============================] - 1s 116us/step - loss: 3.3313 - acc: 0.3983 - val_loss: 2.5661 - val_acc: 0.5770Epoch 2/97982/7982 [==============================] - 1s 78us/step - loss: 2.0006 - acc: 0.6272 - val_loss: 1.6576 - val_acc: 0.6580Epoch 3/97982/7982 [==============================] - 1s 78us/step - loss: 1.3389 - acc: 0.7134 - val_loss: 1.2907 - val_acc: 0.7090Epoch 4/97982/7982 [==============================] - 1s 78us/step - loss: 1.0256 - acc: 0.7762 - val_loss: 1.1484 - val_acc: 0.7630Epoch 5/97982/7982 [==============================] - 1s 80us/step - loss: 0.8077 - acc: 0.8339 - val_loss: 1.0422 - val_acc: 0.7870Epoch 6/97982/7982 [==============================] - 1s 76us/step - loss: 0.6357 - acc: 0.8716 - val_loss: 0.9641 - val_acc: 0.8050Epoch 7/97982/7982 [==============================] - 1s 78us/step - loss: 0.4960 - acc: 0.8990 - val_loss: 0.9275 - val_acc: 0.8050Epoch 8/97982/7982 [==============================] - 1s 78us/step - loss: 0.3909 - acc: 0.9197 - val_loss: 0.8983 - val_acc: 0.8100Epoch 9/97982/7982 [==============================] - 1s 78us/step - loss: 0.3116 - acc: 0.9346 - val_loss: 0.8843 - val_acc: 0.8190</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="reuter_multi_classification_41_0.png" alt="png"></p><p>예측하는 방법은 아까와 동일하고,<br>출력된 레이블 또한 softmax처럼 각 클래스 별 확률이 나오게 됩니다.<br>9번의 에폭에서 과대적합 되던걸 확인하여 이번 학습은 최대 에폭을 9로 설정하여 학습을 진행한 내용의 그래프입니다.</p><h1 id="추가-개선사항"><a href="#추가-개선사항" class="headerlink" title="추가 개선사항"></a>추가 개선사항</h1><p>중간 레이어의 유닛수가 너무 작게 되면 데이터에 대한 손실이 발생할 수 있지만,<br>데이터를 압축하여 노이즈를 줄이는 효과를 얻을 수도 있고, 반대로 유닛수를 크게 한다면,<br>해당 레이블을 표현하기 위한 데이터를 더 넣을 수 있게 된다고 볼 수도 있습니다.<br>이러한 내용을 잘 숙지하여 레이어 구성을 변경해가며 테스트를 진행해봅시다.</p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/14/reuter_multi_classification/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝으로 영화 리뷰의 긍정/부정 이진 분류 모델 만들기</title>
      <link>https://codingcrews.github.io/2019/01/11/imdb/</link>
      <guid>https://codingcrews.github.io/2019/01/11/imdb/</guid>
      <pubDate>Thu, 10 Jan 2019 16:55:22 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification&quot;&gt;&lt;a href=&quot;#IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification&quot; class=&quot;headerlink&quot; title=&quot;IM
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification"><a href="#IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification" class="headerlink" title="IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification"></a>IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification</h1><p>머신러닝은 크게 분류와 회귀 문제로 나누어 지게 됩니다.<br>저희는 분류 문제에서도 이진분류를 진행해볼건데요.<br>이진분류라는건 예를들어 메일을 받았을 때 이 메일이 스팸이냐 아니냐, 말을 했을 때 욕이냐 아니냐와 같이 문제에 대해 <strong>예/아니오</strong> 형태로 구분되는 문제에 적합합니다.  </p><p>이번에는 IMDB 데이터셋을 이용해 해당 리뷰가 긍정적인지 부정적인지 예측하는 이진 분류 모델을 만들어 보겠습니다.  </p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>IMDB 데이터셋이란 영화에 대한 평점과 리뷰들이 들어가있는 데이터인데요, <strong>(영어로 되어 있습니다.)</strong>  </p><p>추후에는 한글로 된 데이터셋을 직접 구하고 만들어서 분류기를 만들어보시면 재밌지 않을까요? </p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><p>데이터셋은 텐서플로의 패키지 안에 다운로더 및 로더가 내장되어 있습니다.<br>(포스팅 시각 기준으로 저는 r1.12 버전의 텐서플로 패키지를 사용중이며, 너무 오래된 버전의 경우 keras 패키지가 내장되어 있지 않을 수 있습니다.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="Dataset-shape-확인"><a href="#Dataset-shape-확인" class="headerlink" title="Dataset shape 확인"></a>Dataset shape 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train Datas Shape : &#123;&#125;"</span>.format(train_data.shape))</span><br><span class="line">print(<span class="string">"Train Labels Shape : &#123;&#125;"</span>.format(train_labels.shape))</span><br></pre></td></tr></table></figure><pre><code>Train Datas Shape : (25000,)Train Labels Shape : (25000,)</code></pre><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><p>25000개의 훈련용 데이터셋이 존재하며, 각 인덱스는 단어 인덱스의 리스트를 가지고 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display(train_data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line">display(train_labels)</span><br></pre></td></tr></table></figure><pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]array([1, 0, 0, ..., 0, 1, 0])</code></pre><h3 id="단어-인덱스를-단어로-치환"><a href="#단어-인덱스를-단어로-치환" class="headerlink" title="단어 인덱스를 단어로 치환"></a>단어 인덱스를 단어로 치환</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">indexes = dict([(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_review = <span class="string">' '</span>.join(indexes.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>])</span><br><span class="line">print(decoded_review)</span><br></pre></td></tr></table></figure><pre><code>? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all</code></pre><h3 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h3><p>단어의 개수를 10,000개로 지정해두었고, 이 단어 인덱스를 원핫인코딩으로 변환하여 10,000차원의 벡터로 변경시키도록 합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension)) <span class="comment"># 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br><span class="line"></span><br><span class="line">display(x_train.shape)</span><br><span class="line">display(x_test.shape)</span><br></pre></td></tr></table></figure><pre><code>(25000, 10000)(25000, 10000)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">display(y_train)</span><br><span class="line">display(y_test)</span><br></pre></td></tr></table></figure><pre><code>array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)array([0., 1., 1., ..., 0., 0., 0.], dtype=float32)</code></pre><h1 id="신경망-구성"><a href="#신경망-구성" class="headerlink" title="신경망 구성"></a>신경망 구성</h1><h2 id="신경망-네트워크-구축"><a href="#신경망-네트워크-구축" class="headerlink" title="신경망 네트워크 구축"></a>신경망 네트워크 구축</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense (Dense)                (None, 16)                160016    _________________________________________________________________dense_1 (Dense)              (None, 16)                272       _________________________________________________________________dense_2 (Dense)              (None, 1)                 17        =================================================================Total params: 160,305Trainable params: 160,305Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>모델을 사용하기 위해선 네트워크를 구성한 모델을 컴파일하는 과정이 필요합니다.<br>rmsprop 옵티마이저를 사용하고,<br>확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택인데,<br>이진 분류로 각 확률을 구하는 모델이니 binary crossentropy를 사용합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> losses</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line"><span class="comment">#     optimizer='rmsprop',</span></span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line"><span class="comment">#     loss='binary_crossentropy',</span></span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line"><span class="comment">#     metrics=['accuracy']</span></span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="검증-데이터-준비-Validation"><a href="#검증-데이터-준비-Validation" class="headerlink" title="검증 데이터 준비 (Validation)"></a>검증 데이터 준비 (Validation)</h2><p>훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해<br>원본 훈련 데이터에서 10,000개의 샘플을 떼내어 검증 데이터 세트를 만들겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure><h1 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h1><p>512의 샘플씩 미니배치를 만들어 20번의 에폭동안 훈련시키고,<br>앞에서 떼어놓은 10,000개의 데이터를 이용해 손실과 정확도를 측정하겠습니다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/2015000/15000 [==============================] - 2s 139us/step - loss: 0.5883 - binary_accuracy: 0.7147 - val_loss: 0.5165 - val_binary_accuracy: 0.7686Epoch 2/2015000/15000 [==============================] - 1s 90us/step - loss: 0.4284 - binary_accuracy: 0.8771 - val_loss: 0.3974 - val_binary_accuracy: 0.8760Epoch 3/2015000/15000 [==============================] - 1s 85us/step - loss: 0.3157 - binary_accuracy: 0.9189 - val_loss: 0.3314 - val_binary_accuracy: 0.8880Epoch 4/2015000/15000 [==============================] - 1s 86us/step - loss: 0.2407 - binary_accuracy: 0.9349 - val_loss: 0.2963 - val_binary_accuracy: 0.8893Epoch 5/2015000/15000 [==============================] - 1s 85us/step - loss: 0.1912 - binary_accuracy: 0.9473 - val_loss: 0.2806 - val_binary_accuracy: 0.8906Epoch 6/2015000/15000 [==============================] - 1s 85us/step - loss: 0.1587 - binary_accuracy: 0.9555 - val_loss: 0.2811 - val_binary_accuracy: 0.8880Epoch 7/2015000/15000 [==============================] - 1s 85us/step - loss: 0.1303 - binary_accuracy: 0.9654 - val_loss: 0.2927 - val_binary_accuracy: 0.8850Epoch 8/2015000/15000 [==============================] - 1s 87us/step - loss: 0.1108 - binary_accuracy: 0.9707 - val_loss: 0.2976 - val_binary_accuracy: 0.8859Epoch 9/2015000/15000 [==============================] - 1s 87us/step - loss: 0.0930 - binary_accuracy: 0.9762 - val_loss: 0.3311 - val_binary_accuracy: 0.8771Epoch 10/2015000/15000 [==============================] - 1s 87us/step - loss: 0.0795 - binary_accuracy: 0.9802 - val_loss: 0.3353 - val_binary_accuracy: 0.8808Epoch 11/2015000/15000 [==============================] - 1s 87us/step - loss: 0.0652 - binary_accuracy: 0.9855 - val_loss: 0.3555 - val_binary_accuracy: 0.8786Epoch 12/2015000/15000 [==============================] - 1s 88us/step - loss: 0.0556 - binary_accuracy: 0.9881 - val_loss: 0.3749 - val_binary_accuracy: 0.8768Epoch 13/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0442 - binary_accuracy: 0.9919 - val_loss: 0.4263 - val_binary_accuracy: 0.8694Epoch 14/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0373 - binary_accuracy: 0.9934 - val_loss: 0.4213 - val_binary_accuracy: 0.8753Epoch 15/2015000/15000 [==============================] - 1s 90us/step - loss: 0.0305 - binary_accuracy: 0.9947 - val_loss: 0.4821 - val_binary_accuracy: 0.8657Epoch 16/2015000/15000 [==============================] - 1s 92us/step - loss: 0.0263 - binary_accuracy: 0.9955 - val_loss: 0.4871 - val_binary_accuracy: 0.8704Epoch 17/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0178 - binary_accuracy: 0.9978 - val_loss: 0.5176 - val_binary_accuracy: 0.8693Epoch 18/2015000/15000 [==============================] - 1s 88us/step - loss: 0.0155 - binary_accuracy: 0.9982 - val_loss: 0.5890 - val_binary_accuracy: 0.8639Epoch 19/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0136 - binary_accuracy: 0.9980 - val_loss: 0.5645 - val_binary_accuracy: 0.8679Epoch 20/2015000/15000 [==============================] - 1s 89us/step - loss: 0.0086 - binary_accuracy: 0.9990 - val_loss: 0.5974 - val_binary_accuracy: 0.8672</code></pre><h2 id="모델의-훈련-정보-그리기"><a href="#모델의-훈련-정보-그리기" class="headerlink" title="모델의 훈련 정보 그리기"></a>모델의 훈련 정보 그리기</h2><p>위에서 fit의 반환으로 받은 history는 각각의 훈련 데이터세트와 검증 데이터세트에 대한 매 에폭마다의 손실율과 정확도를 가지고 있습니다.<br>해당 지표를 matplot를 이용해 시각화 하도록 해보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history)</span>:</span></span><br><span class="line">    history_dict = history.history</span><br><span class="line">    accuracy = history_dict[<span class="string">'binary_accuracy'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_binary_accuracy'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy and loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="imdb_27_0.png" alt="png"></p><p>훈련 데이터셋의 그래프를(점선) 먼저 확인해보면,<br>각 에폭이 돌때마다 정확도가 오르고, 손실은 줄어드는 형태로 제대로 학습이 된것 처럼 보이나, 검증 데이터셋(실선)을 보게 되면 그렇지 않습니다.<br>각 에폭마다도 정확도는 오르지 않고, 손실이 늘어나는걸 확인할 수 있는데, 이런 경우 훈련 데이터셋에 과대적합(overfitting) 되었다고 합니다.<br>과대적합이 된 경우 모델이 새로운 데이터셋을 만났을 때 제대로 분류를 하지 못하게 됩니다.</p><h2 id="모델-재학습하기"><a href="#모델-재학습하기" class="headerlink" title="모델 재학습하기"></a>모델 재학습하기</h2><p>아까와 동일한 형태의 모델을 구성하고,<br>학습과 관련된 하이퍼파라미터만 변경하여 과대적합을 피해보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_3 (Dense)              (None, 16)                160016    _________________________________________________________________dense_4 (Dense)              (None, 16)                272       _________________________________________________________________dense_5 (Dense)              (None, 1)                 17        =================================================================Total params: 160,305Trainable params: 160,305Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 111us/step - loss: 0.5347 - binary_accuracy: 0.7908 - val_loss: 0.4127 - val_binary_accuracy: 0.8663Epoch 2/815000/15000 [==============================] - 1s 89us/step - loss: 0.3256 - binary_accuracy: 0.9005 - val_loss: 0.3316 - val_binary_accuracy: 0.8697Epoch 3/815000/15000 [==============================] - 1s 88us/step - loss: 0.2393 - binary_accuracy: 0.9232 - val_loss: 0.2822 - val_binary_accuracy: 0.8906Epoch 4/815000/15000 [==============================] - 1s 85us/step - loss: 0.1881 - binary_accuracy: 0.9410 - val_loss: 0.2801 - val_binary_accuracy: 0.8875Epoch 5/815000/15000 [==============================] - 1s 85us/step - loss: 0.1525 - binary_accuracy: 0.9524 - val_loss: 0.2770 - val_binary_accuracy: 0.8885Epoch 6/815000/15000 [==============================] - 1s 87us/step - loss: 0.1263 - binary_accuracy: 0.9611 - val_loss: 0.2856 - val_binary_accuracy: 0.8880Epoch 7/815000/15000 [==============================] - 1s 87us/step - loss: 0.1035 - binary_accuracy: 0.9701 - val_loss: 0.3127 - val_binary_accuracy: 0.8846Epoch 8/815000/15000 [==============================] - 1s 89us/step - loss: 0.0863 - binary_accuracy: 0.9754 - val_loss: 0.3270 - val_binary_accuracy: 0.8835</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="imdb_33_0.png" alt="png"></p><p>아까보단 모델이 상대적으로 과대적합 되지 않았습니다.<br>손실 그래프를 확인했을 때 2에폭과 3에폭이 훈련 세트와 검증 세트가 가장 근접한 손실을 갖고있는걸 확인할 수 있고,<br>정확도 또한 2,3에폭이 가장 근접한걸 확인할 수 있습니다.<br>즉, 이 모델의 경우 2에폭 혹은 3에폭을 돌렸을 때 과대적합을 가장 피할 수 있는 학습상태가 된다는걸 확인할 수 있습니다.<br>이렇게 학습에 파라미터를 조작하는 것 이외에도 과대적합을 피하는 기법이 많이 존재합니다.  </p><h1 id="모델의-평가"><a href="#모델의-평가" class="headerlink" title="모델의 평가"></a>모델의 평가</h1><p>모델의 정확도를 측정합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss, accuracy = model.evaluate(x_test, y_test)</span><br><span class="line">print(<span class="string">'accuracy : &#123;acc&#125;, loss : &#123;loss&#125;'</span>.format(acc=accuracy, loss=loss))</span><br></pre></td></tr></table></figure><pre><code>25000/25000 [==============================] - 2s 67us/stepaccuracy : 0.86852, loss : 0.35010315059185027</code></pre><h1 id="모델의-예측"><a href="#모델의-예측" class="headerlink" title="모델의 예측"></a>모델의 예측</h1><p>긍정이거나 부정일 확률 (높으면 긍정, 낮으면 부정)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(x_test[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>array([[0.2251153 ],       [0.9999784 ],       [0.98094064],       [0.94734573],       [0.97099954],       [0.9737046 ],       [0.9995834 ],       [0.01185756],       [0.9645392 ],       [0.99970514]], dtype=float32)</code></pre><h1 id="번외-레이어-변경하여-정확도-개선해보기"><a href="#번외-레이어-변경하여-정확도-개선해보기" class="headerlink" title="번외. 레이어 변경하여 정확도 개선해보기"></a>번외. 레이어 변경하여 정확도 개선해보기</h1><h2 id="레이어를-한개-더-추가하여-테스트-Deep"><a href="#레이어를-한개-더-추가하여-테스트-Deep" class="headerlink" title="레이어를 한개 더 추가하여 테스트 (Deep)"></a>레이어를 한개 더 추가하여 테스트 (Deep)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_6 (Dense)              (None, 16)                160016    _________________________________________________________________dense_7 (Dense)              (None, 16)                272       _________________________________________________________________dense_8 (Dense)              (None, 16)                272       _________________________________________________________________dense_9 (Dense)              (None, 1)                 17        =================================================================Total params: 160,577Trainable params: 160,577Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 113us/step - loss: 0.5264 - binary_accuracy: 0.7816 - val_loss: 0.4348 - val_binary_accuracy: 0.8122Epoch 2/815000/15000 [==============================] - 1s 87us/step - loss: 0.2989 - binary_accuracy: 0.9006 - val_loss: 0.2936 - val_binary_accuracy: 0.8870Epoch 3/815000/15000 [==============================] - 1s 85us/step - loss: 0.2103 - binary_accuracy: 0.9263 - val_loss: 0.2941 - val_binary_accuracy: 0.8812Epoch 4/815000/15000 [==============================] - 1s 86us/step - loss: 0.1550 - binary_accuracy: 0.9481 - val_loss: 0.2963 - val_binary_accuracy: 0.8817Epoch 5/815000/15000 [==============================] - 1s 89us/step - loss: 0.1294 - binary_accuracy: 0.9543 - val_loss: 0.2956 - val_binary_accuracy: 0.8850Epoch 6/815000/15000 [==============================] - 1s 86us/step - loss: 0.0964 - binary_accuracy: 0.9709 - val_loss: 0.3357 - val_binary_accuracy: 0.8745Epoch 7/815000/15000 [==============================] - 1s 86us/step - loss: 0.0814 - binary_accuracy: 0.9739 - val_loss: 0.3678 - val_binary_accuracy: 0.8714Epoch 8/815000/15000 [==============================] - 1s 84us/step - loss: 0.0598 - binary_accuracy: 0.9834 - val_loss: 0.3910 - val_binary_accuracy: 0.8714</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="imdb_43_0.png" alt="png"></p><h2 id="유닛을-추가하여-테스트-Wide"><a href="#유닛을-추가하여-테스트-Wide" class="headerlink" title="유닛을 추가하여 테스트 (Wide)"></a>유닛을 추가하여 테스트 (Wide)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_10 (Dense)             (None, 64)                640064    _________________________________________________________________dense_11 (Dense)             (None, 64)                4160      _________________________________________________________________dense_12 (Dense)             (None, 1)                 65        =================================================================Total params: 644,289Trainable params: 644,289Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 135us/step - loss: 0.4850 - binary_accuracy: 0.7656 - val_loss: 0.3620 - val_binary_accuracy: 0.8517Epoch 2/815000/15000 [==============================] - 2s 107us/step - loss: 0.2538 - binary_accuracy: 0.9058 - val_loss: 0.2754 - val_binary_accuracy: 0.8902Epoch 3/815000/15000 [==============================] - 2s 103us/step - loss: 0.1857 - binary_accuracy: 0.9340 - val_loss: 0.2826 - val_binary_accuracy: 0.8874Epoch 4/815000/15000 [==============================] - 2s 103us/step - loss: 0.1417 - binary_accuracy: 0.9499 - val_loss: 0.3328 - val_binary_accuracy: 0.8734Epoch 5/815000/15000 [==============================] - 2s 104us/step - loss: 0.1128 - binary_accuracy: 0.9601 - val_loss: 0.3275 - val_binary_accuracy: 0.8826Epoch 6/815000/15000 [==============================] - 2s 104us/step - loss: 0.0795 - binary_accuracy: 0.9743 - val_loss: 0.3473 - val_binary_accuracy: 0.8802Epoch 7/815000/15000 [==============================] - 2s 103us/step - loss: 0.0544 - binary_accuracy: 0.9832 - val_loss: 0.3840 - val_binary_accuracy: 0.8780Epoch 8/815000/15000 [==============================] - 2s 107us/step - loss: 0.0514 - binary_accuracy: 0.9849 - val_loss: 0.4150 - val_binary_accuracy: 0.8789</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="imdb_47_0.png" alt="png"></p><h2 id="깊고-넓게-구성하기-Deep-and-wide-network"><a href="#깊고-넓게-구성하기-Deep-and-wide-network" class="headerlink" title="깊고 넓게 구성하기 (Deep and wide network)"></a>깊고 넓게 구성하기 (Deep and wide network)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_13 (Dense)             (None, 64)                640064    _________________________________________________________________dense_14 (Dense)             (None, 64)                4160      _________________________________________________________________dense_15 (Dense)             (None, 32)                2080      _________________________________________________________________dense_16 (Dense)             (None, 32)                1056      _________________________________________________________________dense_17 (Dense)             (None, 1)                 33        =================================================================Total params: 647,393Trainable params: 647,393Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 142us/step - loss: 0.5094 - binary_accuracy: 0.7512 - val_loss: 0.3875 - val_binary_accuracy: 0.8442Epoch 2/815000/15000 [==============================] - 2s 105us/step - loss: 0.2693 - binary_accuracy: 0.8983 - val_loss: 0.4223 - val_binary_accuracy: 0.8310Epoch 3/815000/15000 [==============================] - 2s 105us/step - loss: 0.1949 - binary_accuracy: 0.9275 - val_loss: 0.5629 - val_binary_accuracy: 0.7950Epoch 4/815000/15000 [==============================] - 2s 104us/step - loss: 0.1534 - binary_accuracy: 0.9434 - val_loss: 0.2965 - val_binary_accuracy: 0.8854Epoch 5/815000/15000 [==============================] - 2s 105us/step - loss: 0.1106 - binary_accuracy: 0.9613 - val_loss: 0.3647 - val_binary_accuracy: 0.8718Epoch 6/815000/15000 [==============================] - 2s 104us/step - loss: 0.0797 - binary_accuracy: 0.9733 - val_loss: 0.4042 - val_binary_accuracy: 0.8748Epoch 7/815000/15000 [==============================] - 2s 110us/step - loss: 0.0802 - binary_accuracy: 0.9775 - val_loss: 0.4029 - val_binary_accuracy: 0.8815Epoch 8/815000/15000 [==============================] - 2s 106us/step - loss: 0.0656 - binary_accuracy: 0.9827 - val_loss: 0.4207 - val_binary_accuracy: 0.8809</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="imdb_51_0.png" alt="png"></p><h2 id="손실함수-변경"><a href="#손실함수-변경" class="headerlink" title="손실함수 변경"></a>손실함수 변경</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.MSE,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_18 (Dense)             (None, 64)                640064    _________________________________________________________________dense_19 (Dense)             (None, 64)                4160      _________________________________________________________________dense_20 (Dense)             (None, 32)                2080      _________________________________________________________________dense_21 (Dense)             (None, 32)                1056      _________________________________________________________________dense_22 (Dense)             (None, 1)                 33        =================================================================Total params: 647,393Trainable params: 647,393Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 138us/step - loss: 0.1701 - binary_accuracy: 0.7462 - val_loss: 0.1152 - val_binary_accuracy: 0.8508Epoch 2/815000/15000 [==============================] - 2s 103us/step - loss: 0.0794 - binary_accuracy: 0.8991 - val_loss: 0.0829 - val_binary_accuracy: 0.8917Epoch 3/815000/15000 [==============================] - 2s 107us/step - loss: 0.0580 - binary_accuracy: 0.9281 - val_loss: 0.0892 - val_binary_accuracy: 0.8833Epoch 4/815000/15000 [==============================] - 2s 111us/step - loss: 0.0375 - binary_accuracy: 0.9544 - val_loss: 0.0858 - val_binary_accuracy: 0.8876Epoch 5/815000/15000 [==============================] - 2s 107us/step - loss: 0.0359 - binary_accuracy: 0.9560 - val_loss: 0.0874 - val_binary_accuracy: 0.8863Epoch 6/815000/15000 [==============================] - 2s 107us/step - loss: 0.0216 - binary_accuracy: 0.9751 - val_loss: 0.0927 - val_binary_accuracy: 0.8822Epoch 7/815000/15000 [==============================] - 2s 109us/step - loss: 0.0189 - binary_accuracy: 0.9782 - val_loss: 0.0946 - val_binary_accuracy: 0.8812Epoch 8/815000/15000 [==============================] - 2s 107us/step - loss: 0.0068 - binary_accuracy: 0.9931 - val_loss: 0.1091 - val_binary_accuracy: 0.8678</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="imdb_55_0.png" alt="png"></p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/11/imdb/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
