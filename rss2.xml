<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CodingCrew</title>
    <link>https://codingcrews.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>코딩크루</description>
    <pubDate>Thu, 17 Jan 2019 10:07:58 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>MNIST 손글씨 분류하기</title>
      <link>https://codingcrews.github.io/2019/01/17/mnist/</link>
      <guid>https://codingcrews.github.io/2019/01/17/mnist/</guid>
      <pubDate>Thu, 17 Jan 2019 09:52:53 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Mnist-손글씨-분류하기-DNN-CNN&quot;&gt;&lt;a href=&quot;#Mnist-손글씨-분류하기-DNN-CNN&quot; class=&quot;headerlink&quot; title=&quot;Mnist 손글씨 분류하기 : DNN, CNN&quot;&gt;&lt;/a&gt;Mnist 손글씨 분류하기 : 
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Mnist-손글씨-분류하기-DNN-CNN"><a href="#Mnist-손글씨-분류하기-DNN-CNN" class="headerlink" title="Mnist 손글씨 분류하기 : DNN, CNN"></a>Mnist 손글씨 분류하기 : DNN, CNN</h1><p>지금까지 진행한 포스팅을 기반으로 딥러닝 튜토리얼 시 가장 흔하게 접할 수 있는 손글씨 분류하기를 해보겠습니다.<br>MNIST 문제는 다중 분류 문제로써 0~9까지의 손글씨를 분류하는 문제입니다.<br>이전 포스팅에서 사용한 Dense레이어를 이용하여 0~9의 숫자를 분류해보고, 이후에는 Convolutional 레이어를 이용해 정확도를 개선해보도록 하겠습니다.  </p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>사용할 MNIST 데이터 세트는 텐서플로 패키지에서 다운로드까지 진행해주는 코드를 포함시켜두었습니다.<br>우리가 사용할 데이터셋은 텐서플로 패키지에서 제공하는 데이터셋을 이용할건데요.<br>다른곳에서 MNIST 데이터셋을 이용하여 진행해봐도 무방합니다.  </p><p>우리가 사용할 MNIST 데이터는 손글씨 데이터로써 흑백 Gray Scale로 된 데이터셋을 말합니다.<br>28x28 사이즈의 단일 색상채널을 가지고 있으며, 트레이닝셋과 테스트셋이 각각 60,000개와 10,000개로 구성되어 있습니다.  </p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train),(x_test, y_test) = mnist.load_data()</span><br></pre></td></tr></table></figure><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">display(x_train.shape)</span><br><span class="line">display(y_train.shape)</span><br><span class="line">display(x_test.shape)</span><br><span class="line">display(y_test.shape)</span><br><span class="line"></span><br><span class="line">display(y_test)</span><br></pre></td></tr></table></figure><pre><code>(60000, 28, 28)(60000,)(10000, 28, 28)(10000,)array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)</code></pre><p>데이터는 60,000개의 트레이닝셋과 10,000개의 테스트셋을 확인할 수 있고,<br>테스트 데이터는 28x28 사이즈의 이미지 데이터임을 확인할 수 있습니다. (색상 채널은 단일 채널이라 따로 표기되지 않아요.)<br>레이블 데이터는 0~9까지의 10개 클래스로 구성되어 있습니다.</p><h3 id="데이터-시각화"><a href="#데이터-시각화" class="headerlink" title="데이터 시각화"></a>데이터 시각화</h3><p>우리가 다룰 Mnist 데이터는 이미지 데이터 기반입니다.<br>해당 이미지를 시각화하여 확인해보겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(arr)</span>:</span></span><br><span class="line">    plt.imshow(arr, cmap=plt.cm.binary)</span><br><span class="line">    </span><br><span class="line">    reshape_data = arr.reshape(<span class="number">-1</span>, )</span><br><span class="line">    <span class="keyword">for</span> index, data <span class="keyword">in</span> enumerate(reshape_data):</span><br><span class="line">        print(<span class="string">'&#123;:3d&#125;'</span>.format(data), end=<span class="string">''</span>)</span><br><span class="line">        <span class="keyword">if</span> index % <span class="number">28</span> == <span class="number">27</span>:</span><br><span class="line">            print()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_data(x_train[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51159253159 50  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0 48238252252252237  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0 54227253252239233252 57  6  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0 10 60224252253252202 84252253122  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0163252252252253252252 96189253167  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0 51238253253190114253228 47 79255168  0  0  0  0  0  00  0  0  0  0  0  0  0  0 48238252252179 12 75121 21  0  0253243 50  0  0  0  0  00  0  0  0  0  0  0  0 38165253233208 84  0  0  0  0  0  0253252165  0  0  0  0  00  0  0  0  0  0  0  7178252240 71 19 28  0  0  0  0  0  0253252195  0  0  0  0  00  0  0  0  0  0  0 57252252 63  0  0  0  0  0  0  0  0  0253252195  0  0  0  0  00  0  0  0  0  0  0198253190  0  0  0  0  0  0  0  0  0  0255253196  0  0  0  0  00  0  0  0  0  0 76246252112  0  0  0  0  0  0  0  0  0  0253252148  0  0  0  0  00  0  0  0  0  0 85252230 25  0  0  0  0  0  0  0  0  7135253186 12  0  0  0  0  00  0  0  0  0  0 85252223  0  0  0  0  0  0  0  0  7131252225 71  0  0  0  0  0  00  0  0  0  0  0 85252145  0  0  0  0  0  0  0 48165252173  0  0  0  0  0  0  0  00  0  0  0  0  0 86253225  0  0  0  0  0  0114238253162  0  0  0  0  0  0  0  0  00  0  0  0  0  0 85252249146 48 29 85178225253223167 56  0  0  0  0  0  0  0  0  00  0  0  0  0  0 85252252252229215252252252196130  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0 28199252252253252252233145  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0 25128252253252141 37  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</code></pre><p><img src="/resources/mnist_files/mnist_9_1.png" alt="png"></p><p>위는 해당 데이터를 시각화 한 결과입니다.<br>28x28의 데이터는 각 픽셀별로 해당 픽셀이 표시해야 할 색상값을 표시하는데,<br>이걸 28x28 사이즈에 맞춰 해당 데이터를 출력한 결과입니다.  </p><p>이미지는 matplotlib을 이용하여 해당 데이터를 시각화 한 결과입니다.</p><h2 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h2><p>이미지 데이터를 확인했을 때 0~255까지의 값을 가지고 있는걸 확인할 수 있습니다.<br>이걸 모델에 효율적으로 학습시키기 위해선 0~1 사이의 값으로 일반화(Nomalization)시키는 과정을 거쳐야 합니다.<br>그 이외에도 2D컨볼루셔널 레이어를 사용하기 위해선 채널에 대한 값도 명시되어 있어야하기 때문에<br>(넓이, 높이)의 형태를 가진 데이터를 (넓이,높이,채널)의 형태로 변환시켜보겠습니다.<br>구조는 어떤식으로든 상관없으며 (채널, 넓이, 높이) 순으로 정렬을 시켜도 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reshape_x_train = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">reshape_x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h1><p>이전 포스팅에서 다뤘던 Dense 레이어를 이용해 기본적인 신경망으로 학습을 진행해봅시다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense (Dense)                (None, 28, 28, 64)        128       _________________________________________________________________dense_1 (Dense)              (None, 28, 28, 16)        1040      _________________________________________________________________flatten (Flatten)            (None, 12544)             0         _________________________________________________________________dense_2 (Dense)              (None, 10)                125450    =================================================================Total params: 126,618Trainable params: 126,618Non-trainable params: 0_________________________________________________________________</code></pre><p>총 3개의 레이어로 구성되어 있으며, MNIST 데이터셋에 맞춰 (28, 28, 1)의 형태에 맞춰 인풋 데이터를 받도록 구성했습니다.<br>마지막층은 손글씨 데이터의 10개 클래스(0~9까지의 레이블)를 예측하기 위해 출력을 10으로, 활성함수는 softmax를 사용합니다.  </p><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h2><p>CPU로 학습 시 대략 25분 정도의 시간이 소요됩니다. (맥북프로 15인치 2017 CTO 기준)<br>그래서 저는 GPU 환경을 세팅하고 학습을 진행했습니다.<br>GPU 환경을 구축하기 위한 데스크탑을 만들기 어려우신 분들은 <a href="/2019/01/15/deeplearning-gpu/">AWS로 GPU환경 세팅하기</a>포스팅을 참고하세요.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    epochs=<span class="number">50</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 54000 samples, validate on 6000 samplesEpoch 1/5054000/54000 [==============================] - 65s 1ms/step - loss: 0.6932 - acc: 0.8736 - val_loss: 0.2561 - val_acc: 0.9295Epoch 2/5054000/54000 [==============================] - 8s 152us/step - loss: 0.3104 - acc: 0.9130 - val_loss: 0.2642 - val_acc: 0.9347Epoch 3/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2963 - acc: 0.9182 - val_loss: 0.2445 - val_acc: 0.9352Epoch 4/5054000/54000 [==============================] - 8s 152us/step - loss: 0.2911 - acc: 0.9196 - val_loss: 0.2391 - val_acc: 0.9348Epoch 5/5054000/54000 [==============================] - 8s 152us/step - loss: 0.2834 - acc: 0.9214 - val_loss: 0.2340 - val_acc: 0.9345Epoch 6/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2819 - acc: 0.9221 - val_loss: 0.2359 - val_acc: 0.9362Epoch 7/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2765 - acc: 0.9244 - val_loss: 0.2377 - val_acc: 0.9343Epoch 8/5054000/54000 [==============================] - 8s 155us/step - loss: 0.2750 - acc: 0.9248 - val_loss: 0.2495 - val_acc: 0.9325Epoch 9/5054000/54000 [==============================] - 8s 155us/step - loss: 0.2714 - acc: 0.9251 - val_loss: 0.2447 - val_acc: 0.9325Epoch 10/5054000/54000 [==============================] - 8s 154us/step - loss: 0.2737 - acc: 0.9259 - val_loss: 0.2437 - val_acc: 0.9367Epoch 11/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2730 - acc: 0.9251 - val_loss: 0.2386 - val_acc: 0.9362Epoch 12/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2627 - acc: 0.9288 - val_loss: 0.2715 - val_acc: 0.9382Epoch 13/5054000/54000 [==============================] - 8s 152us/step - loss: 0.2661 - acc: 0.9264 - val_loss: 0.2757 - val_acc: 0.9290Epoch 14/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2622 - acc: 0.9280 - val_loss: 0.2482 - val_acc: 0.9332Epoch 15/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2592 - acc: 0.9287 - val_loss: 0.2319 - val_acc: 0.9370Epoch 16/5054000/54000 [==============================] - 8s 154us/step - loss: 0.2507 - acc: 0.9317 - val_loss: 0.2524 - val_acc: 0.9303Epoch 17/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2417 - acc: 0.9328 - val_loss: 0.2335 - val_acc: 0.9340Epoch 18/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2317 - acc: 0.9355 - val_loss: 0.2430 - val_acc: 0.9317Epoch 19/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2247 - acc: 0.9374 - val_loss: 0.2368 - val_acc: 0.9372Epoch 20/5054000/54000 [==============================] - 8s 154us/step - loss: 0.2161 - acc: 0.9395 - val_loss: 0.2377 - val_acc: 0.9377Epoch 21/5054000/54000 [==============================] - 8s 154us/step - loss: 0.2163 - acc: 0.9404 - val_loss: 0.2461 - val_acc: 0.9383Epoch 22/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2073 - acc: 0.9416 - val_loss: 0.2586 - val_acc: 0.9355Epoch 23/5054000/54000 [==============================] - 8s 153us/step - loss: 0.2053 - acc: 0.9418 - val_loss: 0.2565 - val_acc: 0.9287Epoch 24/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1957 - acc: 0.9449 - val_loss: 0.2431 - val_acc: 0.9395Epoch 25/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1957 - acc: 0.9446 - val_loss: 0.2381 - val_acc: 0.9330Epoch 26/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1871 - acc: 0.9471 - val_loss: 0.2552 - val_acc: 0.9313Epoch 27/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1793 - acc: 0.9484 - val_loss: 0.2542 - val_acc: 0.9328Epoch 28/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1788 - acc: 0.9493 - val_loss: 0.2717 - val_acc: 0.9348Epoch 29/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1698 - acc: 0.9517 - val_loss: 0.2538 - val_acc: 0.9307Epoch 30/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1665 - acc: 0.9521 - val_loss: 0.2724 - val_acc: 0.9338Epoch 31/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1595 - acc: 0.9537 - val_loss: 0.2662 - val_acc: 0.9250Epoch 32/5054000/54000 [==============================] - 8s 154us/step - loss: 0.1514 - acc: 0.9567 - val_loss: 0.2738 - val_acc: 0.9263Epoch 33/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1487 - acc: 0.9572 - val_loss: 0.2862 - val_acc: 0.9233Epoch 34/5054000/54000 [==============================] - 8s 154us/step - loss: 0.1421 - acc: 0.9582 - val_loss: 0.2888 - val_acc: 0.9225Epoch 35/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1388 - acc: 0.9591 - val_loss: 0.2983 - val_acc: 0.9270Epoch 36/5054000/54000 [==============================] - 8s 152us/step - loss: 0.1324 - acc: 0.9615 - val_loss: 0.3080 - val_acc: 0.9275Epoch 37/5054000/54000 [==============================] - 8s 152us/step - loss: 0.1324 - acc: 0.9606 - val_loss: 0.3133 - val_acc: 0.9183Epoch 38/5054000/54000 [==============================] - 8s 151us/step - loss: 0.1230 - acc: 0.9642 - val_loss: 0.3090 - val_acc: 0.9200Epoch 39/5054000/54000 [==============================] - 8s 152us/step - loss: 0.1267 - acc: 0.9621 - val_loss: 0.3596 - val_acc: 0.9268Epoch 40/5054000/54000 [==============================] - 8s 151us/step - loss: 0.1195 - acc: 0.9645 - val_loss: 0.3408 - val_acc: 0.9257Epoch 41/5054000/54000 [==============================] - 8s 151us/step - loss: 0.1155 - acc: 0.9659 - val_loss: 0.3661 - val_acc: 0.9145Epoch 42/5054000/54000 [==============================] - 8s 154us/step - loss: 0.1138 - acc: 0.9653 - val_loss: 0.3488 - val_acc: 0.9250Epoch 43/5054000/54000 [==============================] - 8s 154us/step - loss: 0.1085 - acc: 0.9671 - val_loss: 0.3628 - val_acc: 0.9222Epoch 44/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1107 - acc: 0.9651 - val_loss: 0.3618 - val_acc: 0.9285Epoch 45/5054000/54000 [==============================] - 8s 155us/step - loss: 0.1073 - acc: 0.9675 - val_loss: 0.3617 - val_acc: 0.9185Epoch 46/5054000/54000 [==============================] - 8s 153us/step - loss: 0.1016 - acc: 0.9694 - val_loss: 0.3677 - val_acc: 0.9190Epoch 47/5054000/54000 [==============================] - 8s 152us/step - loss: 0.0972 - acc: 0.9706 - val_loss: 0.3832 - val_acc: 0.9192Epoch 48/5054000/54000 [==============================] - 8s 152us/step - loss: 0.1012 - acc: 0.9691 - val_loss: 0.4142 - val_acc: 0.9060Epoch 49/5054000/54000 [==============================] - 8s 151us/step - loss: 0.0945 - acc: 0.9715 - val_loss: 0.3816 - val_acc: 0.9195Epoch 50/5054000/54000 [==============================] - 8s 152us/step - loss: 0.0943 - acc: 0.9712 - val_loss: 0.4463 - val_acc: 0.8963</code></pre><p>현재 저의 GPU는 GTX 750TI를 사용중입니다.<br>에폭당 8초정도의 시간을 보여주고 있네요.  </p><h2 id="정확도-손실-시각화"><a href="#정확도-손실-시각화" class="headerlink" title="정확도, 손실 시각화"></a>정확도, 손실 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history_dict)</span>:</span></span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy and loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="/resources/mnist_files/mnist_23_0.png" alt="png"></p><p>첫번째 에폭의 결과가 너무 편차가 심하여 그래프를 알아보기 힘드니 2~30번째 에폭의 결과부터 그래프로 그려보도록 하겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">1</span>:]), history.history.items())</span><br><span class="line">not_noise_history = dict(list(m))</span><br><span class="line">show_graph(not_noise_history)</span><br></pre></td></tr></table></figure><p><img src="/resources/mnist_files/mnist_25_0.png" alt="png"></p><p>대략 20 에폭부터 과대적합이 시작되는걸 확인할 수 있습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(reshape_x_test, y_test)</span><br></pre></td></tr></table></figure><pre><code>10000/10000 [==============================] - 1s 124us/step[0.500411651200056, 0.8873]</code></pre><p>과대적합이 된 모델로 오히려 테스트셋의 결과가 88%로 떨어진걸 확인할 수 있습니다.<br>과대적합이 되기 전 20 에폭까지 학습을 진행해본 이후에 모델을 다시 평가해봅시다.</p><h1 id="모델-구성-CNN"><a href="#모델-구성-CNN" class="headerlink" title="모델 구성 - CNN"></a>모델 구성 - CNN</h1><p>지금까지 포스팅한 간단한 DNN으로도 MNIST 데이터셋의 분류를 높은 정확도로 만들 수 있습니다.<br>하지만 CNN을 이용하면 적은 파라메터로 좀더 정확한 모델을 구성할 수 있습니다.<br>비전 인식에서 딥러닝이 크게 활약한 이유가 이런곳에 있습니다.<br>간단한 CNN 모델을 구성해보겠습니다.  </p><h2 id="네트워크-구성"><a href="#네트워크-구성" class="headerlink" title="네트워크 구성"></a>네트워크 구성</h2><p>Kernel Initializer로는 유명한 Xavier Initializer를 사용합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># model.add(layers.InputLayer())</span></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>),</span><br><span class="line">    filters=<span class="number">64</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">128</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span></span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 28, 28, 64)        320       _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         _________________________________________________________________conv2d_1 (Conv2D)            (None, 14, 14, 128)       32896     _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         _________________________________________________________________flatten_1 (Flatten)          (None, 6272)              0         _________________________________________________________________dense_3 (Dense)              (None, 10)                62730     =================================================================Total params: 95,946Trainable params: 95,946Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="모델-컴파일-1"><a href="#모델-컴파일-1" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>저희는 레이블 데이터를 원-핫 인코딩을 진행하지 않았기 때문에 Loss 펑션에 sparse_categorical_crossentropy를 지정합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="모델-학습-1"><a href="#모델-학습-1" class="headerlink" title="모델 학습"></a>모델 학습</h2><p>CPU로 돌리시는 경우 시간이 조금 걸릴 수 있으니 커피한잔 하고 오세요~  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 54000 samples, validate on 6000 samplesEpoch 1/2054000/54000 [==============================] - 15s 278us/step - loss: 4.6634 - acc: 0.6814 - val_loss: 0.0709 - val_acc: 0.9788Epoch 2/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0665 - acc: 0.9799 - val_loss: 0.0560 - val_acc: 0.9840Epoch 3/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0472 - acc: 0.9854 - val_loss: 0.0516 - val_acc: 0.985570 - acc: 0Epoch 4/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0372 - acc: 0.9880 - val_loss: 0.0491 - val_acc: 0.9863 loss: 0.0353 - ac - ETAEpoch 5/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0449 - val_acc: 0.9882Epoch 6/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0235 - acc: 0.9919 - val_loss: 0.0557 - val_acc: 0.9848Epoch 7/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0222 - acc: 0.9928 - val_loss: 0.0687 - val_acc: 0.9840Epoch 8/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.0524 - val_acc: 0.9882Epoch 9/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0798 - val_acc: 0.9818Epoch 10/2054000/54000 [==============================] - 9s 161us/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0748 - val_acc: 0.9853Epoch 11/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0734 - val_acc: 0.9860Epoch 12/2054000/54000 [==============================] - 9s 161us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0706 - val_acc: 0.9860Epoch 13/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0928 - val_acc: 0.9842Epoch 14/2054000/54000 [==============================] - 9s 161us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0892 - val_acc: 0.9847Epoch 15/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0949 - val_acc: 0.9847Epoch 16/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0868 - val_acc: 0.9855Epoch 17/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0988 - val_acc: 0.9838Epoch 18/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.1250 - val_acc: 0.9805Epoch 19/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0855 - val_acc: 0.9860Epoch 20/2054000/54000 [==============================] - 9s 160us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0942 - val_acc: 0.9865</code></pre><p>제 노트북 CPU를 기준으로 모델을 학습하는데 에폭당 48초정도 걸리네요.<br>20 에폭을 돌렸으니 대략 16분 정도 걸릴거라 보시면 됩니다.  </p><p>저는 GPU를 사용했습니다.</p><h2 id="정확도-및-손실-시각화"><a href="#정확도-및-손실-시각화" class="headerlink" title="정확도 및 손실 시각화"></a>정확도 및 손실 시각화</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="/resources/mnist_files/mnist_38_0.png" alt="png"></p><p>첫 에폭의 결과 때문에 편차가 심하여 <strong>첫 에폭을 제외</strong>하고 보도록 하겠습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">1</span>:]), history.history.items())</span><br><span class="line">not_noise_history = dict(list(m))</span><br><span class="line">show_graph(not_noise_history)</span><br></pre></td></tr></table></figure><p><img src="/resources/mnist_files/mnist_40_0.png" alt="png"></p><p>검증세트의 손실을 보니 8번째 에폭부터 과대적합이 된 것 같군요.<br>과대적합을 해결할 수 있다면 정확도를 높일 수 있을까요 ?</p><h1 id="과대적합-해결"><a href="#과대적합-해결" class="headerlink" title="과대적합 해결"></a>과대적합 해결</h1><p>위와 같은 과대적합을 피하는 방법으로는 여러가지 방법이 있는데요.<br>L1, L2 규제와 같은 정규화를 이용하여 가중치를 감쇠(Weight Decay)시키는 방법과 드랍아웃, 모델의 하이퍼파라메터 튜닝 등으로 과대적합을 피하려 할 수 있습니다.  </p><h2 id="모델의-재구성"><a href="#모델의-재구성" class="headerlink" title="모델의 재구성"></a>모델의 재구성</h2><p>기존 모델은 2단의 컨볼루션 레이어와 풀링 레이어 사용했습니다.<br>모델이 정보를 더 가질 수 있도록 레이어를 더 구성시키고, 드랍아웃과 L2 규제를 걸어 더 복잡한 모델이지만 과대적합은 피할 수 있도록 재구성해보죠.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>),</span><br><span class="line">    filters=<span class="number">32</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">64</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">128</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(</span><br><span class="line">    filters=<span class="number">256</span>, </span><br><span class="line">    kernel_size=(<span class="number">2</span>, <span class="number">2</span>), </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=<span class="string">'relu'</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_normal'</span>,</span><br><span class="line">    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),</span><br><span class="line">))</span><br><span class="line">model.add(layers.MaxPool2D())</span><br><span class="line">model.add(layers.Dropout(<span class="number">.5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>, kernel_regularizer=regularizers.l2(<span class="number">0.01</span>),))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_2 (Conv2D)            (None, 28, 28, 32)        160       _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         _________________________________________________________________dropout (Dropout)            (None, 14, 14, 32)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 14, 14, 64)        8256      _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         _________________________________________________________________dropout_1 (Dropout)          (None, 7, 7, 64)          0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 7, 7, 128)         32896     _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         _________________________________________________________________dropout_2 (Dropout)          (None, 3, 3, 128)         0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 3, 3, 256)         131328    _________________________________________________________________max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         _________________________________________________________________dropout_3 (Dropout)          (None, 1, 1, 256)         0         _________________________________________________________________flatten_2 (Flatten)          (None, 256)               0         _________________________________________________________________dense_4 (Dense)              (None, 10)                2570      =================================================================Total params: 175,210Trainable params: 175,210Non-trainable params: 0_________________________________________________________________</code></pre><p>아까보다 두개의 레이어를 더 추가했고, 파라메터 수가 2배 가까이 늘어난게 보입니다.<br>중간중간 드랍아웃이 적용되었고, 레이어별로 L2 정규화를 추가했습니다.<br>이러한 정규화 기법을 추가하지 않고 학습을 했다면 이전보다 더 빠르게 과대적합이 일어나겠죠?<br>그럼 드랍아웃과 L2규제를 추가한 이후로 모델이 과대적합을 어떻게 견뎌내는지 확인해봅시다.</p><h2 id="모델-컴파일-2"><a href="#모델-컴파일-2" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="학습-시작"><a href="#학습-시작" class="headerlink" title="학습 시작"></a>학습 시작</h2><p>Dropout과 Weight Decay를 추가함으로써 과대적합을 막았고, 뉴런의 학습에 제약을 두었으니, 학습이 더뎌질 수 밖에 없습니다.<br>충분히 학습할 수 있도록 에폭수를 조금 더 늘려 학습을 진행하도록 합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_split=<span class="number">.1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 54000 samples, validate on 6000 samplesEpoch 1/3054000/54000 [==============================] - 11s 199us/step - loss: 16.5374 - acc: 0.0995 - val_loss: 15.8382 - val_acc: 0.0952Epoch 2/3054000/54000 [==============================] - 9s 166us/step - loss: 15.3383 - acc: 0.0990 - val_loss: 15.0753 - val_acc: 0.0952 -Epoch 3/3054000/54000 [==============================] - 9s 165us/step - loss: 11.6865 - acc: 0.1130 - val_loss: 2.4193 - val_acc: 0.3727Epoch 4/3054000/54000 [==============================] - 9s 165us/step - loss: 1.6351 - acc: 0.5584 - val_loss: 0.6810 - val_acc: 0.9327Epoch 5/3054000/54000 [==============================] - 9s 165us/step - loss: 0.9334 - acc: 0.8091 - val_loss: 0.4898 - val_acc: 0.9630 - accEpoch 6/3054000/54000 [==============================] - 9s 164us/step - loss: 0.7393 - acc: 0.8710 - val_loss: 0.4268 - val_acc: 0.9718Epoch 7/3054000/54000 [==============================] - 9s 164us/step - loss: 0.6583 - acc: 0.8908 - val_loss: 0.4011 - val_acc: 0.9752Epoch 8/3054000/54000 [==============================] - 9s 164us/step - loss: 0.6127 - acc: 0.9045 - val_loss: 0.3935 - val_acc: 0.9735Epoch 9/3054000/54000 [==============================] - 9s 164us/step - loss: 0.6006 - acc: 0.9076 - val_loss: 0.3735 - val_acc: 0.9792: 0s - loss: 0.6030 -Epoch 10/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5846 - acc: 0.9099 - val_loss: 0.3665 - val_acc: 0.9803Epoch 11/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5774 - acc: 0.9119 - val_loss: 0.3618 - val_acc: 0.9782Epoch 12/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5714 - acc: 0.9148 - val_loss: 0.3764 - val_acc: 0.9780Epoch 13/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5682 - acc: 0.9154 - val_loss: 0.3707 - val_acc: 0.9763Epoch 14/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5633 - acc: 0.9163 - val_loss: 0.3611 - val_acc: 0.9813Epoch 15/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5628 - acc: 0.9177 - val_loss: 0.3728 - val_acc: 0.9792Epoch 16/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5665 - acc: 0.9181 - val_loss: 0.3630 - val_acc: 0.9820Epoch 17/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5650 - acc: 0.9168 - val_loss: 0.3526 - val_acc: 0.9835 - ETA: 3s - losEpoch 18/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5632 - acc: 0.9172 - val_loss: 0.3584 - val_acc: 0.9823Epoch 19/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5597 - acc: 0.9176 - val_loss: 0.3576 - val_acc: 0.9818Epoch 20/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5548 - acc: 0.9193 - val_loss: 0.3577 - val_acc: 0.9813Epoch 21/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5558 - acc: 0.9202 - val_loss: 0.3812 - val_acc: 0.9758Epoch 22/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5601 - acc: 0.9179 - val_loss: 0.3601 - val_acc: 0.9830Epoch 23/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5571 - acc: 0.9189 - val_loss: 0.3596 - val_acc: 0.9818Epoch 24/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5495 - acc: 0.9200 - val_loss: 0.3568 - val_acc: 0.9817Epoch 25/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5490 - acc: 0.9202 - val_loss: 0.3602 - val_acc: 0.9820Epoch 26/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5529 - acc: 0.9205 - val_loss: 0.3563 - val_acc: 0.9822Epoch 27/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5538 - acc: 0.9198 - val_loss: 0.3534 - val_acc: 0.9840Epoch 28/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5492 - acc: 0.9204 - val_loss: 0.3573 - val_acc: 0.9812Epoch 29/3054000/54000 [==============================] - 9s 164us/step - loss: 0.5530 - acc: 0.9198 - val_loss: 0.3508 - val_acc: 0.9848Epoch 30/3054000/54000 [==============================] - 9s 163us/step - loss: 0.5521 - acc: 0.9198 - val_loss: 0.3648 - val_acc: 0.9792</code></pre><h2 id="모델-평가"><a href="#모델-평가" class="headerlink" title="모델 평가"></a>모델 평가</h2><p>정규화 기법을 적용한 학습 결과가 어떻게 변했는지 확인 해봅시다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history.history)</span><br></pre></td></tr></table></figure><p><img src="/resources/mnist_files/mnist_51_0.png" alt="png"></p><p>가중치의 급격환 변화를 L2 규제를 통해 막아두어 처음 2에폭까지는 큰 변화가 없다가 3~5 에폭이 지나서 모델이 높은 성능에 근접해지는걸 확인할 수 있습니다.<br>그럼 조금 더 자세하게 볼 수 있도록 첫 5에폭까지의 데이터는 제외하고 이후의 데이터를 그래프로 확인해보겠습니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">5</span>:]), history.history.items())</span><br><span class="line">not_noise_history = dict(list(m))</span><br><span class="line">show_graph(not_noise_history)</span><br></pre></td></tr></table></figure><p><img src="/resources/mnist_files/mnist_53_0.png" alt="png"></p><p>트레이닝 세트의 정확도가 낮게 측정되는건 드랍아웃 때문입니다.<br>학습마다 몇몇의 특정 뉴런을 비활성화 시키기 때문에 정확도가 낮아지게 됩니다.<br>훈련을 마친 이후 예측시에는 드랍아웃 시키는 뉴런의 수를 0%로 맞추고 사용합니다.<br>(모든 뉴런을 사용)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(</span><br><span class="line">    reshape_x_train,</span><br><span class="line">    y_train</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>60000/60000 [==============================] - 6s 103us/step[0.38104470246632893, 0.9750666666666666]</code></pre><p>드랍아웃을 해제한 후 훈련세트의 손실값과 정확도입니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(</span><br><span class="line">    reshape_x_test,</span><br><span class="line">    y_test</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>10000/10000 [==============================] - 1s 98us/step[0.3756561163902283, 0.9753]</code></pre><p>테스트 세트의 손실과 정확도입니다.<br>98%의 정확도를 보여주고 있습니다.<br>모델의 네트워크를 변경하여 99% 정확도에 도전해보세요.</p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/17/mnist/#disqus_thread</comments>
    </item>
    
    <item>
      <title>AWS로 GPU로 딥러닝 학습환경 구축하기</title>
      <link>https://codingcrews.github.io/2019/01/15/deeplearning-gpu/</link>
      <guid>https://codingcrews.github.io/2019/01/15/deeplearning-gpu/</guid>
      <pubDate>Tue, 15 Jan 2019 07:42:51 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기&quot;&gt;&lt;a href=&quot;#Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기&quot; class=&quot;headerlink&quot; title=&quot;Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기&quot;&gt;&lt;/a
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기"><a href="#Aws-GPU-인스턴스를-이용해-딥러닝-환경-구축하기" class="headerlink" title="Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기"></a>Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기</h1><p>안녕하세요. 오늘은 아마존 웹 서비스(이하 AWS)를 이용하여 GPU 인스턴스를 이용한 딥러닝 학습환경 만들기에 대해 알아봅시다.  </p><p>딥러닝이란 최근 핫해진 뉴럴넷 기반의 기계학습 기법을 말하는데요.<br>학습 시 굉장히 많은 연산을 필요로 하여 학습에 소요되는 시간이 많이 필요합니다.<br>단순한 연산을 많이 하기 때문에 병렬처리에 특화된 GPU를 사용하여 학습시간을 단축시킬 수 있는데요.<br>당연히 좋은 GPU를 사용할수록 짧은 학습시간이 필요하겠지요.<br>하지만 문제는 이 GPU 가격이 개인이 사서 사용하기엔 너무나도 비싸다고 느낄 수 있습니다.  </p><p><img src="/resources/deeplearning-gpu/v100.png" alt="Tesla V100 가격">  </p><p><img src="/resources/deeplearning-gpu/k80.png" alt="Tesla K80 가격"></p><p>위 사진은 GPU 연산에 특화되어 나온 모델들입니다.<br>저희가 사용할 AWS EC2 P2, P3 인스턴스에 제공되는 GPU 모델들입니다.  </p><p>포스팅 날짜를 기준으로 최소 200만원 이상의 가격을 보여주고 있는데요.<br>개인이라도 사지 못할 가격은 아닙니다만, 딥러닝이 무엇인지 이제 알아가는 시점에 입문자가 구매하기엔 굉장히 벅찬 금액이죠.<br>하지만 AWS에서 제공해주는 GPU인스턴스를 빌려서 시간제로 비용을 낼 수 있다면, 비싼 GPU를 구매하지 않고도 필요할 때에만 성능이 좋은 GPU를 이용할 수 있겠죠 ?  </p><h1 id="Aws-EC2"><a href="#Aws-EC2" class="headerlink" title="Aws EC2"></a>Aws EC2</h1><p>EC2에 관한 설명은 생략하도록 하고, <a href="https://aws.amazon.com/ko/ec2/" target="_blank" rel="noopener">여기</a>에서 자세하게 확인 가능합니다.  </p><h2 id="인스턴스-종류-및-가격"><a href="#인스턴스-종류-및-가격" class="headerlink" title="인스턴스 종류 및 가격"></a>인스턴스 종류 및 가격</h2><p>저희는 GPU 인스턴스를 사용할 예정이기 때문에 P, G 두가지 계열을 확인하면 됩니다.<br><a href="https://aws.amazon.com/ko/ec2/instance-types/" target="_blank" rel="noopener">인스턴스 스펙</a>으로 이동하여 자세한 스펙을 확인 가능합니다.<br>각 인스턴스의 가격은 시간에 비례하여 비용이 청구되는 방식이며, 각 리전마다 사용가격의 편차가 있습니다.  </p><p>서울리전은 해외리전보다 비싼편이니 (약 2배가량 비쌈), 연구목적으로 사용하실 때에는 해외리전을 이용하시는걸 추천드립니다.  </p><p><img src="/resources/deeplearning-gpu/ohio.png" alt="오하이오 리전의 가격"><br><img src="/resources/deeplearning-gpu/seoul.png" alt="서울 리전의 가격">  </p><p>위 사진은 오하이오 리전의 가격이며, 아래 사진은 서울리전의 가격입니다.<br>같은 p2.xlarge의 가격을 보더라도 오하이오 리전은 시간당 \$0.9, 서울리전은 시간당 \$1.465의 가격을 보여줍니다.<br>그리고, 서울리전은 g3계열의 인스턴스가 제공되지 않고 있습니다.  </p><p>오하이오 리전의 p2.xlarge 가격을 보면 대략 시간당 1,000원 정도의 가격을 보여줍니다.<br>요새 PC방 가격도 시간당 1,000원이 넘는걸 생각하면 그리 비싸다고 생각하진 않을 수 있습니다.  </p><h2 id="스팟-인스턴스"><a href="#스팟-인스턴스" class="headerlink" title="스팟 인스턴스"></a>스팟 인스턴스</h2><p>하지만 위의 가격은 온디맨드 계약의 가격이며, AWS에서 더 할인을 받을 수 있는 방법이 존재합니다.<br>그건 바로 R.I(예약 인스턴스) 혹은 Spot인스턴스를 이용하는 방법입니다.<br>R.I의 경우는 최소 계약단위가 1년 단위이기 때문에 개인이 하기엔 부담스럽고, 오히려 서비스를 운영중인 회사에서 GPU인스턴스가 필요할 때 사용할 수 있겠습니다.  </p><p>스팟 인스턴스의 경우는 인스턴스를 경매입찰방식으로 할당받는 개념인데, 최대 90%까지 가격이 할인될 수 있습니다.  </p><p><img src="/resources/deeplearning-gpu/ohio-spot.png" alt="오하이오 리전의 스팟 인스턴스 가격"><br><img src="/resources/deeplearning-gpu/seoul-spot.png" alt="서울 리전의 스팟 인스턴스 가격"></p><p>온디맨드의 가격보다 더 저렴한 가격을 확인할 수 있습니다.<br>이 스팟 인스턴스를 이용해보도록 하겠습니다.  </p><h2 id="스팟-인스턴스-생성"><a href="#스팟-인스턴스-생성" class="headerlink" title="스팟 인스턴스 생성"></a>스팟 인스턴스 생성</h2><p><img src="/resources/deeplearning-gpu/aws-1.png" alt="스팟인스턴스 생성 - 1"></p><ol><li>AWS 관리 콘솔에서 스팟 요청을 눌러줍니다.</li></ol><p><img src="/resources/deeplearning-gpu/aws-2.png" alt="스팟인스턴스 생성 - 2"></p><ol start="2"><li>자주 쓰이는 형태의 패턴들이 이미 만들어져 있으나, 명시한 시간동안 인스턴스를 사용할 수 있도록 시간을 지정하여 사용합니다.  <blockquote><p>(최소 1시간에서 최대 6시간까지 사용 가능합니다.)</p></blockquote></li></ol><p><img src="/resources/deeplearning-gpu/aws-3.png" alt="스팟인스턴스 생성 - 3"></p><ol start="3"><li>인스턴스 타입 변경을 눌러 사용할 인스턴스의 타입을 지정합니다.<blockquote><p>저는 GPU compute에서 p2.xlarge를 이용하겠습니다.<br>GPU instance에서 조금 더 저렴한 G3 계열의 인스턴스도 사용 가능합니다.  </p></blockquote></li></ol><p><img src="/resources/deeplearning-gpu/aws-4.png" alt="스팟인스턴스 생성 - 4"></p><ol start="4"><li>AMI 검색을 눌러 사용할 기본 AMI를 지정합니다.  <blockquote><p>딥러닝 프레임워크를 사용하여 GPU를 사용하기 위해선 GPU 설정이 많이 필요한데, 이러한 사전 작업을 마친 템플릿을 AWS에서 공식 AMI로 제공합니다.<br>꼭 소유자를 확인하여 아마존 공식 이미지가 맞는지 확인합니다.<br>(일반 유저도 AMI를 만들어 배포할 수 있어 마이닝 프로그램을 설치해둔 템플릿들이 다수 존재하니 주의합시다.)  </p></blockquote></li></ol><p><img src="/resources/deeplearning-gpu/aws-5.png" alt="스팟인스턴스 생성 - 5"></p><ol start="5"><li>Additional configurations를 눌러 시큐리티 그룹 등 인스턴스에 필요한 설정을 마쳐줍시다.  </li></ol><p><img src="/resources/deeplearning-gpu/aws-6.png" alt="스팟인스턴스 생성 - 6"></p><ol start="6"><li>스팟 인스턴스가 정상적으로 생성되었습니다.<blockquote><p>간혹 해외리전의 스팟 인스턴스 제한이 걸려있어 생성되지 않는 경우가 존재합니다.<br>이때는 AWS에 케이스를 오픈하여 리밋제한을 상향요청 할 수 있습니다.</p></blockquote></li></ol><p><img src="/resources/deeplearning-gpu/aws-7.png" alt="스팟인스턴스 생성 - 7"></p><ol start="7"><li>생성된 인스턴스 확인<blockquote><p>생성된 인스턴스의 유형과 최대 가격을 볼 수 있습니다.<br>최대가격이란 명시된 시간동안 다른 인스턴스들이 높은 입찰가를 제시할 때 인스턴스가 종료 될 수 있으니 최대 가격이 온디맨드 가격으로 자동적으로 조절됩니다.  </p></blockquote></li></ol><h2 id="영구적으로-사용할-볼륨-생성하기"><a href="#영구적으로-사용할-볼륨-생성하기" class="headerlink" title="영구적으로 사용할 볼륨 생성하기"></a>영구적으로 사용할 볼륨 생성하기</h2><p>스팟 인스턴스가 종료될 시 75GB로 할당된 AMI 볼륨이 자동적으로 삭제됩니다.<br>인스턴스 생성 시 해당 볼륨을 삭제하지 않을 수 있으나, 75GB의 볼륨을 갖고 있기는 부담스럽기 때문에 우리가 필요한 데이터들만 저장할 수 있는 작은 크기의 볼륨을 따로 생성하여 인스턴스의 종료와 무관하게 사용할 수 있도록 새로운 볼륨을 만들어 추가해봅시다.  </p><p><img src="/resources/deeplearning-gpu/aws-8.png" alt="스팟인스턴스 생성 - 8"></p><ol start="8"><li>데이터 영구보존 볼륨 생성하기<blockquote><p>좌측의 볼륨 메뉴를 눌러 현재 인스턴스의 가용영역을 확인 한 이후,<br>볼륨 생성을 눌러줍니다.  </p></blockquote></li></ol><p><img src="/resources/deeplearning-gpu/aws-9.png" alt="스팟인스턴스 생성 - 9"></p><ol start="9"><li>바로 이전에 확인한 가용영역을 맞춰 새로운 볼륨을 생성합니다.<blockquote><p>가용영역이 다르다면 볼륨이 인스턴스에 사용할 수 없으니 꼭 가용영역을 맞춰 생성합니다.  </p></blockquote></li></ol><p><img src="/resources/deeplearning-gpu/aws-10.png" alt="스팟인스턴스 생성 - 10"></p><ol start="10"><li>생성된 볼륨을 확인할 수 있습니다. </li></ol><p><img src="/resources/deeplearning-gpu/aws-11.png" alt="스팟인스턴스 생성 - 11"></p><ol start="11"><li>우클릭을 이용해 볼륨 연결을 눌러줍니다.  </li></ol><p><img src="/resources/deeplearning-gpu/aws-12.png" alt="스팟인스턴스 생성 - 12"></p><ol start="12"><li>볼륨을 연결합니다.  <blockquote><p>인스턴스를 눌러주면 자동적으로 같은 가용영역에 있는 사용중인 인스턴스 목록이 나옵니다.<br>디바이스는 해당 인스턴스에 사용될 디바이스명입니다.  </p></blockquote></li></ol><p><strong>여기까지가 콘솔에서 설정 가능한 스팟 인스턴스에 대한 인스턴스 생성과 설정입니다.</strong></p><p>이후는 AWS 인스턴스에 ssh접속을 한 이후 설정하는 부분입니다.</p><h2 id="SSH-접속"><a href="#SSH-접속" class="headerlink" title="SSH 접속"></a>SSH 접속</h2><p>SSH 로 생성한 스팟 인스턴스에 접근하면 초기 화면이 이렇게 보이는걸 확인할 수 있습니다.<br>각각의 가상환경에 진입할 수 있는 명령어와 사용가능한 환경에 대한 설명이 보입니다.  </p><pre><code>=============================================================================       __|  __|_  )       _|  (     /   Deep Learning AMI (Amazon Linux) Version 20.0      ___|\___|___|=============================================================================Please use one of the following commands to start the required environment with the framework of your choice:for MXNet(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p36for MXNet(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p27for MXNet(+Amazon Elastic Inference) with Python3 _______________________________________ source activate amazonei_mxnet_p36for MXNet(+Amazon Elastic Inference) with Python2 _______________________________________ source activate amazonei_mxnet_p27for TensorFlow(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p36for TensorFlow(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p27for Tensorflow(+Amazon Elastic Inference) with Python2 _____________________________ source activate amazonei_tensorflow_p27for Theano(+Keras2) with Python3 (CUDA 9.0) _____________________________________________________ source activate theano_p36for Theano(+Keras2) with Python2 (CUDA 9.0) _____________________________________________________ source activate theano_p27for PyTorch with Python3 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p36for PyTorch with Python2 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p27for CNTK(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p36for CNTK(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p27for Caffe2 with Python2 (CUDA 9.0) ______________________________________________________________ source activate caffe2_p27for Caffe with Python2 (CUDA 8.0) ________________________________________________________________ source activate caffe_p27for Caffe with Python3 (CUDA 8.0) ________________________________________________________________ source activate caffe_p35for Chainer with Python2 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p27for Chainer with Python3 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p36for base Python2 (CUDA 9.0) ________________________________________________________________________ source activate python2for base Python3 (CUDA 9.0) ________________________________________________________________________ source activate python3Official Conda User Guide: https://conda.io/docs/user-guide/index.htmlAWS Deep Learning AMI Homepage: https://aws.amazon.com/machine-learning/amis/Developer Guide and Release Notes: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.htmlSupport: https://forums.aws.amazon.com/forum.jspa?forumID=263For a fully managed experience, check out Amazon SageMaker at https://aws.amazon.com/sagemaker=============================================================================</code></pre><h3 id="파이썬-가상환경-진입"><a href="#파이썬-가상환경-진입" class="headerlink" title="파이썬 가상환경 진입"></a>파이썬 가상환경 진입</h3><p>저희는 Python 3.6기반의 Tensorflow 사용할 수 있는 가상환경으로 진입해보겠습니다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ec2-user@ip-172-31-15-213 ~]$ source activate tensorflow_p36</span><br><span class="line"></span><br><span class="line"># 파이썬 가상환경으로 진입하면 쉘이 아래와 같이 변합니다.  </span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$</span><br></pre></td></tr></table></figure></p><h2 id="연결한-볼륨-확인"><a href="#연결한-볼륨-확인" class="headerlink" title="연결한 볼륨 확인"></a>연결한 볼륨 확인</h2><p>생성하고 나서 연결한 볼륨에 대한 정보를 확인합니다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ lsblk</span><br></pre></td></tr></table></figure><pre><code>NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINTxvda    202:0    0  75G  0 disk└─xvda1 202:1    0  75G  0 part /xvdf    202:80   0   8G  0 disk</code></pre><p>아까 연결한 8GB의 볼륨이 xvdf로 연결되어있는걸 확인할 수 있습니다.</p><h3 id="볼륨의-포맷-확인"><a href="#볼륨의-포맷-확인" class="headerlink" title="볼륨의 포맷 확인"></a>볼륨의 포맷 확인</h3><p>연결된 xvdf 디바이스를 통해 볼륨의 포맷을 확인합니다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf</span><br></pre></td></tr></table></figure></p><pre><code>/dev/xvdf: data</code></pre><p>갓 생성한 볼륨의 경우 data로 나오게 될 것이고, 이 볼륨은 사용하고 있는 os 파일 시스템에 맞춰 파일 포맷을 진행해줘야 합니다.</p><h3 id="볼륨-ext4-포맷"><a href="#볼륨-ext4-포맷" class="headerlink" title="볼륨 ext4 포맷"></a>볼륨 ext4 포맷</h3><p>볼륨의 파일시스템을 ext4 포맷으로 변경합니다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mkfs -t ext4 /dev/xvdf</span><br></pre></td></tr></table></figure></p><pre><code>mke2fs 1.43.5 (04-Aug-2017)Creating filesystem with 2097152 4k blocks and 524288 inodesFilesystem UUID: 6ce22348-395a-4ac1-8df7-e180aaadaadfSuperblock backups stored on blocks:  32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632Allocating group tables: doneWriting inode tables: doneCreating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done</code></pre><h3 id="볼륨-포맷-재확인"><a href="#볼륨-포맷-재확인" class="headerlink" title="볼륨 포맷 재확인"></a>볼륨 포맷 재확인</h3><p>파일 시스템 포맷이 완료 된 후 ext4 포맷으로 변경됬는지 확인합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf</span><br></pre></td></tr></table></figure><pre><code>/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf (extents) (64bit) (large files) (huge files)</code></pre><blockquote><p>결과에 나오는 <strong>UUID</strong>를 잘 기억하세요.</p></blockquote><h2 id="볼륨-마운트"><a href="#볼륨-마운트" class="headerlink" title="볼륨 마운트"></a>볼륨 마운트</h2><p>포맷 완료 된 볼륨을 사용하기 위해서는 마운트 과정이 필요합니다.<br>현재 Home 디렉터리에 새로운 디렉터리를 만들어 마운트를 진행합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ mkdir mount_dir</span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount /dev/xvdf mount_dir/</span><br></pre></td></tr></table></figure><blockquote><p>정상적으로 마운트 되었다면 에러 메세지 없이 완료됩니다.</p></blockquote><h3 id="볼륨-자동-마운트"><a href="#볼륨-자동-마운트" class="headerlink" title="볼륨 자동 마운트"></a>볼륨 자동 마운트</h3><p>위와 같이 수동으로 마운트 시켜 사용하는 경우 인스턴스가 재시동 되는 경우 마운트를 다시 해줘야 합니다.<br>/etc/fstab 이라는 폴더는 부팅 시 마운트 해야할 목록을 가지고 있어 자동으로 마운트가 진행되도록 합니다.<br>fstab 파일이 잘못 작성 되어 있으면 부팅이 안되는 경우도 발생하니 백업을 만들고, 신중하게 파일을 수정하도록 합시다.  </p><p><strong>아래 명령어는 복붙하지마시고 본인의 UUID와 마운트 할 디렉터리에 맞춰 변경하여 사용하세요.</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo cp /etc/fstab /etc/fstab.orig  </span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ echo &quot;UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf       /home/ec2-user/mount_dir   ext4    defaults,nofail        0       2&quot; | sudo tee --apend /etc/fstab</span><br><span class="line"></span><br><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount -a</span><br></pre></td></tr></table></figure></p><blockquote><p>마지막 mount -a 옵션을 꼭 실행해보시고, 정상적이라면 아무 메시지 없이 완료됩니다.<br>에러 메세지가 발생한 경우 fstab 파일에 문제가 있을 수 있으니 꼭 확인하세요.</p></blockquote><h2 id="Jupyter-Notebook-실행"><a href="#Jupyter-Notebook-실행" class="headerlink" title="Jupyter Notebook 실행"></a>Jupyter Notebook 실행</h2><p>원격에서 접속하기 위해 IP 대역을 전부 풀어줍니다.<br>Port 번호는 기본적으로 8888 포트를 사용하니 시큐리티 그룹에서 해당 포트를 허용시켜주시고, 다른 포트번호로 사용하셔도 무방합니다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ jupyter notebook --ip 0.0.0.0</span><br></pre></td></tr></table></figure><pre><code>[I 06:43:24.447 NotebookApp] Using EnvironmentKernelSpecManager...[I 06:43:24.448 NotebookApp] Started periodic updates of the kernel list (every 3 minutes).[I 06:43:24.552 NotebookApp] Writing notebook server cookie secret to /home/ec2-user/.local/share/jupyter/runtime/notebook_cookie_secret[I 06:43:27.661 NotebookApp] Loading IPython parallel extension[I 06:43:27.784 NotebookApp] JupyterLab beta preview extension loaded from /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/jupyterlab[I 06:43:27.784 NotebookApp] JupyterLab application directory is /home/ec2-user/anaconda3/envs/tensorflow_p36/share/jupyter/lab[I 06:43:28.405 NotebookApp] [nb_conda] enabled[I 06:43:28.408 NotebookApp] Serving notebooks from local directory: /home/ec2-user[I 06:43:28.408 NotebookApp] 0 active kernels[I 06:43:28.408 NotebookApp] The Jupyter Notebook is running at:[I 06:43:28.408 NotebookApp] http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d[I 06:43:28.408 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 06:43:28.408 NotebookApp] No web browser found: could not locate runnable browser.[C 06:43:28.408 NotebookApp]  Copy/paste this URL into your browser when you connect for the first time,  to login with a token:      http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d&amp;token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d[I 06:43:28.409 NotebookApp] Starting initial scan of virtual environments...</code></pre><blockquote><p>ec2 인스턴스의 public ip 주소가 만약 1.10.10.10 이라고 가정하면,<br><a href="http://1.10.10.10:8888" target="_blank" rel="noopener">http://1.10.10.10:8888</a> 로 접속하시면 위에 출력된 해당 토큰을 요청합니다.<br>토큰을 입력해주면 쥬피터 노트북을 사용하실 수 있습니다.</p></blockquote>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/15/deeplearning-gpu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>보스턴 집값 데이터를 이용한 선형 회귀</title>
      <link>https://codingcrews.github.io/2019/01/15/boston_linear_regression/</link>
      <guid>https://codingcrews.github.io/2019/01/15/boston_linear_regression/</guid>
      <pubDate>Mon, 14 Jan 2019 18:06:54 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;선형-회귀-문제-Linear-Regression&quot;&gt;&lt;a href=&quot;#선형-회귀-문제-Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;선형 회귀 문제 : Linear Regression&quot;&gt;&lt;/a&gt;선형 회귀 
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="선형-회귀-문제-Linear-Regression"><a href="#선형-회귀-문제-Linear-Regression" class="headerlink" title="선형 회귀 문제 : Linear Regression"></a>선형 회귀 문제 : Linear Regression</h1><p>머신러닝으로 해결할 수 있는 문제 중 분류 문제들을 이전 포스팅에서 다뤄보았고,<br>이번에는 연속된 데이터를 예측해야하는 회귀 문제를 보겠다.  </p><p>회귀 문제란 예를 들어 주택에 대한 주위 범죄율, 세율 등의 데이터가 주어졌을 때 주택 가격을 예측하는것처럼<br>연속된 값을 예측하는걸 회귀 문제라고 한다.</p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>사용하는 데이터셋은 카네기 멜런 대학교에서 제공하는 데이터셋으로 데이터의 값에 해당하는 설명은 <a href="http://lib.stat.cmu.edu/datasets/boston" target="_blank" rel="noopener">여기</a>에서 확인할 수 있다.</p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br></pre></td></tr></table></figure><h2 id="Dataset-shape-확인"><a href="#Dataset-shape-확인" class="headerlink" title="Dataset shape 확인"></a>Dataset shape 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train Datas Shape : &#123;&#125;"</span>.format(train_data.shape))</span><br><span class="line">print(<span class="string">"Train Labels Shape : &#123;&#125;"</span>.format(train_targets.shape))</span><br></pre></td></tr></table></figure><pre><code>Train Datas Shape : (404, 13)Train Labels Shape : (404,)</code></pre><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><p>각 컬럼별로 해당하는 데이터들을 확인할 수 있고, 레이블에는 해당 주택의 가격값이 들어가있는걸 확인할 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display(train_data[<span class="number">0</span>])</span><br><span class="line">display(train_targets[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,        18.72   ])array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4])</code></pre><h2 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h2><p>신경망 학습 시 차이가 큰 값을 신경망에 주입하면 학습이 잘 되지 않는다.<br>(0.01, 0.1)과 (1, 10)을 보면 비율대로만 보게되면 10배의 차이를 가지는걸 볼 수 있지만,<br>실수 체계에서 값의 차이는 어마어마하게 큰 차이로 볼 수 있다.<br>(1, 10)과 같은 데이터를 Sparse 하다라고 표현하며,<br>(0.01, 0.1)과 같은 밀도가 높은 데이터를 Dense 데이터라고 표현한다.  </p><p>이렇게 Dense 데이터로 변경하는 과정이 필요한데, 이 부분을 Scaling 혹은 일반화 한다고 한다.  </p><p>데이터의 일반화를 위해서 각 특성별 평균값을 뺀 이후에 표준편차로 나눠주게 되면,<br>특성의 중앙이 0으로, 표준편차는 1인 정규분포가 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">test_data -= mean</span><br><span class="line">test_data /= std</span><br></pre></td></tr></table></figure><h1 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h1><p>데이터셋이 작으므로 간단한 모델을 구성하는게 과대적합을 피하기가 손쉽다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_network</span><span class="params">(input_shape=<span class="params">(<span class="number">0</span>,)</span>)</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=input_shape))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'mse'</span>, metrics=[<span class="string">'mae'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>모델 구성의 마지막 Dense layer를 보면 활성함수가 지정 되어있지 않은걸 확인할 수 있는데,<br>우리가 필요한 데이터는 선형 데이터를 원하기 때문에 활성함수를 지정하지 않으면 스칼라 회귀를 위한 구성이 완성된다.<br>반대로 sigmoid 같은 활성화 함수를 적용하면 네트워크가 0~1 사이의 값을 만들도록 될 것이다.  </p><p>우리가 필요한 데이터는 주택가격을 예측하기 위함으로 활성함수를 지정하지 않고 스칼라 회귀 값을 예측하도록 구성한다.  </p><h2 id="K-Fold-Validation"><a href="#K-Fold-Validation" class="headerlink" title="K-Fold Validation"></a>K-Fold Validation</h2><p>데이터셋의 크기가 매우 작기 때문에 검증셋(Validation)의 크기도 매우 작아지게 되는데,<br>이때 데이터셋이 적어지면서 훈련세트 중 어느 한 특정 부분이 훈련세트로 사용되는지에 따라 모델의 정확도가 크게 달라질 수 있다.<br>이 이유는 훈련세트와 검증세트로 나눴을 때 각 값들의 분포도가 고르게 되지 못할 경우가 생기기 때문이다.  </p><p>이런 상황에서 가장 좋은 방법은 K-겹 교차 검증(K-Fold Cross Validation)을 실시하는것인데,<br>데이터를 K개의 분할로 나누고 각각 K개의 모델을 만들어 K-1 개의 분할에서 훈련하고 나머지 분할에서 평가를 하는법이다.<br>이 점수는 각 검증 데이터셋의 평균으로 모델을 평가하게 된다.  </p><p>즉, 여러 폴드의 데이터셋으로 나누어 교차검증을 함으로써 데이터 분포에 신경쓰지 않고 훈련세트의 모든 부분을 사용해 모델을 평가할 수 있는 장점이 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line">num_epochs = <span class="number">150</span></span><br><span class="line">all_scores = []</span><br><span class="line">all_history = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'폴드 번호 #&#123;&#125;'</span>.format(i))</span><br><span class="line">    fold_start_index = i * num_val_samples</span><br><span class="line">    fold_end_index = (i + <span class="number">1</span>) * num_val_samples</span><br><span class="line">    </span><br><span class="line">    val_data = train_data[fold_start_index : fold_end_index]</span><br><span class="line">    val_targets = train_targets[fold_start_index : fold_end_index]</span><br><span class="line">    </span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:fold_start_index], train_data[fold_end_index:]], </span><br><span class="line">        axis=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:fold_start_index], train_targets[fold_end_index:]],</span><br><span class="line">        axis=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    model = build_network((partial_train_data.shape[<span class="number">1</span>], ))</span><br><span class="line">    history = model.fit(</span><br><span class="line">        partial_train_data,</span><br><span class="line">        partial_train_targets,</span><br><span class="line">        epochs=num_epochs, </span><br><span class="line">        validation_data=(val_data, val_targets),</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        verbose=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="number">0</span>)</span><br><span class="line">    all_scores.append(val_mse)</span><br><span class="line">    all_history.append(history.history)</span><br></pre></td></tr></table></figure><pre><code>폴드 번호 #0폴드 번호 #1폴드 번호 #2폴드 번호 #3</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val_mae_lst = map(<span class="keyword">lambda</span> x: x[<span class="string">'val_mean_absolute_error'</span>], all_history)</span><br><span class="line">val_mae_lst = np.array(list(val_mae_lst))</span><br><span class="line">avg_mae = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> val_mae_lst]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h2 id="검증-데이터-시각화"><a href="#검증-데이터-시각화" class="headerlink" title="검증 데이터 시각화"></a>검증 데이터 시각화</h2><p>검증 데이터 시각화를 위해 history 데이터를 에폭별 평균 데이터로 치환시킨다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val_mae_lst = map(<span class="keyword">lambda</span> x: x[<span class="string">'val_mean_absolute_error'</span>], all_history)</span><br><span class="line">val_mae_lst = np.array(list(val_mae_lst))</span><br><span class="line">avg_mae = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> val_mae_lst]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>그래프 변동이 심하지 않도록 이전 포인트의 지수 이동 평균값으로 변경시켜주는 함수를 만든다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">.9</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(data)</span>:</span></span><br><span class="line">    smooth_data = smooth_curve(data)</span><br><span class="line">    plt.plot(range(<span class="number">1</span>, len(smooth_data) + <span class="number">1</span>), smooth_data)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(avg_mae[<span class="number">10</span>:])</span><br></pre></td></tr></table></figure><p><img src="/resources/boston_linear_regression_files/boston_linear_regression_21_0.png" alt="png"></p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/15/boston_linear_regression/#disqus_thread</comments>
    </item>
    
    <item>
      <title>로이터 데이터셋을 이용한 뉴스 기사 다중 분류</title>
      <link>https://codingcrews.github.io/2019/01/14/reuter_multi_classification/</link>
      <guid>https://codingcrews.github.io/2019/01/14/reuter_multi_classification/</guid>
      <pubDate>Mon, 14 Jan 2019 14:27:59 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;다중-분류-문제-Multiple-class-classification&quot;&gt;&lt;a href=&quot;#다중-분류-문제-Multiple-class-classification&quot; class=&quot;headerlink&quot; title=&quot;다중 분류 문제 : Multi
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="다중-분류-문제-Multiple-class-classification"><a href="#다중-분류-문제-Multiple-class-classification" class="headerlink" title="다중 분류 문제 : Multiple class classification"></a>다중 분류 문제 : Multiple class classification</h1><p>이번 포스팅에서는 2개 이상의 클래스를 가진 경우 사용할 수 있는 다중 분류 문제를 해결해보도록 한다.  </p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>로이터 데이터셋을 사용하도록 한다.<br>데이터셋은 46개의 토픽을 갖고 있으며 각 뉴스기사마다 하나의 토픽이 정해져있다.<br>즉, 단일 레이블 다중분류 문제로 볼 수 있다.</p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="Dataset-shape-확인"><a href="#Dataset-shape-확인" class="headerlink" title="Dataset shape 확인"></a>Dataset shape 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train Datas Shape : &#123;&#125;"</span>.format(train_data.shape))</span><br><span class="line">print(<span class="string">"Train Labels Shape : &#123;&#125;"</span>.format(train_labels.shape))</span><br></pre></td></tr></table></figure><pre><code>Train Datas Shape : (8982,)Train Labels Shape : (8982,)</code></pre><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><p>확인한대로 8982개의 훈련 데이터셋이 존재하며,<br>각 인덱스는 단어 인덱스의 리스트를 뜻 한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">display(train_data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line">display(train_labels)</span><br><span class="line"></span><br><span class="line">display(test_labels)</span><br></pre></td></tr></table></figure><pre><code>[1, 2, 2, 8, 43, 10, 447, 5, 25, 207]array([ 3,  4,  3, ..., 25,  3, 25])array([ 3, 10,  1, ...,  3,  3, 24])</code></pre><h3 id="단어를-텍스트로-디코딩"><a href="#단어를-텍스트로-디코딩" class="headerlink" title="단어를 텍스트로 디코딩"></a>단어를 텍스트로 디코딩</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_index = reuters.get_word_index()</span><br><span class="line">reverse_word_index = dict([(val, key) <span class="keyword">for</span> (key, val) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_news = <span class="string">' '</span>.join([reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</span><br><span class="line">print(decoded_news)</span><br></pre></td></tr></table></figure><pre><code>? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3</code></pre><h3 id="라벨-확인"><a href="#라벨-확인" class="headerlink" title="라벨 확인"></a>라벨 확인</h3><p>샘플의 라벨은 토픽의 인덱스로써 0~45의 값을 가진다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>3</code></pre><h3 id="데이터-변환"><a href="#데이터-변환" class="headerlink" title="데이터 변환"></a>데이터 변환</h3><p>학습에 용이하도록 뉴스 기사와 라벨 데이터를 벡터로 변환시킨다.<br>학습에 사용되는 데이터셋의 인풋 데이터는 해당 뉴스 기사의 들어가있는 단어 인덱스를 1.0 으로 변경시킨다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension)) <span class="comment"># 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br><span class="line"></span><br><span class="line">display(x_train.shape)</span><br><span class="line">display(x_test.shape)</span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br><span class="line"></span><br><span class="line">display(one_hot_train_labels.shape)</span><br><span class="line">display(one_hot_test_labels.shape)</span><br></pre></td></tr></table></figure><pre><code>(8982, 10000)(2246, 10000)(8982, 46)(2246, 46)</code></pre><h1 id="신경망-구성"><a href="#신경망-구성" class="headerlink" title="신경망 구성"></a>신경망 구성</h1><p>단일 레이블 다중 분류를 위한 모델을 작성 해보도록 하겠다.<br>다만 레이어 구축 시 참고해야 할 부분이 있는데,<br>각 레이어를 통과 할 때 유닛의 수가 레이블 보다 적다면 가지고 있어야 할 정보가 많이 사라질 수 있다.  </p><p>학습 시 파라미터를 줄이기 위해서나 노이즈를 줄이기 위해서 유닛을 줄이는 경우도 있지만,<br>데이터가 가지고 있어야 할 필수 데이터를 잃어버릴 수 있다는 점도 참고하여 네트워크를 구성해야 한다.</p><h2 id="신경망-네트워크-구축"><a href="#신경망-네트워크-구축" class="headerlink" title="신경망 네트워크 구축"></a>신경망 네트워크 구축</h2><p>로이터 데이터셋의 단어수를 10,000개로 제한해두었으니,<br>신경망의 입력 차원수도 10,000으로 설정한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_3 (Dense)              (None, 64)                640064    _________________________________________________________________dense_4 (Dense)              (None, 64)                4160      _________________________________________________________________dense_5 (Dense)              (None, 46)                2990      =================================================================Total params: 647,214Trainable params: 647,214Non-trainable params: 0_________________________________________________________________</code></pre><p>이진분류와 비슷하게 마지막 Dense 레이어의 아웃풋 벡터 개수는(46개) 예측하기 위한 클래스의 개수와 동일합니다.<br>다른점은 이진 분류에서는 활성함수로 sigmoid를 사용한 반면 여기서는 softmax를 이용했는데,<br>softmax는 각 클래스별로 해당 클래스 일 확률을 표시하도록 만들어집니다.   </p><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>다중 분류에서 손실함수로써는 categorical_crossentropy를 주로 사용합니다.<br>옵티마이저는 가장 빠르고 효과가 좋다고 알려진 adam 옵티마이저를 사용하도록 설정합니다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'accuracy'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="검증-데이터셋-구성"><a href="#검증-데이터셋-구성" class="headerlink" title="검증 데이터셋 구성"></a>검증 데이터셋 구성</h2><p>훈련용 데이터셋에서 Validation으로 사용할 데이터셋을 분리시킨다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure><h1 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train, </span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 1s 152us/step - loss: 3.3032 - acc: 0.4328 - val_loss: 2.6156 - val_acc: 0.5320Epoch 2/207982/7982 [==============================] - 1s 78us/step - loss: 2.0615 - acc: 0.6076 - val_loss: 1.6695 - val_acc: 0.6460Epoch 3/207982/7982 [==============================] - 1s 78us/step - loss: 1.3844 - acc: 0.7051 - val_loss: 1.3219 - val_acc: 0.7100Epoch 4/207982/7982 [==============================] - 1s 77us/step - loss: 1.0796 - acc: 0.7669 - val_loss: 1.1773 - val_acc: 0.7520Epoch 5/207982/7982 [==============================] - 1s 79us/step - loss: 0.8673 - acc: 0.8132 - val_loss: 1.0768 - val_acc: 0.7790Epoch 6/207982/7982 [==============================] - 1s 78us/step - loss: 0.6909 - acc: 0.8515 - val_loss: 0.9995 - val_acc: 0.7910Epoch 7/207982/7982 [==============================] - 1s 78us/step - loss: 0.5410 - acc: 0.8880 - val_loss: 0.9467 - val_acc: 0.8010Epoch 8/207982/7982 [==============================] - 1s 78us/step - loss: 0.4177 - acc: 0.9137 - val_loss: 0.9069 - val_acc: 0.8190Epoch 9/207982/7982 [==============================] - 1s 78us/step - loss: 0.3253 - acc: 0.9300 - val_loss: 0.8855 - val_acc: 0.8160Epoch 10/207982/7982 [==============================] - 1s 79us/step - loss: 0.2593 - acc: 0.9432 - val_loss: 0.8973 - val_acc: 0.8090Epoch 11/207982/7982 [==============================] - 1s 80us/step - loss: 0.2123 - acc: 0.9503 - val_loss: 0.8912 - val_acc: 0.8210Epoch 12/207982/7982 [==============================] - 1s 78us/step - loss: 0.1792 - acc: 0.9549 - val_loss: 0.9012 - val_acc: 0.8230Epoch 13/207982/7982 [==============================] - 1s 78us/step - loss: 0.1594 - acc: 0.9565 - val_loss: 0.9194 - val_acc: 0.8170Epoch 14/207982/7982 [==============================] - 1s 78us/step - loss: 0.1410 - acc: 0.9573 - val_loss: 0.9531 - val_acc: 0.8150Epoch 15/207982/7982 [==============================] - 1s 79us/step - loss: 0.1294 - acc: 0.9578 - val_loss: 0.9501 - val_acc: 0.8160Epoch 16/207982/7982 [==============================] - 1s 78us/step - loss: 0.1192 - acc: 0.9603 - val_loss: 0.9757 - val_acc: 0.8130Epoch 17/207982/7982 [==============================] - 1s 77us/step - loss: 0.1146 - acc: 0.9592 - val_loss: 0.9701 - val_acc: 0.8100Epoch 18/207982/7982 [==============================] - 1s 79us/step - loss: 0.1058 - acc: 0.9600 - val_loss: 0.9883 - val_acc: 0.8080Epoch 19/207982/7982 [==============================] - 1s 80us/step - loss: 0.1055 - acc: 0.9624 - val_loss: 1.0214 - val_acc: 0.8130Epoch 20/207982/7982 [==============================] - 1s 81us/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.9970 - val_acc: 0.8150</code></pre><h2 id="훈련의-정확도와-손실-시각화"><a href="#훈련의-정확도와-손실-시각화" class="headerlink" title="훈련의 정확도와 손실 시각화"></a>훈련의 정확도와 손실 시각화</h2><p>이전 이진분류 포스팅에서 사용했던 함수를 그대로 끌어와 사용하도록 한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history)</span>:</span></span><br><span class="line">    history_dict = history.history</span><br><span class="line">    accuracy = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/reuter_multi_classification_files/reuter_multi_classification_27_0.png" alt="png"></p><p>그래프를 보면 대략 8~9번째 에폭부터 과대적합이 시작되는걸 알 수 있습니다.  </p><h2 id="새로운-데이터-예측해보기"><a href="#새로운-데이터-예측해보기" class="headerlink" title="새로운 데이터 예측해보기"></a>새로운 데이터 예측해보기</h2><p>모델을 이용해 각 뉴스기사에 대한 토픽을 예측해보도록 합니다.<br>softmax를 사용하여 각 뉴스별로 46개의 토픽에 해당하는 확률을 출력합니다.<br>각 클래스 별 확률을 모두 더하면 1.0(100%)가 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br><span class="line">display(predictions.shape)</span><br></pre></td></tr></table></figure><pre><code>(2246, 46)</code></pre><p>predictions는 테스트 데이터셋의 개수에 맞게 2246개의 결과가 들어있습니다.<br>각 결과안에는 46개 클래스에 해당하는 확률값이 들어가있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>1.0</code></pre><p>각 46개의 모든 원소의 값을 모두 더하면 1.0(100%)가 된걸 확인할 수 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display(np.argmax(predictions[<span class="number">0</span>]))</span><br><span class="line">display(predictions[<span class="number">0</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure><pre><code>30.96262544</code></pre><p>predictions[0]의 3번째 인덱스가 가장 큰 값을 가진걸 확인하였고,<br>해당 인덱스의 값을 확인하였더니 0.96262544(96.262544%)로 모델이 예측한걸 확인할 수 있습니다.</p><h2 id="데이터-레이블-인코딩-방식-변경하여-학습하기"><a href="#데이터-레이블-인코딩-방식-변경하여-학습하기" class="headerlink" title="데이터 레이블 인코딩 방식 변경하여 학습하기"></a>데이터 레이블 인코딩 방식 변경하여 학습하기</h2><p>one hot 인코딩이 아닌 정수형으로 토픽을 예측하도록 레이블 인코딩을 사용하도록 변경해본다.<br>손실함수를 변경해주면 되는데,<br>categorical_crossentropy는 범주형 인코딩일 시 사용하는 손실함수이고,<br>정수형을 사용할 때에는 sparse_categorical_crossentropy를 사용한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.array(train_labels)</span><br><span class="line">y_test = np.array(test_labels)</span><br><span class="line"></span><br><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = y_train[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_6 (Dense)              (None, 64)                640064    _________________________________________________________________dense_7 (Dense)              (None, 64)                4160      _________________________________________________________________dense_8 (Dense)              (None, 46)                2990      =================================================================Total params: 647,214Trainable params: 647,214Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train, </span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">9</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/97982/7982 [==============================] - 1s 116us/step - loss: 3.3313 - acc: 0.3983 - val_loss: 2.5661 - val_acc: 0.5770Epoch 2/97982/7982 [==============================] - 1s 78us/step - loss: 2.0006 - acc: 0.6272 - val_loss: 1.6576 - val_acc: 0.6580Epoch 3/97982/7982 [==============================] - 1s 78us/step - loss: 1.3389 - acc: 0.7134 - val_loss: 1.2907 - val_acc: 0.7090Epoch 4/97982/7982 [==============================] - 1s 78us/step - loss: 1.0256 - acc: 0.7762 - val_loss: 1.1484 - val_acc: 0.7630Epoch 5/97982/7982 [==============================] - 1s 80us/step - loss: 0.8077 - acc: 0.8339 - val_loss: 1.0422 - val_acc: 0.7870Epoch 6/97982/7982 [==============================] - 1s 76us/step - loss: 0.6357 - acc: 0.8716 - val_loss: 0.9641 - val_acc: 0.8050Epoch 7/97982/7982 [==============================] - 1s 78us/step - loss: 0.4960 - acc: 0.8990 - val_loss: 0.9275 - val_acc: 0.8050Epoch 8/97982/7982 [==============================] - 1s 78us/step - loss: 0.3909 - acc: 0.9197 - val_loss: 0.8983 - val_acc: 0.8100Epoch 9/97982/7982 [==============================] - 1s 78us/step - loss: 0.3116 - acc: 0.9346 - val_loss: 0.8843 - val_acc: 0.8190</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/reuter_multi_classification_files/reuter_multi_classification_41_0.png" alt="png"></p><p>예측하는 방법은 아까와 동일하고,<br>출력된 레이블 또한 softmax처럼 각 클래스 별 확률이 나오게 된다.<br>9번의 에폭에서 과대적합 되던걸 확인하여 이번 학습은 최대 에폭을 9로 설정하여 학습을 진행한 내용의 그래프이다.</p><h1 id="추가-개선사항"><a href="#추가-개선사항" class="headerlink" title="추가 개선사항"></a>추가 개선사항</h1><p>중간 레이어의 유닛수가 너무 작게 되면 데이터에 대한 손실이 발생할 수 있지만,<br>데이터를 압축하여 노이즈를 줄이는 효과를 얻을 수도 있고, 반대로 유닛수를 크게 한다면,<br>해당 레이블을 표현하기 위한 데이터를 더 넣을 수 있게 된다고 볼 수도 있다.<br>이러한 내용을 잘 숙지하여 레이어 구성을 변경해가며 테스트를 진행해보자.</p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/14/reuter_multi_classification/#disqus_thread</comments>
    </item>
    
    <item>
      <title>IMDB 데이터셋을 이용한 영화 리뷰 분류</title>
      <link>https://codingcrews.github.io/2019/01/11/imdb/</link>
      <guid>https://codingcrews.github.io/2019/01/11/imdb/</guid>
      <pubDate>Thu, 10 Jan 2019 16:55:22 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification&quot;&gt;&lt;a href=&quot;#IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification&quot; class=&quot;headerlink&quot; title=&quot;IM
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification"><a href="#IMDB-데이터셋을-이용한-영화-리뷰-분류-Binary-Classification" class="headerlink" title="IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification"></a>IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification</h1><p>영화 평점 데이터를 통한 이진 분류</p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="Dataset-shape-확인"><a href="#Dataset-shape-확인" class="headerlink" title="Dataset shape 확인"></a>Dataset shape 확인</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train Datas Shape : &#123;&#125;"</span>.format(train_data.shape))</span><br><span class="line">print(<span class="string">"Train Labels Shape : &#123;&#125;"</span>.format(train_labels.shape))</span><br></pre></td></tr></table></figure><pre><code>Train Datas Shape : (25000,)Train Labels Shape : (25000,)</code></pre><h2 id="데이터-확인"><a href="#데이터-확인" class="headerlink" title="데이터 확인"></a>데이터 확인</h2><p>25000개의 훈련용 데이터셋이 존재하며, 각 인덱스는 단어 인덱스의 리스트</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display(train_data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line">display(train_labels)</span><br></pre></td></tr></table></figure><pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]array([1, 0, 0, ..., 0, 1, 0])</code></pre><h3 id="단어-인덱스를-단어로-치환"><a href="#단어-인덱스를-단어로-치환" class="headerlink" title="단어 인덱스를 단어로 치환"></a>단어 인덱스를 단어로 치환</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">indexes = dict([(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_review = <span class="string">' '</span>.join(indexes.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>])</span><br><span class="line">print(decoded_review)</span><br></pre></td></tr></table></figure><pre><code>? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all</code></pre><h3 id="데이터-텐서-치환"><a href="#데이터-텐서-치환" class="headerlink" title="데이터 텐서 치환"></a>데이터 텐서 치환</h3><p>단어의 개수를 10,000개로 지정해두었고, 이 단어 인덱스를 원핫인코딩으로 변환하여<br>10,000차원의 벡터로 변경시킴</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension)) <span class="comment"># 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br><span class="line"></span><br><span class="line">display(x_train.shape)</span><br><span class="line">display(x_test.shape)</span><br></pre></td></tr></table></figure><pre><code>(25000, 10000)(25000, 10000)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">display(y_train)</span><br><span class="line">display(y_test)</span><br></pre></td></tr></table></figure><pre><code>array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)array([0., 1., 1., ..., 0., 0., 0.], dtype=float32)</code></pre><h1 id="신경망-구성"><a href="#신경망-구성" class="headerlink" title="신경망 구성"></a>신경망 구성</h1><h2 id="신경망-네트워크-구축"><a href="#신경망-네트워크-구축" class="headerlink" title="신경망 네트워크 구축"></a>신경망 네트워크 구축</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense (Dense)                (None, 16)                160016    _________________________________________________________________dense_1 (Dense)              (None, 16)                272       _________________________________________________________________dense_2 (Dense)              (None, 1)                 17        =================================================================Total params: 160,305Trainable params: 160,305Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="모델-컴파일"><a href="#모델-컴파일" class="headerlink" title="모델 컴파일"></a>모델 컴파일</h2><p>모델을 사용하기 위해선 네트워크를 구성한 모델을 컴파일하는 과정이 필요하다.<br>rmsprop 옵티마이저를 사용하고,<br>확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택인데,<br>이진 분류로 각 확률을 구하는 모델이니 binary crossentropy를 사용한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> losses</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line"><span class="comment">#     optimizer='rmsprop',</span></span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line"><span class="comment">#     loss='binary_crossentropy',</span></span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line"><span class="comment">#     metrics=['accuracy']</span></span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="검증-데이터-준비-Validation"><a href="#검증-데이터-준비-Validation" class="headerlink" title="검증 데이터 준비 (Validation)"></a>검증 데이터 준비 (Validation)</h2><p>훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해<br>원본 훈련 데이터에서 10,000개의 샘플을 떼내어 검증 데이터 세트를 만듬</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure><h1 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h1><p>512의 샘플씩 미니배치를 만들어 20번의 에폭동안 훈련<br>앞에서 떼어놓은 10,000개의 데이터를 이용해 손실과 정확도를 측정</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/2015000/15000 [==============================] - 2s 139us/step - loss: 0.5883 - binary_accuracy: 0.7147 - val_loss: 0.5165 - val_binary_accuracy: 0.7686Epoch 2/2015000/15000 [==============================] - 1s 90us/step - loss: 0.4284 - binary_accuracy: 0.8771 - val_loss: 0.3974 - val_binary_accuracy: 0.8760Epoch 3/2015000/15000 [==============================] - 1s 85us/step - loss: 0.3157 - binary_accuracy: 0.9189 - val_loss: 0.3314 - val_binary_accuracy: 0.8880Epoch 4/2015000/15000 [==============================] - 1s 86us/step - loss: 0.2407 - binary_accuracy: 0.9349 - val_loss: 0.2963 - val_binary_accuracy: 0.8893Epoch 5/2015000/15000 [==============================] - 1s 85us/step - loss: 0.1912 - binary_accuracy: 0.9473 - val_loss: 0.2806 - val_binary_accuracy: 0.8906Epoch 6/2015000/15000 [==============================] - 1s 85us/step - loss: 0.1587 - binary_accuracy: 0.9555 - val_loss: 0.2811 - val_binary_accuracy: 0.8880Epoch 7/2015000/15000 [==============================] - 1s 85us/step - loss: 0.1303 - binary_accuracy: 0.9654 - val_loss: 0.2927 - val_binary_accuracy: 0.8850Epoch 8/2015000/15000 [==============================] - 1s 87us/step - loss: 0.1108 - binary_accuracy: 0.9707 - val_loss: 0.2976 - val_binary_accuracy: 0.8859Epoch 9/2015000/15000 [==============================] - 1s 87us/step - loss: 0.0930 - binary_accuracy: 0.9762 - val_loss: 0.3311 - val_binary_accuracy: 0.8771Epoch 10/2015000/15000 [==============================] - 1s 87us/step - loss: 0.0795 - binary_accuracy: 0.9802 - val_loss: 0.3353 - val_binary_accuracy: 0.8808Epoch 11/2015000/15000 [==============================] - 1s 87us/step - loss: 0.0652 - binary_accuracy: 0.9855 - val_loss: 0.3555 - val_binary_accuracy: 0.8786Epoch 12/2015000/15000 [==============================] - 1s 88us/step - loss: 0.0556 - binary_accuracy: 0.9881 - val_loss: 0.3749 - val_binary_accuracy: 0.8768Epoch 13/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0442 - binary_accuracy: 0.9919 - val_loss: 0.4263 - val_binary_accuracy: 0.8694Epoch 14/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0373 - binary_accuracy: 0.9934 - val_loss: 0.4213 - val_binary_accuracy: 0.8753Epoch 15/2015000/15000 [==============================] - 1s 90us/step - loss: 0.0305 - binary_accuracy: 0.9947 - val_loss: 0.4821 - val_binary_accuracy: 0.8657Epoch 16/2015000/15000 [==============================] - 1s 92us/step - loss: 0.0263 - binary_accuracy: 0.9955 - val_loss: 0.4871 - val_binary_accuracy: 0.8704Epoch 17/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0178 - binary_accuracy: 0.9978 - val_loss: 0.5176 - val_binary_accuracy: 0.8693Epoch 18/2015000/15000 [==============================] - 1s 88us/step - loss: 0.0155 - binary_accuracy: 0.9982 - val_loss: 0.5890 - val_binary_accuracy: 0.8639Epoch 19/2015000/15000 [==============================] - 1s 91us/step - loss: 0.0136 - binary_accuracy: 0.9980 - val_loss: 0.5645 - val_binary_accuracy: 0.8679Epoch 20/2015000/15000 [==============================] - 1s 89us/step - loss: 0.0086 - binary_accuracy: 0.9990 - val_loss: 0.5974 - val_binary_accuracy: 0.8672</code></pre><h2 id="모델의-훈련-정보-그리기"><a href="#모델의-훈련-정보-그리기" class="headerlink" title="모델의 훈련 정보 그리기"></a>모델의 훈련 정보 그리기</h2><p>위에서 fit의 반환으로 받은 history는 각각의 훈련 데이터세트와 검증 데이터세트에 대한<br>매 에폭마다의 손실율과 정확도를 가지고 있다.<br>해당 지표를 matplot를 이용해 시각화 하도록 한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(history)</span>:</span></span><br><span class="line">    history_dict = history.history</span><br><span class="line">    accuracy = history_dict[<span class="string">'binary_accuracy'</span>]</span><br><span class="line">    val_accuracy = history_dict[<span class="string">'val_binary_accuracy'</span>]</span><br><span class="line">    loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">    val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">    epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, accuracy, <span class="string">'ro'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">    plt.plot(epochs, val_accuracy, <span class="string">'r'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">    plt.title(<span class="string">'Trainging and validation accuracy and loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy and Loss'</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">              fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, -0.1))</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.1</span>),</span><br><span class="line">          fancybox=<span class="keyword">True</span>, shadow=<span class="keyword">True</span>, ncol=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#     plt.legend(bbox_to_anchor=(1, 0))</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/imdb_files/imdb_27_0.png" alt="png"></p><p>훈련 데이터셋의 그래프를(점선) 먼저 확인해보면,<br>각 에폭이 돌때마다 정확도가 오르고, 손실은 줄어드는 형태로 제대로 학습이 된것 처럼 보이나,<br>검증 데이터셋(실선)을 보게 되면 그렇지 않다.<br>각 에폭마다도 정확도는 오르지 않고, 손실이 늘어나는걸 확인할 수 있는데<br>이런 경우 훈련 데이터셋에 과대적합(overfitting) 되었다고 한다.<br>과대적합이 된 경우 모델이 새로운 데이터셋을 만났을 때 제대로 분류를 하지 못하게 된다.</p><h2 id="모델-재학습하기"><a href="#모델-재학습하기" class="headerlink" title="모델 재학습하기"></a>모델 재학습하기</h2><p>아까와 동일한 형태의 모델을 구성하고,<br>학습과 관련된 하이퍼파라미터만 변경하여 과대적합을 피해보자</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_3 (Dense)              (None, 16)                160016    _________________________________________________________________dense_4 (Dense)              (None, 16)                272       _________________________________________________________________dense_5 (Dense)              (None, 1)                 17        =================================================================Total params: 160,305Trainable params: 160,305Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 111us/step - loss: 0.5347 - binary_accuracy: 0.7908 - val_loss: 0.4127 - val_binary_accuracy: 0.8663Epoch 2/815000/15000 [==============================] - 1s 89us/step - loss: 0.3256 - binary_accuracy: 0.9005 - val_loss: 0.3316 - val_binary_accuracy: 0.8697Epoch 3/815000/15000 [==============================] - 1s 88us/step - loss: 0.2393 - binary_accuracy: 0.9232 - val_loss: 0.2822 - val_binary_accuracy: 0.8906Epoch 4/815000/15000 [==============================] - 1s 85us/step - loss: 0.1881 - binary_accuracy: 0.9410 - val_loss: 0.2801 - val_binary_accuracy: 0.8875Epoch 5/815000/15000 [==============================] - 1s 85us/step - loss: 0.1525 - binary_accuracy: 0.9524 - val_loss: 0.2770 - val_binary_accuracy: 0.8885Epoch 6/815000/15000 [==============================] - 1s 87us/step - loss: 0.1263 - binary_accuracy: 0.9611 - val_loss: 0.2856 - val_binary_accuracy: 0.8880Epoch 7/815000/15000 [==============================] - 1s 87us/step - loss: 0.1035 - binary_accuracy: 0.9701 - val_loss: 0.3127 - val_binary_accuracy: 0.8846Epoch 8/815000/15000 [==============================] - 1s 89us/step - loss: 0.0863 - binary_accuracy: 0.9754 - val_loss: 0.3270 - val_binary_accuracy: 0.8835</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/imdb_files/imdb_33_0.png" alt="png"></p><p>아까보단 과대적합을 피한 부분이 보인다.<br>손실 그래프를 확인했을 때 2에폭과 3에폭이 훈련 세트와 검증 세트가 가장 근접한 손실을 갖고있는결 확인할 수 있고,<br>정확도 또한 2,3에폭이 가장 근접한걸 확인할 수 있다.<br>즉, 이 모델의 경우 2에폭 혹은 3에폭을 돌렸을 때 과대적합을 가장 피할 수 있는 학습상태가 된다는걸 확인할 수 있다.<br>이렇게 학습에 파라미터를 조작하는 것 이외에도 과대적합을 피하는 기법이 많이 존재한다.  </p><h1 id="모델의-평가"><a href="#모델의-평가" class="headerlink" title="모델의 평가"></a>모델의 평가</h1><p>모델의 정확도를 측정한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss, accuracy = model.evaluate(x_test, y_test)</span><br><span class="line">print(<span class="string">'accuracy : &#123;acc&#125;, loss : &#123;loss&#125;'</span>.format(acc=accuracy, loss=loss))</span><br></pre></td></tr></table></figure><pre><code>25000/25000 [==============================] - 2s 67us/stepaccuracy : 0.86852, loss : 0.35010315059185027</code></pre><h1 id="모델의-예측"><a href="#모델의-예측" class="headerlink" title="모델의 예측"></a>모델의 예측</h1><p>긍정이거나 부정일 확률 (높으면 긍정, 낮으면 부정)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(x_test[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>array([[0.2251153 ],       [0.9999784 ],       [0.98094064],       [0.94734573],       [0.97099954],       [0.9737046 ],       [0.9995834 ],       [0.01185756],       [0.9645392 ],       [0.99970514]], dtype=float32)</code></pre><h1 id="번외-레이어-변경하여-정확도-개선해보기"><a href="#번외-레이어-변경하여-정확도-개선해보기" class="headerlink" title="번외. 레이어 변경하여 정확도 개선해보기"></a>번외. 레이어 변경하여 정확도 개선해보기</h1><h2 id="레이어를-한개-더-추가하여-테스트-Deep"><a href="#레이어를-한개-더-추가하여-테스트-Deep" class="headerlink" title="레이어를 한개 더 추가하여 테스트 (Deep)"></a>레이어를 한개 더 추가하여 테스트 (Deep)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_6 (Dense)              (None, 16)                160016    _________________________________________________________________dense_7 (Dense)              (None, 16)                272       _________________________________________________________________dense_8 (Dense)              (None, 16)                272       _________________________________________________________________dense_9 (Dense)              (None, 1)                 17        =================================================================Total params: 160,577Trainable params: 160,577Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 113us/step - loss: 0.5264 - binary_accuracy: 0.7816 - val_loss: 0.4348 - val_binary_accuracy: 0.8122Epoch 2/815000/15000 [==============================] - 1s 87us/step - loss: 0.2989 - binary_accuracy: 0.9006 - val_loss: 0.2936 - val_binary_accuracy: 0.8870Epoch 3/815000/15000 [==============================] - 1s 85us/step - loss: 0.2103 - binary_accuracy: 0.9263 - val_loss: 0.2941 - val_binary_accuracy: 0.8812Epoch 4/815000/15000 [==============================] - 1s 86us/step - loss: 0.1550 - binary_accuracy: 0.9481 - val_loss: 0.2963 - val_binary_accuracy: 0.8817Epoch 5/815000/15000 [==============================] - 1s 89us/step - loss: 0.1294 - binary_accuracy: 0.9543 - val_loss: 0.2956 - val_binary_accuracy: 0.8850Epoch 6/815000/15000 [==============================] - 1s 86us/step - loss: 0.0964 - binary_accuracy: 0.9709 - val_loss: 0.3357 - val_binary_accuracy: 0.8745Epoch 7/815000/15000 [==============================] - 1s 86us/step - loss: 0.0814 - binary_accuracy: 0.9739 - val_loss: 0.3678 - val_binary_accuracy: 0.8714Epoch 8/815000/15000 [==============================] - 1s 84us/step - loss: 0.0598 - binary_accuracy: 0.9834 - val_loss: 0.3910 - val_binary_accuracy: 0.8714</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/imdb_files/imdb_43_0.png" alt="png"></p><h2 id="유닛을-추가하여-테스트-Wide"><a href="#유닛을-추가하여-테스트-Wide" class="headerlink" title="유닛을 추가하여 테스트 (Wide)"></a>유닛을 추가하여 테스트 (Wide)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_10 (Dense)             (None, 64)                640064    _________________________________________________________________dense_11 (Dense)             (None, 64)                4160      _________________________________________________________________dense_12 (Dense)             (None, 1)                 65        =================================================================Total params: 644,289Trainable params: 644,289Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 135us/step - loss: 0.4850 - binary_accuracy: 0.7656 - val_loss: 0.3620 - val_binary_accuracy: 0.8517Epoch 2/815000/15000 [==============================] - 2s 107us/step - loss: 0.2538 - binary_accuracy: 0.9058 - val_loss: 0.2754 - val_binary_accuracy: 0.8902Epoch 3/815000/15000 [==============================] - 2s 103us/step - loss: 0.1857 - binary_accuracy: 0.9340 - val_loss: 0.2826 - val_binary_accuracy: 0.8874Epoch 4/815000/15000 [==============================] - 2s 103us/step - loss: 0.1417 - binary_accuracy: 0.9499 - val_loss: 0.3328 - val_binary_accuracy: 0.8734Epoch 5/815000/15000 [==============================] - 2s 104us/step - loss: 0.1128 - binary_accuracy: 0.9601 - val_loss: 0.3275 - val_binary_accuracy: 0.8826Epoch 6/815000/15000 [==============================] - 2s 104us/step - loss: 0.0795 - binary_accuracy: 0.9743 - val_loss: 0.3473 - val_binary_accuracy: 0.8802Epoch 7/815000/15000 [==============================] - 2s 103us/step - loss: 0.0544 - binary_accuracy: 0.9832 - val_loss: 0.3840 - val_binary_accuracy: 0.8780Epoch 8/815000/15000 [==============================] - 2s 107us/step - loss: 0.0514 - binary_accuracy: 0.9849 - val_loss: 0.4150 - val_binary_accuracy: 0.8789</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/imdb_files/imdb_47_0.png" alt="png"></p><h2 id="깊고-넓게-구성하기-Deep-and-wide-network"><a href="#깊고-넓게-구성하기-Deep-and-wide-network" class="headerlink" title="깊고 넓게 구성하기 (Deep and wide network)"></a>깊고 넓게 구성하기 (Deep and wide network)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.binary_crossentropy,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_13 (Dense)             (None, 64)                640064    _________________________________________________________________dense_14 (Dense)             (None, 64)                4160      _________________________________________________________________dense_15 (Dense)             (None, 32)                2080      _________________________________________________________________dense_16 (Dense)             (None, 32)                1056      _________________________________________________________________dense_17 (Dense)             (None, 1)                 33        =================================================================Total params: 647,393Trainable params: 647,393Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 142us/step - loss: 0.5094 - binary_accuracy: 0.7512 - val_loss: 0.3875 - val_binary_accuracy: 0.8442Epoch 2/815000/15000 [==============================] - 2s 105us/step - loss: 0.2693 - binary_accuracy: 0.8983 - val_loss: 0.4223 - val_binary_accuracy: 0.8310Epoch 3/815000/15000 [==============================] - 2s 105us/step - loss: 0.1949 - binary_accuracy: 0.9275 - val_loss: 0.5629 - val_binary_accuracy: 0.7950Epoch 4/815000/15000 [==============================] - 2s 104us/step - loss: 0.1534 - binary_accuracy: 0.9434 - val_loss: 0.2965 - val_binary_accuracy: 0.8854Epoch 5/815000/15000 [==============================] - 2s 105us/step - loss: 0.1106 - binary_accuracy: 0.9613 - val_loss: 0.3647 - val_binary_accuracy: 0.8718Epoch 6/815000/15000 [==============================] - 2s 104us/step - loss: 0.0797 - binary_accuracy: 0.9733 - val_loss: 0.4042 - val_binary_accuracy: 0.8748Epoch 7/815000/15000 [==============================] - 2s 110us/step - loss: 0.0802 - binary_accuracy: 0.9775 - val_loss: 0.4029 - val_binary_accuracy: 0.8815Epoch 8/815000/15000 [==============================] - 2s 106us/step - loss: 0.0656 - binary_accuracy: 0.9827 - val_loss: 0.4207 - val_binary_accuracy: 0.8809</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/imdb_files/imdb_51_0.png" alt="png"></p><h2 id="손실함수-변경"><a href="#손실함수-변경" class="headerlink" title="손실함수 변경"></a>손실함수 변경</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">    loss=losses.MSE,</span><br><span class="line">    metrics=[metrics.binary_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_18 (Dense)             (None, 64)                640064    _________________________________________________________________dense_19 (Dense)             (None, 64)                4160      _________________________________________________________________dense_20 (Dense)             (None, 32)                2080      _________________________________________________________________dense_21 (Dense)             (None, 32)                1056      _________________________________________________________________dense_22 (Dense)             (None, 1)                 33        =================================================================Total params: 647,393Trainable params: 647,393Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    partial_x_train,</span><br><span class="line">    partial_y_train,</span><br><span class="line">    epochs=<span class="number">8</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    validation_data=(x_val, y_val),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>Train on 15000 samples, validate on 10000 samplesEpoch 1/815000/15000 [==============================] - 2s 138us/step - loss: 0.1701 - binary_accuracy: 0.7462 - val_loss: 0.1152 - val_binary_accuracy: 0.8508Epoch 2/815000/15000 [==============================] - 2s 103us/step - loss: 0.0794 - binary_accuracy: 0.8991 - val_loss: 0.0829 - val_binary_accuracy: 0.8917Epoch 3/815000/15000 [==============================] - 2s 107us/step - loss: 0.0580 - binary_accuracy: 0.9281 - val_loss: 0.0892 - val_binary_accuracy: 0.8833Epoch 4/815000/15000 [==============================] - 2s 111us/step - loss: 0.0375 - binary_accuracy: 0.9544 - val_loss: 0.0858 - val_binary_accuracy: 0.8876Epoch 5/815000/15000 [==============================] - 2s 107us/step - loss: 0.0359 - binary_accuracy: 0.9560 - val_loss: 0.0874 - val_binary_accuracy: 0.8863Epoch 6/815000/15000 [==============================] - 2s 107us/step - loss: 0.0216 - binary_accuracy: 0.9751 - val_loss: 0.0927 - val_binary_accuracy: 0.8822Epoch 7/815000/15000 [==============================] - 2s 109us/step - loss: 0.0189 - binary_accuracy: 0.9782 - val_loss: 0.0946 - val_binary_accuracy: 0.8812Epoch 8/815000/15000 [==============================] - 2s 107us/step - loss: 0.0068 - binary_accuracy: 0.9931 - val_loss: 0.1091 - val_binary_accuracy: 0.8678</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_graph(history)</span><br></pre></td></tr></table></figure><p><img src="/resources/imdb_files/imdb_55_0.png" alt="png"></p>]]></content:encoded>
      
      <comments>https://codingcrews.github.io/2019/01/11/imdb/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
