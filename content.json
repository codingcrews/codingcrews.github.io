{"pages":[{"title":"About","date":"2019-01-10T06:28:13.000Z","updated":"2019-01-11T06:16:46.403Z","comments":true,"path":"about/index.html","permalink":"https://codingcrews.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-01-10T09:19:08.000Z","updated":"2019-01-10T09:19:31.122Z","comments":true,"path":"categories/index.html","permalink":"https://codingcrews.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-01-10T09:18:04.000Z","updated":"2019-01-10T09:18:25.185Z","comments":true,"path":"tags/index.html","permalink":"https://codingcrews.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"딥러닝으로 개와 고양이를 분류해보자 - 전이학습과 미세조정","slug":"transfer-learning-and-find-tuning","date":"2019-01-22T09:29:03.000Z","updated":"2019-02-01T23:14:14.183Z","comments":true,"path":"2019/01/22/transfer-learning-and-find-tuning/","link":"","permalink":"https://codingcrews.github.io/2019/01/22/transfer-learning-and-find-tuning/","excerpt":"","text":"Transfer Learning &amp; Find Tuning안녕하세요.이번 시간은 학습된 모델을 우리의 도메인에 맞춰 사용할 수 있도록 변경하는법을 알아봅시다.저번 포스팅에서 개와 고양이를 분류하는 CNN 모델을 만들어보고 학습을 진행해봤는데요.ResNet50 모델의 경우 모델이 무거운데다 저희의 학습 데이터량도 많지 않고(증식을 이용하긴 했으나, 과적합될 가능성이 존재),학습시간도 모자랐었습니다. (32에폭 도는데 대략 40분정도의 시간이 소요됨. 대회에 나간 모델의 경우 2~3주동안 학습을 진행했다고 함.) 그럼 매번 모델을 만들때마다 새로 학습을 진행해야 할까요 ?개와 고양이의 분류 모델을 만드는데 2주,개, 고양이와 거북이를 분류하는 모델을 다시 만드는데 2주… 이렇게 시간이 소요될 수 있습니다. 저희가 사용한 모델들은 CNN 레이어는 이미지의 특징들을 뽑아내주고,마지막 Fully Connected 레이어에서 그 해당 특징들을 기반으로 분류를 하는 형태를 띄는데요.그럼 기존에 학습된 모델을 가지고 특징을 추출하여 FC레이어만 새로 재학습을 할수는 없을까요 ? 오늘은 InceptionV3 모델을 이용하여 Feature Extraction, Transfer Learning을 진행해보도록 하겠습니다.(이번에 사용하는 InceptionV3 모델은 마지막 레이어가 FC 레이어가 아닌 GAP를 통해 분류를 하고 있지만, Transfer learning의 기본적인 부분은 동일합니다.) Feature Extraction어파인 레이어(Fully Connected Layer)를 거치기 바로 직전의 추상화된 레이어의 특징들을 보틀넥 피쳐라 말합니다.저희는 기존 imagenet 데이터셋을 기반으로 학습된 모델을 사용하여, 이미지의 특징을 추출하고,이 특징을 기반으로 FC레이어를 분류시킬 수 있는 모델을 만들어 보도록 하겠습니다.이 과정을 Feature Extraction 혹은 Transfer Learning이라고 말하기도 합니다. 이 예제는 마지막 분류 레이어만 학습을 진행해도 되기 때문에 CPU로도 빠른 속도로 학습이 가능합니다.이 뒤의 Fine Tuning시에는 GPU가 아닐경우 많은 시간이 소요되니 GPU 환경을 권장합니다.(피쳐 추출 부분에서 시간이 많이 소요되는건 비밀..) 데이터셋 준비데이터셋은 이전 포스팅에서 이용한 개, 고양이 데이터셋을 이용합니다. 123456789101112131415161718192021222324252627282930313233import osfrom tensorflow.keras.preprocessing.image import ImageDataGenerator# 위노그라드 알고리즘 설정 (GPU 사용시 conv 연산이 빨라짐)os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'rootPath = './datasets/cat-and-dog'# 이미지의 사이즈는 논문에서는 (224, 224, 3)을 사용하여, 빠른 학습을 위해 사이즈를 조정IMG_SIZE = (150, 150, 3) imageGenerator = ImageDataGenerator( rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, brightness_range=[.2, .2], horizontal_flip=True)trainGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=IMG_SIZE[:2], batch_size=20)testGen = ImageDataGenerator( rescale=1./255,).flow_from_directory( os.path.join(rootPath, 'test_set'), target_size=IMG_SIZE[:2], batch_size=20,) Found 8005 images belonging to 2 classes. Found 2023 images belonging to 2 classes. 모델 구성모델은 피쳐추출기로 사용할 InceptionV3 모델과 이를 분류할 분류기 두개를 만들겠습니다.이미지의 특징을 추출해주니 꼭 신경망이 아니더라도 다른 분류기를(예를 들면 SVM 등) 사용하셔도 무방합니다. 12345678910111213141516from tensorflow.keras import layersfrom tensorflow.keras.models import Model, Sequentialfrom tensorflow.keras.applications import InceptionV3# Fc 레이어를 포함하지 않고, imagenet기반으로 학습된 가중치를 로드한 뒤 GAP 레이어를 추가extractor = Sequential()extractor.add(InceptionV3(include_top=False, weights='imagenet', input_shape=IMG_SIZE))extractor.add(layers.GlobalAveragePooling2D())extractor_output_shape = extractor.get_output_shape_at(0)[1:]model = Sequential()model.add(layers.InputLayer(input_shape=extractor_output_shape))model.add(layers.Dense(2, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 2) 4098 ================================================================= Total params: 4,098 Trainable params: 4,098 Non-trainable params: 0 _________________________________________________________________ 모델 컴파일12345678910111213from tensorflow.keras.optimizers import Adamoptimizer = Adam(lr=0.001)model.compile( optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'],)extractor.compile( optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'],) 특징 추출기 생성이미지 제너레이터에서 특징을 추출하기 위한 펑션을 하나 만들어 줍니다. 123456789101112131415161718import numpy as npdef get_features(extractor, gen, cnt): bs = gen.batch_size ds = cnt extractor_shapes = list(extractor.get_output_shape_at(0)[1:]) features = np.empty([0] + extractor_shapes) labels = np.empty((0, 2)) for i, (trainX, trainY) in enumerate(gen): features = np.append(features, extractor.predict(trainX), axis=0) labels = np.append(labels, trainY, axis=0) print('batch index: &#123;&#125;/&#123;&#125;'.format(i * bs, ds), end='\\r') if bs * i &gt;= cnt: break print() return features, labels 특징추출12trainX, trainY = get_features(extractor, trainGen, 3000)testX, testY = get_features(extractor, testGen, 1000) batch index: 3000/3000 batch index: 1000/1000 학습 시작123456789epochs = 32history = model.fit( trainX, trainY, epochs=epochs, batch_size=32, validation_split=.1,) Train on 2718 samples, validate on 302 samples Epoch 1/32 2718/2718 [==============================] - 2s 575us/step - loss: 0.3848 - acc: 0.8214 - val_loss: 0.2531 - val_acc: 0.8791 Epoch 2/32 2718/2718 [==============================] - 0s 95us/step - loss: 0.2236 - acc: 0.9031 - val_loss: 0.2380 - val_acc: 0.8891 Epoch 3/32 2718/2718 [==============================] - 0s 88us/step - loss: 0.1946 - acc: 0.9187 - val_loss: 0.2312 - val_acc: 0.8891 Epoch 4/32 2718/2718 [==============================] - 0s 88us/step - loss: 0.1733 - acc: 0.9284 - val_loss: 0.2322 - val_acc: 0.8907 Epoch 5/32 2718/2718 [==============================] - 0s 90us/step - loss: 0.1541 - acc: 0.9391 - val_loss: 0.2334 - val_acc: 0.8940 Epoch 6/32 2718/2718 [==============================] - 0s 93us/step - loss: 0.1441 - acc: 0.9430 - val_loss: 0.2444 - val_acc: 0.8957 Epoch 7/32 2718/2718 [==============================] - 0s 86us/step - loss: 0.1337 - acc: 0.9513 - val_loss: 0.2425 - val_acc: 0.8825 Epoch 8/32 2718/2718 [==============================] - 0s 88us/step - loss: 0.1295 - acc: 0.9507 - val_loss: 0.2432 - val_acc: 0.8940 Epoch 9/32 2718/2718 [==============================] - 0s 87us/step - loss: 0.1211 - acc: 0.9533 - val_loss: 0.2524 - val_acc: 0.8974 Epoch 10/32 2718/2718 [==============================] - 0s 90us/step - loss: 0.1143 - acc: 0.9573 - val_loss: 0.2539 - val_acc: 0.8924 Epoch 11/32 2718/2718 [==============================] - 0s 89us/step - loss: 0.1123 - acc: 0.9562 - val_loss: 0.2560 - val_acc: 0.8874 Epoch 12/32 2718/2718 [==============================] - 0s 85us/step - loss: 0.1009 - acc: 0.9639 - val_loss: 0.2659 - val_acc: 0.8940 Epoch 13/32 2718/2718 [==============================] - 0s 85us/step - loss: 0.0955 - acc: 0.9682 - val_loss: 0.2550 - val_acc: 0.8924 Epoch 14/32 2718/2718 [==============================] - 0s 91us/step - loss: 0.0901 - acc: 0.9673 - val_loss: 0.2738 - val_acc: 0.8940 Epoch 15/32 2718/2718 [==============================] - 0s 90us/step - loss: 0.0897 - acc: 0.9678 - val_loss: 0.2909 - val_acc: 0.8907 Epoch 16/32 2718/2718 [==============================] - 0s 87us/step - loss: 0.0843 - acc: 0.9722 - val_loss: 0.2811 - val_acc: 0.8891 Epoch 17/32 2718/2718 [==============================] - 0s 87us/step - loss: 0.0812 - acc: 0.9731 - val_loss: 0.2788 - val_acc: 0.8924 Epoch 18/32 2718/2718 [==============================] - 0s 90us/step - loss: 0.0800 - acc: 0.9737 - val_loss: 0.3198 - val_acc: 0.8891 Epoch 19/32 2718/2718 [==============================] - 0s 89us/step - loss: 0.0746 - acc: 0.9748 - val_loss: 0.2781 - val_acc: 0.8940 Epoch 20/32 2718/2718 [==============================] - 0s 85us/step - loss: 0.0759 - acc: 0.9748 - val_loss: 0.2898 - val_acc: 0.8924 Epoch 21/32 2718/2718 [==============================] - 0s 87us/step - loss: 0.0704 - acc: 0.9774 - val_loss: 0.2810 - val_acc: 0.8990 Epoch 22/32 2718/2718 [==============================] - 0s 88us/step - loss: 0.0660 - acc: 0.9792 - val_loss: 0.2831 - val_acc: 0.8940 Epoch 23/32 2718/2718 [==============================] - 0s 88us/step - loss: 0.0631 - acc: 0.9825 - val_loss: 0.2886 - val_acc: 0.8974 Epoch 24/32 2718/2718 [==============================] - 0s 89us/step - loss: 0.0611 - acc: 0.9827 - val_loss: 0.2841 - val_acc: 0.8990 Epoch 25/32 2718/2718 [==============================] - 0s 89us/step - loss: 0.0599 - acc: 0.9829 - val_loss: 0.2893 - val_acc: 0.8974 Epoch 26/32 2718/2718 [==============================] - 0s 89us/step - loss: 0.0575 - acc: 0.9845 - val_loss: 0.2965 - val_acc: 0.8990 Epoch 27/32 2718/2718 [==============================] - 0s 89us/step - loss: 0.0556 - acc: 0.9875 - val_loss: 0.2971 - val_acc: 0.9007 Epoch 28/32 2718/2718 [==============================] - 0s 90us/step - loss: 0.0531 - acc: 0.9860 - val_loss: 0.3044 - val_acc: 0.9007 Epoch 29/32 2718/2718 [==============================] - 0s 88us/step - loss: 0.0512 - acc: 0.9875 - val_loss: 0.3063 - val_acc: 0.8974 Epoch 30/32 2718/2718 [==============================] - 0s 87us/step - loss: 0.0498 - acc: 0.9893 - val_loss: 0.3097 - val_acc: 0.8990 Epoch 31/32 2718/2718 [==============================] - 0s 91us/step - loss: 0.0499 - acc: 0.9880 - val_loss: 0.3155 - val_acc: 0.8924 Epoch 32/32 2718/2718 [==============================] - 0s 94us/step - loss: 0.0463 - acc: 0.9904 - val_loss: 0.3197 - val_acc: 0.8957 결과 시각화12345678910111213141516171819202122232425262728293031323334353637383940414243import matplotlib.pyplot as pltdef show_graph(history_dict): accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5) plt.show() def smooth_curve(points, factor=.8): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 1show_graph(history.history) 1234smooth_data = &#123;&#125;for key, val in history.history.items(): smooth_data[key] = smooth_curve(val[:])show_graph(smooth_data) 1model.evaluate(testX, testY) 1020/1020 [==============================] - 0s 59us/step [0.16686982696547228, 0.9480392149850434] imagenet으로 학습한 모델을 이용했을 때 이미지의 분류를 위한 특징을 잘 뽑아내도록 필터가 학습되어 있으므로,첫 에폭에서부터 높은 결과를 보여주는걸 확인할 수 있습니다. Fine TuningFind Tuning이란 미세조정인데요.아무래도 기존 학습된 네트워크는 학습한 데이터셋에 잘맞도록 학습이 되어 있겠죠.그렇다고 우리 데이터셋으로 밑바닥부터 학습시키기엔 데이터를 모으기가 힘든 경우가 대다수입니다.이런 경우에 학습된 네트워크를 미세조정하여 우리의 데이터셋에 조금 더 잘 맞도록 모델의 파라메터를 조정하는 과정을 거치는데요.이 과정을 Fine Tuning이라고 합니다. 하지만 연산량이 많아지기 때문에 위의 Transfer Learning보다 상대적으로 시간이 조금 더 필요하게 됩니다.Fine Tuning은 CPU로 하기엔 연산량이 많아 GPU 사용을 권장합니다. 미세조정의 순서는 대략 아래와 같습니다. 분류기를 학습한다. (이때 분류기를 제외한 모델의 모든 레이어는 가중치를 업데이트 되지 않도록 동결(Freeze) 시킵니다) 모델의 마지막 레이어들을 미세조정을 실시한다. 이렇게 먼저 분류기를 학습하는 이유는 저희가 추가한 분류기의 가중치는 랜덤하게 초기화가 되어 있습니다.이러한 분류기를 통해 그대로 보틀넥 레이어를 재학습 할 경우, Loss가 높아 기존의 레이어가 망가질 우려가 있습니다.기존 학습된 모델의 가중치를 최대한 미세하게 조정하기 위하여 분류기를 선학습 한 이후, 보틀넥 레이어를 다시 재학습 하도록 합니다. 가장 중요한 부분은 모델의 가중치 동결인데요.기존 학습된 모델의 경우 앞의 레이어들은 이미지를 직관적으로 이해할 수 있도록 이미지 자체의 엣지 디텍터등의 역할을 수행합니다.레이어가 뒤로 향할수록 필터에 대한 데이터를 추상적으로 표현하게 되는데, 앞단의 레이어들의 가중치를 동결함으로써 학습 시 필터의 역할이 망가지지 않도록 하는 역할을 합니다. 데이터셋 준비위에서 사용한 제너레이터를 조금 변경하여 사용하도록 하겠습니다. 12345678910111213141516171819202122232425262728293031323334rootPath = './datasets/cat-and-dog'IMG_SIZE = (150, 150, 3) imageGenerator = ImageDataGenerator( rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, brightness_range=[.2, .2], horizontal_flip=True, validation_split=0.1,)trainGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=IMG_SIZE[:2], batch_size=32, subset='training',)validationGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=IMG_SIZE[:2], batch_size=32, subset='validation',)testGen = ImageDataGenerator( rescale=1./255,).flow_from_directory( os.path.join(rootPath, 'test_set'), target_size=IMG_SIZE[:2], batch_size=32,) Found 7205 images belonging to 2 classes. Found 800 images belonging to 2 classes. Found 2023 images belonging to 2 classes. 모델 구성위에서 사용한 InceptionV3 모델을 이용해도 되지만, 저희는 새로운 모델을 만들어 진행해보겠습니다. 12345678910111213inception = InceptionV3(include_top=False, weights='imagenet', input_shape=IMG_SIZE)gap = layers.GlobalAveragePooling2D()(inception.output)# 마지막 31번째 뒤 레이어들을 제외한 모든 레이어를 가중치 동결합니다.for l in inception.layers[:-31]: l.trainable = False# 위의 Transfer Learning에서 훈련시킨 분류기를 이용합니다.classifier = modelmodel = Model(inception.input, classifier(gap))model.summary() __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 150, 150, 3) 0 __________________________________________________________________________________________________ conv2d_94 (Conv2D) (None, 74, 74, 32) 864 input_3[0][0] __________________________________________________________________________________________________ batch_normalization_94 (BatchNo (None, 74, 74, 32) 96 conv2d_94[0][0] __________________________________________________________________________________________________ activation_94 (Activation) (None, 74, 74, 32) 0 batch_normalization_94[0][0] __________________________________________________________________________________________________ conv2d_95 (Conv2D) (None, 72, 72, 32) 9216 activation_94[0][0] 중간 생략 ... __________________________________________________________________________________________________ concatenate_3 (Concatenate) (None, 3, 3, 768) 0 activation_185[0][0] activation_186[0][0] __________________________________________________________________________________________________ activation_187 (Activation) (None, 3, 3, 192) 0 batch_normalization_187[0][0] __________________________________________________________________________________________________ mixed10 (Concatenate) (None, 3, 3, 2048) 0 activation_179[0][0] mixed9_1[0][0] concatenate_3[0][0] activation_187[0][0] __________________________________________________________________________________________________ global_average_pooling2d_1 (Glo (None, 2048) 0 mixed10[0][0] __________________________________________________________________________________________________ sequential_1 (Sequential) (None, 2) 4098 global_average_pooling2d_1[0][0] ================================================================================================== Total params: 21,806,882 Trainable params: 6,077,634 Non-trainable params: 15,729,248 __________________________________________________________________________________________________ 모델의 구성을 잘 보시면 총 파라메터의 개수는 약 2,200만개의 파라메터를 가지고 있지만,훈련이 가능한 파라메터의 수는 약 600만개 입니다.나머지 1,500만개의 파라메터는 저희가 설정한대로 가중치 업데이트가 동결 되었습니다. 모델 컴파일모델의 미세조정을 위해 러닝레이트를 줄여 학습을 진행합니다. 123456optimizer = Adam(lr=0.0001)model.compile( optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'],) 학습 시작12345678epochs = 32history = model.fit_generator( generator=trainGen, epochs=epochs, steps_per_epoch=trainGen.n / trainGen.batch_size, validation_data=validationGen, validation_steps=validationGen.n / validationGen.batch_size,) Epoch 1/32 226/225 [==============================] - 57s 253ms/step - loss: 0.2900 - acc: 0.8724 - val_loss: 0.3690 - val_acc: 0.9169 Epoch 2/32 226/225 [==============================] - 55s 245ms/step - loss: 0.2683 - acc: 0.8868 - val_loss: 0.4129 - val_acc: 0.9081 Epoch 3/32 226/225 [==============================] - 56s 246ms/step - loss: 0.2487 - acc: 0.8915 - val_loss: 0.4223 - val_acc: 0.9081 Epoch 4/32 226/225 [==============================] - 56s 246ms/step - loss: 0.2156 - acc: 0.9079 - val_loss: 0.4608 - val_acc: 0.9131 Epoch 5/32 226/225 [==============================] - 55s 244ms/step - loss: 0.2214 - acc: 0.9061 - val_loss: 0.3838 - val_acc: 0.9144 Epoch 6/32 226/225 [==============================] - 55s 243ms/step - loss: 0.1986 - acc: 0.9154 - val_loss: 0.4782 - val_acc: 0.9019 Epoch 7/32 226/225 [==============================] - 55s 245ms/step - loss: 0.1875 - acc: 0.9258 - val_loss: 0.3910 - val_acc: 0.9313 Epoch 8/32 226/225 [==============================] - 55s 243ms/step - loss: 0.1663 - acc: 0.9322 - val_loss: 0.4091 - val_acc: 0.9250 Epoch 9/32 226/225 [==============================] - 55s 241ms/step - loss: 0.1813 - acc: 0.9256 - val_loss: 0.5089 - val_acc: 0.9163 Epoch 10/32 226/225 [==============================] - 55s 244ms/step - loss: 0.1742 - acc: 0.9275 - val_loss: 0.4822 - val_acc: 0.9094 Epoch 11/32 226/225 [==============================] - 55s 242ms/step - loss: 0.1580 - acc: 0.9341 - val_loss: 0.4449 - val_acc: 0.9137 Epoch 12/32 226/225 [==============================] - 56s 247ms/step - loss: 0.1556 - acc: 0.9375 - val_loss: 0.5008 - val_acc: 0.9163 Epoch 13/32 226/225 [==============================] - 55s 243ms/step - loss: 0.1676 - acc: 0.9314 - val_loss: 0.4137 - val_acc: 0.9087 Epoch 14/32 226/225 [==============================] - 55s 244ms/step - loss: 0.1466 - acc: 0.9375 - val_loss: 0.6155 - val_acc: 0.8925 Epoch 15/32 226/225 [==============================] - 55s 244ms/step - loss: 0.1364 - acc: 0.9459 - val_loss: 0.5151 - val_acc: 0.9125 Epoch 16/32 226/225 [==============================] - 55s 243ms/step - loss: 0.1325 - acc: 0.9458 - val_loss: 0.5785 - val_acc: 0.9025 Epoch 17/32 226/225 [==============================] - 56s 246ms/step - loss: 0.1303 - acc: 0.9496 - val_loss: 0.5260 - val_acc: 0.9081 Epoch 18/32 226/225 [==============================] - 55s 242ms/step - loss: 0.1337 - acc: 0.9463 - val_loss: 0.5409 - val_acc: 0.9106 Epoch 19/32 226/225 [==============================] - 56s 247ms/step - loss: 0.1189 - acc: 0.9550 - val_loss: 0.4270 - val_acc: 0.9181 Epoch 20/32 226/225 [==============================] - 55s 245ms/step - loss: 0.1166 - acc: 0.9532 - val_loss: 0.5129 - val_acc: 0.9250 Epoch 21/32 226/225 [==============================] - 55s 244ms/step - loss: 0.1117 - acc: 0.9556 - val_loss: 0.7072 - val_acc: 0.8981 Epoch 22/32 226/225 [==============================] - 56s 247ms/step - loss: 0.1155 - acc: 0.9562 - val_loss: 0.4914 - val_acc: 0.9187 Epoch 23/32 226/225 [==============================] - 56s 247ms/step - loss: 0.1055 - acc: 0.9602 - val_loss: 0.6032 - val_acc: 0.9006 Epoch 24/32 226/225 [==============================] - 55s 246ms/step - loss: 0.1140 - acc: 0.9549 - val_loss: 0.4621 - val_acc: 0.9044 Epoch 25/32 226/225 [==============================] - 55s 244ms/step - loss: 0.1158 - acc: 0.9532 - val_loss: 0.5440 - val_acc: 0.9144 Epoch 26/32 226/225 [==============================] - 55s 241ms/step - loss: 0.1042 - acc: 0.9628 - val_loss: 0.5088 - val_acc: 0.9100 Epoch 27/32 226/225 [==============================] - 55s 245ms/step - loss: 0.0982 - acc: 0.9605 - val_loss: 0.5012 - val_acc: 0.9200 Epoch 28/32 226/225 [==============================] - 55s 242ms/step - loss: 0.1063 - acc: 0.9577 - val_loss: 0.5262 - val_acc: 0.9056 Epoch 29/32 226/225 [==============================] - 55s 241ms/step - loss: 0.0933 - acc: 0.9637 - val_loss: 0.4498 - val_acc: 0.9294 Epoch 30/32 226/225 [==============================] - 54s 241ms/step - loss: 0.0916 - acc: 0.9658 - val_loss: 0.5562 - val_acc: 0.9044 Epoch 31/32 226/225 [==============================] - 55s 244ms/step - loss: 0.0870 - acc: 0.9666 - val_loss: 0.4454 - val_acc: 0.9169 Epoch 32/32 226/225 [==============================] - 55s 245ms/step - loss: 0.0857 - acc: 0.9660 - val_loss: 0.5616 - val_acc: 0.9119 모델 평가 및 시각화1model.evaluate_generator(testGen) [0.2855920347539756, 0.95551161632288] 테스트셋에 위의 분류기보다 조금 더 높은 정확도를 확인할 수 있습니다.정학도가 높아질수록 정확도 1% 올리는게 더더욱 힘들어지죠 1show_graph(history.history) 1234smooth_data = &#123;&#125;for key, val in history.history.items(): smooth_data[key] = smooth_curve(val[:])show_graph(smooth_data) 5~10에폭에서부터 과대적합이 시작되는것처럼 보이는데요.이 예제에서는 분류기를 이전에 학습한걸 그대로 사용하였으나, 여러가지 방법으로 테스트를 진행해보세요.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"},{"name":"CNN","slug":"cnn","permalink":"https://codingcrews.github.io/tags/cnn/"}]},{"title":"딥러닝으로 개와 고양이를 분류하는 모델을 만들어보자 - ResNet50","slug":"cat-dog-resnet","date":"2019-01-18T18:21:43.000Z","updated":"2019-01-29T18:38:26.920Z","comments":true,"path":"2019/01/19/cat-dog-resnet/","link":"","permalink":"https://codingcrews.github.io/2019/01/19/cat-dog-resnet/","excerpt":"","text":"사진으로 개 고양이 분류하기 - Ver. ResNet안녕하세요.저번 포스팅에서는 일반적인 CNN을 이용하여 네트워크를 구성하고, 개와 고양이 사진을 분류시키는 모델을 만들어 봤는데요.이번 시간에는 대회에서 사람보다 적은 에러율을 보였던 ResNet을 이용하여 모델을 만들어 보겠습니다. 토이 프로젝트의 튜토리얼이지만 모델 자체가 굉장히 무겁다보니 CPU로는 극악의 시간이 소요됩니다.꼭 GPU 환경을 구축하시고 진행하세요. 데스크톱으로 GPU 학습환경 구성이 어려우시다면, AWS로 GPU기반의 딥러닝 학습환경 구축하기 포스팅을 참고하세요. ResNetResNet 모델을 사용하기 전에 간단하게만 알아봅시다.참고한 논문은 여기에서 확인하실 수 있고, 가장 흥미로운 부분은 Residual Network 입니다. 기존의 CNN 학습 방법으로 VGG의 구현을 살짝 보겠습니다.위 이미지에서 보면 좌측의 네트워크는 저희가 지금까지 만든 각 레이어를 쭉 일자로 연결한 형태이고,우측의 Residual을 보면 이전 레이어에서 나온 데이터를 한단계를 건너뛰어 재사용하는걸 볼 수 있습니다.좌측의 평평한 레이어에서 하나의 지름길을 만들어 한단계를 넘어 다음 레이어에 연결시킨 부분이 인상적입니다.이러한 부분으로 인해 VGG모델에 비해 깊은 레이어를 가지고 있지만, 실제 사용시에는 굉장히 빠른 속도를 보여줍니다.다만 학습시간이 오래걸리는건 당연하겠죠 ? 어떤 강의에서는 이 Residual network를 Fast network라고 표현한답니다. 좀 더 자세한 설명은 해당 논문을 참고하시기 바랍니다. Dataset데이터셋은 이전 포스팅과 마찬가지로 동일한 데이터셋인 kaggle에서 제공하는 데이터셋을 이용할 예정입니다. Download직접 데이터셋을 배포하진 않고 있습니다.데이터셋은 여기에서 직접 다운로드 받으실 수 있고, kaggle api를 통하여 받으실 수 있습니다. 저희는 저번 포스팅과 동일하게 datasets이란 폴더에 데이터셋을 넣어두고 사용하도록 하겠습니다. 123456789101112datasets└── cat-and-dog ├── test_set │ ├── cats │ │ └── datas... │ └── dogs │ └── datas... └── training_set ├── cats │ └── datas... └── dogs └── datas... 이와 같은 형태의 구성이 되어 있다고 가정합니다. 데이터 로드마찬가지로 저번 포스팅과 동일하게 데이터 증식을 위한 제너레이터로 데이터를 읽어옵니다.관련된 설명은 이전 포스팅을 참고해주세요. 1234567891011121314151617181920212223242526272829import osfrom tensorflow.keras.preprocessing.image import ImageDataGenerator# 위노그라드 알고리즘 설정os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'rootPath = './datasets/cat-and-dog'imageGenerator = ImageDataGenerator( rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, brightness_range=[.2, .2], horizontal_flip=True, validation_split=.1)trainGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=(64, 64), subset='training')validationGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=(64, 64), subset='validation') Found 7205 images belonging to 2 classes. Found 800 images belonging to 2 classes. 모델 구성텐서플로 케라스에서 ResNet 모델을 제공해주고 있습니다.저희는 해당 모델의 구성을 그대로 가져다 사용하도록 하겠습니다. 12345678from tensorflow.keras.applications import ResNet50from tensorflow.keras.models import Sequentialfrom tensorflow.keras import layersmodel = Sequential()model.add(ResNet50(include_top=True, weights=None, input_shape=(64, 64, 3), classes=2))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= resnet50 (Model) (None, 2) 23591810 ================================================================= Total params: 23,591,810 Trainable params: 23,538,690 Non-trainable params: 53,120 _________________________________________________________________ include_top 모델의 최상위 Fully Connected 레이어를 추가할것인지 추가하지 않을것인지에 대한 토글입니다. weights 기존 학습된 가중치 데이터를 읽어들일 가중치 경로입니다. imagenet이라 입력하면 imagenet 데이터셋을 기반으로 기존 학습된 가중치 데이터를 불러옵니다. input_shape 입력값의 형태를 지정합니다. 3D 텐서를 입력해야합니다. classes 모델의 마지막 출력 차원을 지정합니다. 저희는 ResNet50 모델을 이용하여 개와 고양이를 학습하는 모델을 학습시켜보도록 하겠습니다. 모델 컴파일모델의 구성이 완료된 이후에는 항상 컴파일이 필요합니다. 12345model.compile( optimizer='adam', loss='binary_crossentropy', metrics=['acc'],) 학습 시작ResNet 모델의 학습이 시작됩니다. 12345678epochs = 32history = model.fit_generator( trainGen, epochs=epochs, steps_per_epoch=trainGen.samples / epochs, validation_data=validationGen, validation_steps=trainGen.samples / epochs,) Epoch 1/32 226/225 [==============================] - 100s 441ms/step - loss: 1.0603 - acc: 0.5530 - val_loss: 0.9258 - val_acc: 0.5022 Epoch 2/32 226/225 [==============================] - 80s 355ms/step - loss: 0.9658 - acc: 0.6102 - val_loss: 0.8094 - val_acc: 0.5333 Epoch 3/32 226/225 [==============================] - 80s 354ms/step - loss: 0.9366 - acc: 0.6113 - val_loss: 4.5736 - val_acc: 0.5488 Epoch 4/32 226/225 [==============================] - 80s 354ms/step - loss: 0.9635 - acc: 0.5666 - val_loss: 4.1998 - val_acc: 0.5292 Epoch 5/32 226/225 [==============================] - 80s 354ms/step - loss: 0.8955 - acc: 0.6076 - val_loss: 2.3943 - val_acc: 0.5353 Epoch 6/32 226/225 [==============================] - 80s 353ms/step - loss: 0.8526 - acc: 0.6103 - val_loss: 1.5856 - val_acc: 0.6340 Epoch 7/32 226/225 [==============================] - 80s 354ms/step - loss: 0.8663 - acc: 0.6193 - val_loss: 0.6723 - val_acc: 0.6033 Epoch 8/32 226/225 [==============================] - 80s 353ms/step - loss: 0.8656 - acc: 0.5956 - val_loss: 0.9622 - val_acc: 0.6312 Epoch 9/32 226/225 [==============================] - 80s 354ms/step - loss: 0.7594 - acc: 0.6465 - val_loss: 0.6115 - val_acc: 0.6655 Epoch 10/32 226/225 [==============================] - 80s 354ms/step - loss: 0.8273 - acc: 0.6457 - val_loss: 5.6990 - val_acc: 0.5010 Epoch 11/32 226/225 [==============================] - 80s 355ms/step - loss: 0.8979 - acc: 0.5210 - val_loss: 0.7881 - val_acc: 0.5429 Epoch 12/32 226/225 [==============================] - 80s 355ms/step - loss: 0.7378 - acc: 0.5575 - val_loss: 0.7642 - val_acc: 0.5741 Epoch 13/32 226/225 [==============================] - 80s 354ms/step - loss: 0.6924 - acc: 0.5781 - val_loss: 0.7743 - val_acc: 0.5873 Epoch 14/32 226/225 [==============================] - 80s 355ms/step - loss: 0.6797 - acc: 0.5925 - val_loss: 0.6813 - val_acc: 0.5864 Epoch 15/32 226/225 [==============================] - 80s 353ms/step - loss: 0.6758 - acc: 0.6071 - val_loss: 0.6564 - val_acc: 0.6355 Epoch 16/32 226/225 [==============================] - 80s 352ms/step - loss: 0.6618 - acc: 0.6306 - val_loss: 0.6620 - val_acc: 0.6398 Epoch 17/32 226/225 [==============================] - 80s 354ms/step - loss: 0.6850 - acc: 0.5840 - val_loss: 0.6861 - val_acc: 0.5525 Epoch 18/32 226/225 [==============================] - 80s 355ms/step - loss: 0.6746 - acc: 0.5918 - val_loss: 0.6706 - val_acc: 0.5904 Epoch 19/32 226/225 [==============================] - 81s 357ms/step - loss: 0.6513 - acc: 0.6269 - val_loss: 0.6519 - val_acc: 0.6323 Epoch 20/32 226/225 [==============================] - 80s 353ms/step - loss: 0.6425 - acc: 0.6363 - val_loss: 0.6568 - val_acc: 0.6289 Epoch 21/32 226/225 [==============================] - 80s 353ms/step - loss: 0.6306 - acc: 0.6528 - val_loss: 0.6282 - val_acc: 0.6532 Epoch 22/32 226/225 [==============================] - 80s 356ms/step - loss: 0.6204 - acc: 0.6574 - val_loss: 0.6147 - val_acc: 0.6574 Epoch 23/32 226/225 [==============================] - 80s 352ms/step - loss: 0.5968 - acc: 0.6823 - val_loss: 0.6732 - val_acc: 0.6366 Epoch 24/32 226/225 [==============================] - 80s 353ms/step - loss: 0.5842 - acc: 0.6910 - val_loss: 0.5618 - val_acc: 0.7125 Epoch 25/32 226/225 [==============================] - 80s 352ms/step - loss: 0.5762 - acc: 0.6996 - val_loss: 0.9354 - val_acc: 0.5436 Epoch 26/32 226/225 [==============================] - 80s 353ms/step - loss: 0.5724 - acc: 0.7059 - val_loss: 4.0694 - val_acc: 0.4996 Epoch 27/32 226/225 [==============================] - 80s 354ms/step - loss: 0.5702 - acc: 0.7041 - val_loss: 0.5508 - val_acc: 0.7060 Epoch 28/32 226/225 [==============================] - 80s 353ms/step - loss: 0.5459 - acc: 0.7235 - val_loss: 0.5939 - val_acc: 0.6792 Epoch 29/32 226/225 [==============================] - 80s 352ms/step - loss: 0.5353 - acc: 0.7347 - val_loss: 0.6381 - val_acc: 0.6692 Epoch 30/32 226/225 [==============================] - 80s 354ms/step - loss: 0.5242 - acc: 0.7404 - val_loss: 0.7704 - val_acc: 0.6610 Epoch 31/32 226/225 [==============================] - 80s 355ms/step - loss: 0.5159 - acc: 0.7481 - val_loss: 1.4030 - val_acc: 0.5737 Epoch 32/32 226/225 [==============================] - 80s 352ms/step - loss: 0.4922 - acc: 0.7576 - val_loss: 0.5993 - val_acc: 0.7081 학습결과 시각화1234567891011121314151617181920212223242526272829303132333435import matplotlib.pyplot as pltdef show_graph(history_dict): accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy and loss') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history.history) 그래프의 변동이 심하니 지수 이동 평균값을 구하여 그래프를 다시 그립니다. 123456789def smooth_curve(points, factor=.8): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 1234smooth_data = &#123;&#125;for key, val in history.history.items(): smooth_data[key] = smooth_curve(val)show_graph(smooth_data) 현재 Tesla K80으로 약 40분을 훈련시켰는데, 그래프로만 봐서는 과소적합일 가능성이 보입니다.조금 더 학습을 시켜보면 알수 있겠네요. 모델의 평가를 해본 이후에 32에폭을 더 돌려보도록 하겠습니다. 모델 중간 평가현재 모델을 기준으로 테스트셋의 정확도와 손실율을 구해보겠습니다.테스트셋의 경우 rescale만 적용하여 원본 이미지 그대로 사용합니다. 12345678910testGenerator = ImageDataGenerator( rescale=1./255)testGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'test_set'), target_size=(64, 64),)model.evaluate_generator(testGen) Found 2023 images belonging to 2 classes. [0.6065594666792624, 0.7162629758080102] 모델 추가 학습이전의 그래프로는 과소적합일 가능성이 있기 때문에 32에폭만 추가적으로 더 학습을 진행해보겠습니다. 12345678epochs = 32history = model.fit_generator( trainGen, epochs=epochs, steps_per_epoch=trainGen.samples / epochs, validation_data=validationGen, validation_steps=trainGen.samples / epochs,) Epoch 1/32 226/225 [==============================] - 80s 355ms/step - loss: 0.4832 - acc: 0.7692 - val_loss: 0.5355 - val_acc: 0.7609 Epoch 2/32 226/225 [==============================] - 80s 353ms/step - loss: 0.4713 - acc: 0.7720 - val_loss: 0.4927 - val_acc: 0.7584 Epoch 3/32 226/225 [==============================] - 80s 356ms/step - loss: 0.4615 - acc: 0.7808 - val_loss: 0.5980 - val_acc: 0.7175 Epoch 4/32 226/225 [==============================] - 80s 352ms/step - loss: 0.4611 - acc: 0.7821 - val_loss: 1.0948 - val_acc: 0.5885 Epoch 5/32 226/225 [==============================] - 80s 352ms/step - loss: 0.4445 - acc: 0.7929 - val_loss: 0.5592 - val_acc: 0.7476 Epoch 6/32 226/225 [==============================] - 80s 353ms/step - loss: 0.4443 - acc: 0.7986 - val_loss: 0.5090 - val_acc: 0.7360 Epoch 7/32 226/225 [==============================] - 80s 355ms/step - loss: 0.4169 - acc: 0.8060 - val_loss: 0.5854 - val_acc: 0.7501 Epoch 8/32 226/225 [==============================] - 80s 352ms/step - loss: 0.4104 - acc: 0.8143 - val_loss: 0.9060 - val_acc: 0.7382 Epoch 9/32 226/225 [==============================] - 80s 352ms/step - loss: 0.4023 - acc: 0.8198 - val_loss: 0.4205 - val_acc: 0.8031 Epoch 10/32 226/225 [==============================] - 80s 352ms/step - loss: 0.3985 - acc: 0.8197 - val_loss: 1.0545 - val_acc: 0.6121 Epoch 11/32 226/225 [==============================] - 81s 356ms/step - loss: 0.3883 - acc: 0.8228 - val_loss: 0.4979 - val_acc: 0.7987 Epoch 12/32 226/225 [==============================] - 80s 352ms/step - loss: 0.3755 - acc: 0.8326 - val_loss: 0.6414 - val_acc: 0.6928 Epoch 13/32 226/225 [==============================] - 80s 352ms/step - loss: 0.3692 - acc: 0.8364 - val_loss: 0.5251 - val_acc: 0.7304 Epoch 14/32 226/225 [==============================] - 80s 352ms/step - loss: 0.3950 - acc: 0.8204 - val_loss: 3.7220 - val_acc: 0.5317 Epoch 15/32 226/225 [==============================] - 80s 354ms/step - loss: 0.3995 - acc: 0.8192 - val_loss: 0.4228 - val_acc: 0.7976 Epoch 16/32 226/225 [==============================] - 80s 353ms/step - loss: 0.3514 - acc: 0.8469 - val_loss: 0.5189 - val_acc: 0.7638 Epoch 17/32 226/225 [==============================] - 80s 352ms/step - loss: 0.3363 - acc: 0.8533 - val_loss: 0.3654 - val_acc: 0.8330 Epoch 18/32 226/225 [==============================] - 80s 355ms/step - loss: 0.3307 - acc: 0.8577 - val_loss: 0.4184 - val_acc: 0.8128 Epoch 19/32 226/225 [==============================] - 80s 354ms/step - loss: 0.3644 - acc: 0.8398 - val_loss: 0.5159 - val_acc: 0.7530 Epoch 20/32 226/225 [==============================] - 80s 353ms/step - loss: 0.3657 - acc: 0.8429 - val_loss: 0.5468 - val_acc: 0.7775 Epoch 21/32 226/225 [==============================] - 80s 355ms/step - loss: 0.3313 - acc: 0.8565 - val_loss: 1.4114 - val_acc: 0.5933 Epoch 22/32 226/225 [==============================] - 80s 353ms/step - loss: 0.3172 - acc: 0.8613 - val_loss: 1.1401 - val_acc: 0.6374 Epoch 23/32 226/225 [==============================] - 80s 353ms/step - loss: 0.3278 - acc: 0.8590 - val_loss: 0.5699 - val_acc: 0.7566 Epoch 24/32 226/225 [==============================] - 80s 354ms/step - loss: 0.3031 - acc: 0.8703 - val_loss: 0.3731 - val_acc: 0.8324 Epoch 25/32 226/225 [==============================] - 80s 353ms/step - loss: 0.2799 - acc: 0.8809 - val_loss: 0.4147 - val_acc: 0.8131 Epoch 26/32 226/225 [==============================] - 80s 355ms/step - loss: 0.2879 - acc: 0.8765 - val_loss: 0.3705 - val_acc: 0.8410 Epoch 27/32 226/225 [==============================] - 80s 354ms/step - loss: 0.2953 - acc: 0.8763 - val_loss: 0.4153 - val_acc: 0.8234 Epoch 28/32 226/225 [==============================] - 80s 352ms/step - loss: 0.2705 - acc: 0.8829 - val_loss: 0.3329 - val_acc: 0.8583 Epoch 29/32 226/225 [==============================] - 80s 355ms/step - loss: 0.2615 - acc: 0.8906 - val_loss: 0.3774 - val_acc: 0.8317 Epoch 30/32 226/225 [==============================] - 81s 356ms/step - loss: 0.2550 - acc: 0.8910 - val_loss: 0.4223 - val_acc: 0.8317 Epoch 31/32 226/225 [==============================] - 79s 351ms/step - loss: 0.2552 - acc: 0.8906 - val_loss: 0.4189 - val_acc: 0.8092 Epoch 32/32 226/225 [==============================] - 80s 352ms/step - loss: 0.2475 - acc: 0.8917 - val_loss: 0.3132 - val_acc: 0.8566 학습결과 시각화마찬가지로 지수이동평균을 적용하여 그래프를 그려보겠습니다. 1234smooth_data = &#123;&#125;for key, val in history.history.items(): smooth_data[key] = smooth_curve(val)show_graph(smooth_data) 1model.evaluate_generator(testGen) [0.33355356405939846, 0.8581314879482004] 여전히 과소적합인 것 같네요.테스트셋에 대한 정확도가 85%까지 올라왔는데 아마 학습을 더 진행하게 된다면 더 높은 성능을 내줄거 같습니다. 아무래도 모델을 처음부터 모델을 학습시키다보니 시간이 오래 걸리네요.대회에 나갈때에는 모델을 2~3주동안 학습시켰답니다.여러분도 학습을 추가적으로 진행시켜보세요 !","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"},{"name":"CNN","slug":"cnn","permalink":"https://codingcrews.github.io/tags/cnn/"}]},{"title":"딥러닝으로 개와 고양이를 분류하는 모델을 만들어 보자.","slug":"cat-dog","date":"2019-01-17T14:25:11.000Z","updated":"2019-01-29T18:39:15.398Z","comments":true,"path":"2019/01/17/cat-dog/","link":"","permalink":"https://codingcrews.github.io/2019/01/17/cat-dog/","excerpt":"","text":"개와 고양이 분류하기이번에는 CNN(Convolutional Neural Network)를 이용하여 개와 고양이 사진을 분류하는 모델을 만들어 봅시다. 사람은 개와 고양이를 분류하기가 굉장히 쉽죠. 딱 보면 아니까요.하지만 컴퓨터 공학에서는 이걸 분류하는 문제가 해결하기 굉장히 어려운 문제였습니다.컴퓨터는 일일히 룰을 지정해가며 개와 고양이를 분류하도록 만들어야 하니까요. 그래서 최근에는 딥러닝이 이러한 문제들을 해결하면서 최근 비전 관련 문제들을 딥러닝으로 풀어내려는 시도가 많습니다.오늘 소개할 CNN은 이미지 인식 등에서 기존의 전통적인 비전 프로세싱에 비해 높은 성능을 보여줍니다. 오늘 시도하는 튜토리얼은 CPU로 학습시키기엔 꽤 오랜 시간이 걸리므로 인내심을 가지고 학습을 진행하시거나,AWS로 GPU 딥러닝 환경 구축하기 포스팅을 참고하여 환경을 세팅한 뒤 진행해보세요. Dataset이번 데이터셋은 kaggle에서 제공하는 데이터셋을 이용할 예정입니다.MNIST 데이터셋은 흑백의 이미지였지만, 이번에 사용할 이미지는 컬러를 가지고 있습니다. Download직접 데이터셋을 배포하진 않고 있습니다.데이터셋은 여기에서 직접 다운로드 받으실 수 있고, kaggle api를 통하여 받으실 수 있습니다.로그인을 해야하니 만약 가입하지 않으셨다면 이번 기회에 가입해보시는걸 추천합니다. 저희는 이 노트와 동일한 위치에 datasets이란 폴더에 데이터셋을 넣어두고 사용하도록 하겠습니다. 123456789101112datasets└── cat-and-dog ├── test_set │ ├── cats │ │ └── datas... │ └── dogs │ └── datas... └── training_set ├── cats │ └── datas... └── dogs └── datas... 이와 같은 형태의 구성이 되어 있다고 가정합니다. 데이터 증식데이터 증식(Data argumentaion)을 위해 케라스에서 제공하는 이미지 제너레이터를 사용합니다.이미지의 위치를 조금 옮긴다거나, 회전, 좌우반전등을 했을 때 컴퓨터가 받아들이는 이미지는 전혀 다른것이 됩니다.이러한 변형을 줌으로써 학습 데이터를 늘리고, 이러한 변조에 강하게 모델을 학습시킬 수 있습니다. 1234567891011121314151617181920212223242526import osfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorrootPath = './datasets/cat-and-dog'imageGenerator = ImageDataGenerator( rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, brightness_range=[.2, .2], horizontal_flip=True, validation_split=.1)trainGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=(64, 64), subset='training')validationGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'training_set'), target_size=(64, 64), subset='validation') Found 7205 images belonging to 2 classes. Found 800 images belonging to 2 classes. rescale은 이미지의 nomalization을 위해 사용합니다. 각 이미지별로 255로 나눈 값으로 데이터가 변형됩니다.rotation_range는 이미지의 최대 회전각을 지정합니다. 최대 20도까지 회전합니다.width,height shift_range는 이미지의 이동을 말합니다. 좌우, 위아래로 이미지의 이동하는 백분율을 지정합니다. (0.1은 10%)brightness_range는 이미지 밝기에 대한 내용입니다.horizontal_flip은 이미지의 수평 반전을 시켜줍니다. 이 옵션의 경우 데이터셋의 이해가 필요합니다. 예를 들면 MNIST 데이터셋의 경우 손글씨 데이터이기 때문에 수평 반전이 일어나면 안됩니다.validation_split은 검증세트의 비율을 지정해줍니다. 설정한 이미지 제너레이터를 통해 특정 디렉터리의 데이터들을 손쉽게 불러올 수 있습니다.현재 데이터셋 구성에 맞춰 폴더이름은 레이블명, 폴더안의 데이터는 해당 레이블의 데이터셋이 됩니다.target_size는 이미지를 해당 형태로 변형시켜주는데, 저희는 64x64 사이즈로 데이터로 읽어오겠습니다. 모델 구성간단한 CNN 모델을 구성하여 학습을 진행해봅니다. 123456789101112131415161718192021222324from tensorflow.keras.models import Sequentialfrom tensorflow.keras import layersmodel = Sequential()model.add(layers.InputLayer(input_shape=(64, 64, 3)))model.add(layers.Conv2D(16, (3, 3), (1, 1), 'same', activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Dropout(rate=0.3))model.add(layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Dropout(rate=0.3))model.add(layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Dropout(rate=0.3))model.add(layers.Flatten())model.add(layers.Dense(512, activation='relu'))model.add(layers.Dense(256, activation='relu'))model.add(layers.Dense(2, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_3 (Conv2D) (None, 64, 64, 16) 448 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 32, 32, 16) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 32, 32, 16) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 32, 32, 32) 4640 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32) 0 _________________________________________________________________ dropout_4 (Dropout) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ dropout_5 (Dropout) (None, 8, 8, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_3 (Dense) (None, 512) 2097664 _________________________________________________________________ dense_4 (Dense) (None, 256) 131328 _________________________________________________________________ dense_5 (Dense) (None, 2) 514 ================================================================= Total params: 2,253,090 Trainable params: 2,253,090 Non-trainable params: 0 _________________________________________________________________ 1trainGen.samples 7205 12345model.compile( optimizer='adam', loss='binary_crossentropy', metrics=['acc'],) 12345678epochs = 32history = model.fit_generator( trainGen, epochs=epochs, steps_per_epoch=trainGen.samples / epochs, validation_data=validationGen, validation_steps=trainGen.samples / epochs,) Epoch 1/32 226/225 [==============================] - 37s 164ms/step - loss: 0.6921 - acc: 0.5143 - val_loss: 0.6901 - val_acc: 0.5551 Epoch 2/32 226/225 [==============================] - 36s 159ms/step - loss: 0.6824 - acc: 0.5672 - val_loss: 0.6680 - val_acc: 0.5807 Epoch 3/32 226/225 [==============================] - 36s 158ms/step - loss: 0.6727 - acc: 0.5781 - val_loss: 0.6682 - val_acc: 0.5857 Epoch 4/32 226/225 [==============================] - 36s 159ms/step - loss: 0.6614 - acc: 0.6063 - val_loss: 0.6381 - val_acc: 0.6231 Epoch 5/32 226/225 [==============================] - 36s 159ms/step - loss: 0.6451 - acc: 0.6323 - val_loss: 0.6168 - val_acc: 0.6573 Epoch 6/32 226/225 [==============================] - 36s 159ms/step - loss: 0.6329 - acc: 0.6482 - val_loss: 0.6262 - val_acc: 0.6653 Epoch 7/32 226/225 [==============================] - 36s 158ms/step - loss: 0.6093 - acc: 0.6664 - val_loss: 0.5814 - val_acc: 0.6937 Epoch 8/32 226/225 [==============================] - 37s 165ms/step - loss: 0.5997 - acc: 0.6765 - val_loss: 0.6226 - val_acc: 0.6498 Epoch 9/32 226/225 [==============================] - 36s 158ms/step - loss: 0.6055 - acc: 0.6683 - val_loss: 0.5609 - val_acc: 0.7178 Epoch 10/32 226/225 [==============================] - 36s 159ms/step - loss: 0.5880 - acc: 0.6837 - val_loss: 0.5585 - val_acc: 0.7047 Epoch 11/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5769 - acc: 0.6972 - val_loss: 0.5701 - val_acc: 0.7006 Epoch 12/32 226/225 [==============================] - 36s 159ms/step - loss: 0.5727 - acc: 0.7003 - val_loss: 0.5507 - val_acc: 0.7168 Epoch 13/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5673 - acc: 0.7059 - val_loss: 0.5697 - val_acc: 0.7028 Epoch 14/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5588 - acc: 0.7132 - val_loss: 0.5845 - val_acc: 0.6786 Epoch 15/32 226/225 [==============================] - 36s 160ms/step - loss: 0.5522 - acc: 0.7187 - val_loss: 0.5251 - val_acc: 0.7267 Epoch 16/32 226/225 [==============================] - 37s 163ms/step - loss: 0.5570 - acc: 0.7141 - val_loss: 0.5353 - val_acc: 0.7317 Epoch 17/32 226/225 [==============================] - 37s 162ms/step - loss: 0.5441 - acc: 0.7182 - val_loss: 0.5237 - val_acc: 0.7369 Epoch 18/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5399 - acc: 0.7227 - val_loss: 0.5142 - val_acc: 0.7402 Epoch 19/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5343 - acc: 0.7278 - val_loss: 0.5283 - val_acc: 0.7217 Epoch 20/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5352 - acc: 0.7262 - val_loss: 0.5031 - val_acc: 0.7476 Epoch 21/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5278 - acc: 0.7312 - val_loss: 0.5222 - val_acc: 0.7347 Epoch 22/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5251 - acc: 0.7374 - val_loss: 0.4975 - val_acc: 0.7667 Epoch 23/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5186 - acc: 0.7448 - val_loss: 0.4979 - val_acc: 0.7543 Epoch 24/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5091 - acc: 0.7504 - val_loss: 0.5470 - val_acc: 0.7277 Epoch 25/32 226/225 [==============================] - 38s 168ms/step - loss: 0.5108 - acc: 0.7416 - val_loss: 0.5053 - val_acc: 0.7491 Epoch 26/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5028 - acc: 0.7507 - val_loss: 0.5248 - val_acc: 0.7302 Epoch 27/32 226/225 [==============================] - 36s 159ms/step - loss: 0.5008 - acc: 0.7523 - val_loss: 0.4948 - val_acc: 0.7599 Epoch 28/32 226/225 [==============================] - 36s 158ms/step - loss: 0.5063 - acc: 0.7456 - val_loss: 0.5401 - val_acc: 0.7167 Epoch 29/32 226/225 [==============================] - 36s 157ms/step - loss: 0.4870 - acc: 0.7671 - val_loss: 0.5423 - val_acc: 0.7310 Epoch 30/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4976 - acc: 0.7625 - val_loss: 0.5083 - val_acc: 0.7450 Epoch 31/32 226/225 [==============================] - 36s 160ms/step - loss: 0.4917 - acc: 0.7613 - val_loss: 0.5162 - val_acc: 0.7344 Epoch 32/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4840 - acc: 0.7648 - val_loss: 0.5015 - val_acc: 0.7436 학습결과 시각화 및 평가1234567891011121314151617181920212223242526272829303132333435import matplotlib.pyplot as pltdef show_graph(history_dict): accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy and loss') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history.history) 32 에폭의 학습을 마친 그래프입니다.GPU를 GTX 750TI 모델을 사용중인데 에폭당 36초씩 약 20분 가량 학습 시간이 소요되었습니다.그래프를 보면 과소적합 된것처럼 보입니다. 모델 중간 평가현재 모델을 기준으로 테스트셋의 정확도와 손실율을 구해보겠습니다.테스트셋의 경우 rescale만 적용하여 원본 이미지 그대로 넣습니다. 12345678910testGenerator = ImageDataGenerator( rescale=1./255)testGen = imageGenerator.flow_from_directory( os.path.join(rootPath, 'test_set'), target_size=(64, 64),)model.evaluate_generator(testGen) Found 2023 images belonging to 2 classes. [0.5174207690176962, 0.7412259022139466] 74.1% 의 정확도를 보여줍니다.과소적합이 의심되니 32에폭을 더 돌려보겠습니다. 12345678epochs = 32history = model.fit_generator( trainGen, epochs=epochs, steps_per_epoch=trainGen.samples / epochs, validation_data=validationGen, validation_steps=trainGen.samples / epochs,) Epoch 1/32 226/225 [==============================] - 37s 162ms/step - loss: 0.4809 - acc: 0.7647 - val_loss: 0.4936 - val_acc: 0.7552 Epoch 2/32 226/225 [==============================] - 38s 167ms/step - loss: 0.4797 - acc: 0.7714 - val_loss: 0.5129 - val_acc: 0.7468 Epoch 3/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4706 - acc: 0.7766 - val_loss: 0.4703 - val_acc: 0.7754 Epoch 4/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4771 - acc: 0.7682 - val_loss: 0.4822 - val_acc: 0.7671 Epoch 5/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4721 - acc: 0.7637 - val_loss: 0.4953 - val_acc: 0.7539 Epoch 6/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4714 - acc: 0.7736 - val_loss: 0.4742 - val_acc: 0.7727 Epoch 7/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4698 - acc: 0.7747 - val_loss: 0.5087 - val_acc: 0.7595 Epoch 8/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4577 - acc: 0.7805 - val_loss: 0.4736 - val_acc: 0.7674 Epoch 9/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4615 - acc: 0.7771 - val_loss: 0.5172 - val_acc: 0.7386 Epoch 10/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4628 - acc: 0.7766 - val_loss: 0.5027 - val_acc: 0.7499 Epoch 11/32 226/225 [==============================] - 37s 165ms/step - loss: 0.4598 - acc: 0.7785 - val_loss: 0.4804 - val_acc: 0.7623 Epoch 12/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4515 - acc: 0.7859 - val_loss: 0.4893 - val_acc: 0.7549 Epoch 13/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4569 - acc: 0.7862 - val_loss: 0.4973 - val_acc: 0.7631 Epoch 14/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4350 - acc: 0.7974 - val_loss: 0.4770 - val_acc: 0.7730 Epoch 15/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4376 - acc: 0.8010 - val_loss: 0.5099 - val_acc: 0.7562 Epoch 16/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4416 - acc: 0.7891 - val_loss: 0.4770 - val_acc: 0.7665 Epoch 17/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4342 - acc: 0.7915 - val_loss: 0.4721 - val_acc: 0.7779 Epoch 18/32 226/225 [==============================] - 36s 159ms/step - loss: 0.4388 - acc: 0.7926 - val_loss: 0.4733 - val_acc: 0.7666 Epoch 19/32 226/225 [==============================] - 37s 163ms/step - loss: 0.4189 - acc: 0.7991 - val_loss: 0.4757 - val_acc: 0.7711 Epoch 20/32 226/225 [==============================] - 36s 161ms/step - loss: 0.4194 - acc: 0.7991 - val_loss: 0.4706 - val_acc: 0.7801 Epoch 21/32 226/225 [==============================] - 36s 159ms/step - loss: 0.4334 - acc: 0.7942 - val_loss: 0.4563 - val_acc: 0.7807 Epoch 22/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4205 - acc: 0.8017 - val_loss: 0.4695 - val_acc: 0.7667 Epoch 23/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4246 - acc: 0.8006 - val_loss: 0.4741 - val_acc: 0.7678 Epoch 24/32 226/225 [==============================] - 36s 159ms/step - loss: 0.4097 - acc: 0.8103 - val_loss: 0.5746 - val_acc: 0.7368 Epoch 25/32 226/225 [==============================] - 36s 158ms/step - loss: 0.4137 - acc: 0.8051 - val_loss: 0.4951 - val_acc: 0.7663 Epoch 26/32 226/225 [==============================] - 39s 173ms/step - loss: 0.4066 - acc: 0.8077 - val_loss: 0.4999 - val_acc: 0.7667 Epoch 27/32 226/225 [==============================] - 36s 159ms/step - loss: 0.4121 - acc: 0.8110 - val_loss: 0.4716 - val_acc: 0.7789 Epoch 28/32 226/225 [==============================] - 38s 167ms/step - loss: 0.3985 - acc: 0.8153 - val_loss: 0.5083 - val_acc: 0.7613 Epoch 29/32 226/225 [==============================] - 36s 158ms/step - loss: 0.3980 - acc: 0.8153 - val_loss: 0.4996 - val_acc: 0.7672 Epoch 30/32 226/225 [==============================] - 36s 159ms/step - loss: 0.4105 - acc: 0.8083 - val_loss: 0.4759 - val_acc: 0.7745 Epoch 31/32 226/225 [==============================] - 36s 159ms/step - loss: 0.4027 - acc: 0.8112 - val_loss: 0.4851 - val_acc: 0.7696 Epoch 32/32 226/225 [==============================] - 36s 158ms/step - loss: 0.3988 - acc: 0.8205 - val_loss: 0.4962 - val_acc: 0.7692 1show_graph(history.history) 그래프를 보니 과대적합이 되어가고 있는것처럼 보입니다.모델이 문제에 비해 간단한 모델인 것 같네요. 1model.evaluate_generator(testGen) [0.5502711092547738, 0.736529906139006] 테스트셋의 정확도를 봤을 때 73%로 아까보다 오히려 떨어진 결과를 보여줍니다.예상대로 과대적합이 되어가고 있던 모양입니다. 모델 예측모델 학습시켰으니 개와 고양이를 예측시키는걸 한번 해보도록 하겠습니다. 12from tensorflow.keras.preprocessing.image import array_to_imgimport numpy as np 123456789cls_index = ['고양이', '개']imgs = testGen.next()arr = imgs[0][0]img = array_to_img(arr).resize((128, 128))plt.imshow(img)result = model.predict_classes(arr.reshape(1, 64, 64, 3))print('예측: &#123;&#125;'.format(cls_index[result[0]]))print('정답: &#123;&#125;'.format(cls_index[np.argmax(imgs[1][0])])) 예측: 고양이 정답: 고양이 사이즈를 줄인 이미지를 강제로 키웠더니 이미지 품질이 많이 떨어졌네요.모델이 예측한 답과 원본 답이 일치하는걸 볼 수 있습니다.물론 정확도가 많이 낮아 여러번 하면 틀린답도 자주 나옵니다. 다음에는 이미지넷에서 큰 성과를 이뤘던 모델들을 가지고 정확도를 더 높일 수 있도록 학습을 진행해보겠습니다.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"},{"name":"CNN","slug":"cnn","permalink":"https://codingcrews.github.io/tags/cnn/"}]},{"title":"딥러닝으로 손글씨를 인식하는 모델을 만들어보자.","slug":"mnist","date":"2019-01-17T09:52:53.000Z","updated":"2019-01-29T18:36:55.501Z","comments":true,"path":"2019/01/17/mnist/","link":"","permalink":"https://codingcrews.github.io/2019/01/17/mnist/","excerpt":"","text":"Mnist 데이터셋을 이용한 손글씨 분류하기 : DNN, CNN지금까지 진행한 포스팅을 기반으로 딥러닝 튜토리얼 시 가장 흔하게 접할 수 있는 손글씨 분류하기를 해보겠습니다.MNIST 문제는 다중 분류 문제로써 0~9까지의 손글씨를 분류하는 문제입니다.이전 포스팅에서 사용한 선형 레이어를 이용하여 0~9의 숫자를 분류해보고, 이후에는 CNN을 이용해 정확도를 개선해보도록 하겠습니다. Dataset사용할 MNIST 데이터 세트는 텐서플로 패키지에서 다운로드까지 진행해주는 코드를 포함시켜두었습니다.우리가 사용할 데이터셋은 텐서플로 패키지에서 제공하는 데이터셋을 이용할건데요.다른곳에서 MNIST 데이터셋을 이용하여 진행해봐도 무방합니다. 우리가 사용할 MNIST 데이터는 손글씨 데이터로써 흑백 Gray Scale로 된 데이터셋을 말합니다.28x28 사이즈의 단일 색상채널을 가지고 있으며, 트레이닝셋과 테스트셋이 각각 60,000개와 10,000개로 구성되어 있습니다. Download1234import tensorflow as tfimport numpy as npmnist = tf.keras.datasets.mnist(x_train, y_train),(x_test, y_test) = mnist.load_data() 데이터 확인123456display(x_train.shape)display(y_train.shape)display(x_test.shape)display(y_test.shape)display(y_test) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) array([7, 2, 1, ..., 4, 5, 6], dtype=uint8) 데이터는 60,000개의 트레이닝셋과 10,000개의 테스트셋을 확인할 수 있고,테스트 데이터는 28x28 사이즈의 이미지 데이터임을 확인할 수 있습니다. (색상 채널은 단일 채널이라 따로 표기되지 않아요.)레이블 데이터는 0~9까지의 10개 클래스로 구성되어 있습니다. 데이터 시각화우리가 다룰 Mnist 데이터는 이미지 데이터 기반입니다.해당 이미지를 시각화하여 확인해보겠습니다. 12345678910import matplotlib.pyplot as pltdef show_data(arr): plt.imshow(arr, cmap=plt.cm.binary) reshape_data = arr.reshape(-1, ) for index, data in enumerate(reshape_data): print('&#123;:3d&#125;'.format(data), end='') if index % 28 == 27: print() 1show_data(x_train[1]) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 51159253159 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 48238252252252237 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 54227253252239233252 57 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 60224252253252202 84252253122 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0163252252252253252252 96189253167 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 51238253253190114253228 47 79255168 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 48238252252179 12 75121 21 0 0253243 50 0 0 0 0 0 0 0 0 0 0 0 0 0 38165253233208 84 0 0 0 0 0 0253252165 0 0 0 0 0 0 0 0 0 0 0 0 7178252240 71 19 28 0 0 0 0 0 0253252195 0 0 0 0 0 0 0 0 0 0 0 0 57252252 63 0 0 0 0 0 0 0 0 0253252195 0 0 0 0 0 0 0 0 0 0 0 0198253190 0 0 0 0 0 0 0 0 0 0255253196 0 0 0 0 0 0 0 0 0 0 0 76246252112 0 0 0 0 0 0 0 0 0 0253252148 0 0 0 0 0 0 0 0 0 0 0 85252230 25 0 0 0 0 0 0 0 0 7135253186 12 0 0 0 0 0 0 0 0 0 0 0 85252223 0 0 0 0 0 0 0 0 7131252225 71 0 0 0 0 0 0 0 0 0 0 0 0 85252145 0 0 0 0 0 0 0 48165252173 0 0 0 0 0 0 0 0 0 0 0 0 0 0 86253225 0 0 0 0 0 0114238253162 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 85252249146 48 29 85178225253223167 56 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 85252252252229215252252252196130 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 28199252252253252252233145 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 25128252253252141 37 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 위는 해당 데이터를 시각화 한 결과입니다.28x28의 데이터는 각 픽셀별로 해당 픽셀이 표시해야 할 색상값을 표시하는데,이걸 28x28 사이즈에 맞춰 해당 데이터를 출력한 결과입니다. 이미지는 matplotlib을 이용하여 해당 데이터를 시각화 한 결과입니다. 데이터 변환이미지 데이터를 확인했을 때 0~255까지의 값을 가지고 있는걸 확인할 수 있습니다.이걸 모델에 효율적으로 학습시키기 위해선 0~1 사이의 값으로 일반화(Nomalization)시키는 과정을 거쳐야 합니다.그 이외에도 2D컨볼루셔널 레이어를 사용하기 위해선 채널에 대한 값도 명시되어 있어야하기 때문에(넓이, 높이)의 형태를 가진 데이터를 (넓이,높이,채널)의 형태로 변환시켜보겠습니다.구조는 어떤식으로든 상관없으며 (채널, 넓이, 높이) 순으로 정렬을 시켜도 됩니다. 12reshape_x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)reshape_x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) 모델 구성이전 포스팅에서 다뤘던 Dense 레이어를 이용해 기본적인 신경망으로 학습을 진행해봅시다. 1234567891011from tensorflow.keras.models import Sequentialfrom tensorflow.keras import layersmodel = Sequential()model.add(layers.InputLayer(input_shape=(28, 28, 1)))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 64) 50240 _________________________________________________________________ dense_1 (Dense) (None, 16) 1040 _________________________________________________________________ dense_2 (Dense) (None, 10) 170 ================================================================= Total params: 51,450 Trainable params: 51,450 Non-trainable params: 0 _________________________________________________________________ 총 3개의 레이어로 구성되어 있으며, MNIST 데이터셋에 맞춰 (28, 28, 1)의 형태에 맞춰 인풋 데이터를 받도록 구성했습니다.마지막층은 손글씨 데이터의 10개 클래스(0~9까지의 레이블)를 예측하기 위해 출력을 10으로, 활성함수는 softmax를 사용합니다. 모델 컴파일12345model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) 모델 학습CPU로 학습 시 대략 25분 정도의 시간이 소요됩니다. (맥북프로 15인치 2017 CTO 기준)그래서 저는 GPU 환경을 세팅하고 학습을 진행했습니다.GPU 환경을 구축하기 위한 데스크탑을 만들기 어려우신 분들은 AWS로 GPU환경 세팅하기포스팅을 참고하세요. 모델의 구성을 변경했습니다.CPU에서도 에폭당 1초의 결과를 보여줍니다. 1234567history = model.fit( reshape_x_train, y_train, batch_size=128, epochs=50, validation_split=.1,) Train on 54000 samples, validate on 6000 samples Epoch 1/50 54000/54000 [==============================] - 1s 20us/step - loss: 6.2426 - acc: 0.5904 - val_loss: 4.6204 - val_acc: 0.6893 Epoch 2/50 54000/54000 [==============================] - 1s 14us/step - loss: 3.7938 - acc: 0.7398 - val_loss: 1.9817 - val_acc: 0.8462 Epoch 3/50 54000/54000 [==============================] - 1s 15us/step - loss: 1.2180 - acc: 0.8636 - val_loss: 0.6667 - val_acc: 0.8982 Epoch 4/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.6308 - acc: 0.8874 - val_loss: 0.4537 - val_acc: 0.9087 Epoch 5/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.4778 - acc: 0.9019 - val_loss: 0.3766 - val_acc: 0.9243 Epoch 6/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.3996 - acc: 0.9122 - val_loss: 0.3421 - val_acc: 0.9307 Epoch 7/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.3400 - acc: 0.9253 - val_loss: 0.2921 - val_acc: 0.9385 Epoch 8/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.2954 - acc: 0.9321 - val_loss: 0.2361 - val_acc: 0.9437 Epoch 9/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.2549 - acc: 0.9403 - val_loss: 0.2291 - val_acc: 0.9452 Epoch 10/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.2292 - acc: 0.9447 - val_loss: 0.2212 - val_acc: 0.9495 Epoch 11/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.2008 - acc: 0.9490 - val_loss: 0.2298 - val_acc: 0.9518 Epoch 12/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.1871 - acc: 0.9531 - val_loss: 0.1879 - val_acc: 0.9533 Epoch 13/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.1681 - acc: 0.9563 - val_loss: 0.1907 - val_acc: 0.9565 Epoch 14/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.1610 - acc: 0.9572 - val_loss: 0.2007 - val_acc: 0.9540 Epoch 15/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.1477 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9582 Epoch 16/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.1330 - acc: 0.9646 - val_loss: 0.1695 - val_acc: 0.9598 Epoch 17/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.1262 - acc: 0.9661 - val_loss: 0.1596 - val_acc: 0.9627 Epoch 18/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.1226 - acc: 0.9669 - val_loss: 0.1563 - val_acc: 0.9642 Epoch 19/50 54000/54000 [==============================] - 1s 12us/step - loss: 0.1081 - acc: 0.9708 - val_loss: 0.1706 - val_acc: 0.9580 Epoch 20/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0998 - acc: 0.9729 - val_loss: 0.1407 - val_acc: 0.9658 Epoch 21/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0970 - acc: 0.9733 - val_loss: 0.1428 - val_acc: 0.9653 Epoch 22/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0829 - acc: 0.9774 - val_loss: 0.1584 - val_acc: 0.9637 Epoch 23/50 54000/54000 [==============================] - 1s 15us/step - loss: 0.0851 - acc: 0.9763 - val_loss: 0.1609 - val_acc: 0.9645 Epoch 24/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0833 - acc: 0.9766 - val_loss: 0.1462 - val_acc: 0.9682 Epoch 25/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0730 - acc: 0.9794 - val_loss: 0.1344 - val_acc: 0.9658 Epoch 26/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0781 - acc: 0.9788 - val_loss: 0.1881 - val_acc: 0.9623 Epoch 27/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0766 - acc: 0.9786 - val_loss: 0.1654 - val_acc: 0.9615 Epoch 28/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0664 - acc: 0.9809 - val_loss: 0.1617 - val_acc: 0.9663 Epoch 29/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0661 - acc: 0.9819 - val_loss: 0.1393 - val_acc: 0.9670 Epoch 30/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0649 - acc: 0.9814 - val_loss: 0.1492 - val_acc: 0.9655 Epoch 31/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0552 - acc: 0.9842 - val_loss: 0.1669 - val_acc: 0.9647 Epoch 32/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0523 - acc: 0.9849 - val_loss: 0.1880 - val_acc: 0.9648 Epoch 33/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0534 - acc: 0.9852 - val_loss: 0.1748 - val_acc: 0.9652 Epoch 34/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0533 - acc: 0.9847 - val_loss: 0.1453 - val_acc: 0.9708 Epoch 35/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0528 - acc: 0.9850 - val_loss: 0.1709 - val_acc: 0.9673 Epoch 36/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0498 - acc: 0.9856 - val_loss: 0.1492 - val_acc: 0.9653 Epoch 37/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0523 - acc: 0.9850 - val_loss: 0.1639 - val_acc: 0.9663 Epoch 38/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0467 - acc: 0.9863 - val_loss: 0.1682 - val_acc: 0.9660 Epoch 39/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0430 - acc: 0.9876 - val_loss: 0.1630 - val_acc: 0.9657 Epoch 40/50 54000/54000 [==============================] - 1s 13us/step - loss: 0.0397 - acc: 0.9890 - val_loss: 0.1517 - val_acc: 0.9688 Epoch 41/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0450 - acc: 0.9871 - val_loss: 0.1916 - val_acc: 0.9662 Epoch 42/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0416 - acc: 0.9877 - val_loss: 0.1888 - val_acc: 0.9640 Epoch 43/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0457 - acc: 0.9867 - val_loss: 0.1827 - val_acc: 0.9670 Epoch 44/50 54000/54000 [==============================] - 1s 15us/step - loss: 0.0375 - acc: 0.9892 - val_loss: 0.1546 - val_acc: 0.9708 Epoch 45/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0357 - acc: 0.9897 - val_loss: 0.1752 - val_acc: 0.9673 Epoch 46/50 54000/54000 [==============================] - 1s 15us/step - loss: 0.0391 - acc: 0.9894 - val_loss: 0.1772 - val_acc: 0.9672 Epoch 47/50 54000/54000 [==============================] - 1s 15us/step - loss: 0.0386 - acc: 0.9893 - val_loss: 0.1684 - val_acc: 0.9700 Epoch 48/50 54000/54000 [==============================] - 1s 15us/step - loss: 0.0392 - acc: 0.9886 - val_loss: 0.1530 - val_acc: 0.9723 Epoch 49/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0336 - acc: 0.9905 - val_loss: 0.1777 - val_acc: 0.9667 Epoch 50/50 54000/54000 [==============================] - 1s 14us/step - loss: 0.0315 - acc: 0.9907 - val_loss: 0.1884 - val_acc: 0.9678 현재 저의 GPU는 GTX 750TI를 사용중입니다.에폭당 8초정도의 시간을 보여주고 있네요. CPU로도 빠른속도를 보일 수 있도록 모델을 변경했습니다.변경된 모델로 학습에 약 1분정도 소요됩니다. 정확도, 손실 시각화1234567891011121314151617181920212223242526272829303132333435import matplotlib.pyplot as pltdef show_graph(history_dict): accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy and loss') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history.history) 첫번째 에폭의 결과가 너무 편차가 심하여 그래프를 알아보기 힘드니 10번째 에폭의 결과부터 그래프로 그려보도록 하겠습니다. 123m = map(lambda x: (x[0], x[1][10:]), history.history.items())not_noise_history = dict(list(m))show_graph(not_noise_history) 대략 20 에폭부터 과대적합이 시작되는걸 확인할 수 있습니다. 1model.evaluate(reshape_x_test, y_test) 10000/10000 [==============================] - 0s 14us/step [0.21370663271014564, 0.9631] 96%의 정확도를 보여주고 있습니다.레이어를 더 깊고 넓게 구성을 하여 높은 정확도를 만들어보세요. 모델 구성 - CNN지금까지 포스팅한 간단한 DNN으로도 MNIST 데이터셋의 분류를 높은 정확도로 만들 수 있습니다.하지만 CNN을 이용하면 적은 파라메터로 좀더 정확한 모델을 구성할 수 있습니다.비전 인식에서 딥러닝이 크게 활약한 이유가 이런곳에 있습니다.간단한 CNN 모델을 구성해보겠습니다. 네트워크 구성Kernel Initializer로는 유명한 Xavier Initializer를 사용합니다. 12345678910111213141516171819202122232425model = Sequential()# model.add(layers.InputLayer())model.add(layers.Conv2D( input_shape=(28, 28, 1), filters=64, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_normal',))model.add(layers.MaxPool2D())model.add(layers.Conv2D( filters=128, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_normal'))model.add(layers.MaxPool2D())model.add(layers.Flatten())model.add(layers.Dense(10, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 64) 320 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 14, 14, 64) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 14, 14, 128) 32896 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 6272) 0 _________________________________________________________________ dense_3 (Dense) (None, 10) 62730 ================================================================= Total params: 95,946 Trainable params: 95,946 Non-trainable params: 0 _________________________________________________________________ 모델 컴파일저희는 레이블 데이터를 원-핫 인코딩을 진행하지 않았기 때문에 Loss 펑션에 sparse_categorical_crossentropy를 지정합니다. 12345model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) 모델 학습CPU로 돌리시는 경우 시간이 조금 걸릴 수 있으니 커피한잔 하고 오세요~ 1234567history = model.fit( reshape_x_train, y_train, batch_size=128, epochs=20, validation_split=.1,) Train on 54000 samples, validate on 6000 samples Epoch 1/20 54000/54000 [==============================] - 15s 278us/step - loss: 4.6634 - acc: 0.6814 - val_loss: 0.0709 - val_acc: 0.9788 Epoch 2/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0665 - acc: 0.9799 - val_loss: 0.0560 - val_acc: 0.9840 Epoch 3/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0472 - acc: 0.9854 - val_loss: 0.0516 - val_acc: 0.985570 - acc: 0 Epoch 4/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0372 - acc: 0.9880 - val_loss: 0.0491 - val_acc: 0.9863 loss: 0.0353 - ac - ETA Epoch 5/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0449 - val_acc: 0.9882 Epoch 6/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0235 - acc: 0.9919 - val_loss: 0.0557 - val_acc: 0.9848 Epoch 7/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0222 - acc: 0.9928 - val_loss: 0.0687 - val_acc: 0.9840 Epoch 8/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.0524 - val_acc: 0.9882 Epoch 9/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0798 - val_acc: 0.9818 Epoch 10/20 54000/54000 [==============================] - 9s 161us/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0748 - val_acc: 0.9853 Epoch 11/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0734 - val_acc: 0.9860 Epoch 12/20 54000/54000 [==============================] - 9s 161us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0706 - val_acc: 0.9860 Epoch 13/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0928 - val_acc: 0.9842 Epoch 14/20 54000/54000 [==============================] - 9s 161us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0892 - val_acc: 0.9847 Epoch 15/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0949 - val_acc: 0.9847 Epoch 16/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0868 - val_acc: 0.9855 Epoch 17/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0988 - val_acc: 0.9838 Epoch 18/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.1250 - val_acc: 0.9805 Epoch 19/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0855 - val_acc: 0.9860 Epoch 20/20 54000/54000 [==============================] - 9s 160us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0942 - val_acc: 0.9865 제 노트북 CPU를 기준으로 모델을 학습하는데 에폭당 48초정도 걸리네요.20 에폭을 돌렸으니 대략 16분 정도 걸릴거라 보시면 됩니다. 저는 GPU를 사용하겠습니다. 정확도 및 손실 시각화1show_graph(history.history) 첫 에폭의 결과 때문에 편차가 심하여 첫 에폭을 제외하고 보도록 하겠습니다. 123m = map(lambda x: (x[0], x[1][1:]), history.history.items())not_noise_history = dict(list(m))show_graph(not_noise_history) 검증세트의 손실을 보니 8번째 에폭부터 과대적합이 된 것 같군요.과대적합을 해결할 수 있다면 정확도를 높일 수 있을까요 ? 과대적합 해결위와 같은 과대적합을 피하는 방법으로는 여러가지 방법이 있는데요.L1, L2 규제와 같은 정규화를 이용하여 가중치를 감쇠(Weight Decay)시키는 방법과 드랍아웃, 모델의 하이퍼파라메터 튜닝 등으로 과대적합을 피하려 할 수 있습니다. 모델의 재구성기존 모델은 2단의 컨볼루션 레이어와 풀링 레이어 사용했습니다.모델이 정보를 더 가질 수 있도록 레이어를 더 구성시키고, 드랍아웃과 L2 규제를 걸어 더 복잡한 모델이지만 과대적합은 피할 수 있도록 재구성해보죠. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from tensorflow.keras.models import Sequentialfrom tensorflow.keras import layersfrom tensorflow.keras import regularizersmodel = Sequential()model.add(layers.Conv2D( input_shape=(28, 28, 1), filters=32, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.01),))model.add(layers.MaxPool2D())model.add(layers.Dropout(.5))model.add(layers.Conv2D( filters=64, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.01),))model.add(layers.MaxPool2D())model.add(layers.Dropout(.5))model.add(layers.Conv2D( filters=128, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.01),))model.add(layers.MaxPool2D())model.add(layers.Dropout(.5))model.add(layers.Conv2D( filters=256, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.01),))model.add(layers.MaxPool2D())model.add(layers.Dropout(.5))model.add(layers.Flatten())model.add(layers.Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(0.01),))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_2 (Conv2D) (None, 28, 28, 32) 160 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32) 0 _________________________________________________________________ dropout (Dropout) (None, 14, 14, 32) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 14, 14, 64) 8256 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 7, 7, 64) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 7, 7, 128) 32896 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 3, 3, 128) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 3, 3, 256) 131328 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 1, 1, 256) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 256) 0 _________________________________________________________________ dense_4 (Dense) (None, 10) 2570 ================================================================= Total params: 175,210 Trainable params: 175,210 Non-trainable params: 0 _________________________________________________________________ 아까보다 두개의 레이어를 더 추가했고, 파라메터 수가 2배 가까이 늘어난게 보입니다.중간중간 드랍아웃이 적용되었고, 레이어별로 L2 정규화를 추가했습니다.이러한 정규화 기법을 추가하지 않고 학습을 했다면 이전보다 더 빠르게 과대적합이 일어나겠죠?그럼 드랍아웃과 L2규제를 추가한 이후로 모델이 과대적합을 어떻게 견뎌내는지 확인해봅시다. 모델 컴파일12345model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) 학습 시작Dropout과 Weight Decay를 추가함으로써 과대적합을 막았고, 뉴런의 학습에 제약을 두었으니, 학습이 더뎌질 수 밖에 없습니다.충분히 학습할 수 있도록 에폭수를 조금 더 늘려 학습을 진행하도록 합니다. 1234567history = model.fit( reshape_x_train, y_train, batch_size=128, epochs=30, validation_split=.1,) Train on 54000 samples, validate on 6000 samples Epoch 1/30 54000/54000 [==============================] - 11s 199us/step - loss: 16.5374 - acc: 0.0995 - val_loss: 15.8382 - val_acc: 0.0952 Epoch 2/30 54000/54000 [==============================] - 9s 166us/step - loss: 15.3383 - acc: 0.0990 - val_loss: 15.0753 - val_acc: 0.0952 - Epoch 3/30 54000/54000 [==============================] - 9s 165us/step - loss: 11.6865 - acc: 0.1130 - val_loss: 2.4193 - val_acc: 0.3727 Epoch 4/30 54000/54000 [==============================] - 9s 165us/step - loss: 1.6351 - acc: 0.5584 - val_loss: 0.6810 - val_acc: 0.9327 Epoch 5/30 54000/54000 [==============================] - 9s 165us/step - loss: 0.9334 - acc: 0.8091 - val_loss: 0.4898 - val_acc: 0.9630 - acc Epoch 6/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.7393 - acc: 0.8710 - val_loss: 0.4268 - val_acc: 0.9718 Epoch 7/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.6583 - acc: 0.8908 - val_loss: 0.4011 - val_acc: 0.9752 Epoch 8/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.6127 - acc: 0.9045 - val_loss: 0.3935 - val_acc: 0.9735 Epoch 9/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.6006 - acc: 0.9076 - val_loss: 0.3735 - val_acc: 0.9792: 0s - loss: 0.6030 - Epoch 10/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5846 - acc: 0.9099 - val_loss: 0.3665 - val_acc: 0.9803 Epoch 11/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5774 - acc: 0.9119 - val_loss: 0.3618 - val_acc: 0.9782 Epoch 12/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5714 - acc: 0.9148 - val_loss: 0.3764 - val_acc: 0.9780 Epoch 13/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5682 - acc: 0.9154 - val_loss: 0.3707 - val_acc: 0.9763 Epoch 14/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5633 - acc: 0.9163 - val_loss: 0.3611 - val_acc: 0.9813 Epoch 15/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5628 - acc: 0.9177 - val_loss: 0.3728 - val_acc: 0.9792 Epoch 16/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5665 - acc: 0.9181 - val_loss: 0.3630 - val_acc: 0.9820 Epoch 17/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5650 - acc: 0.9168 - val_loss: 0.3526 - val_acc: 0.9835 - ETA: 3s - los Epoch 18/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5632 - acc: 0.9172 - val_loss: 0.3584 - val_acc: 0.9823 Epoch 19/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5597 - acc: 0.9176 - val_loss: 0.3576 - val_acc: 0.9818 Epoch 20/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5548 - acc: 0.9193 - val_loss: 0.3577 - val_acc: 0.9813 Epoch 21/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5558 - acc: 0.9202 - val_loss: 0.3812 - val_acc: 0.9758 Epoch 22/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5601 - acc: 0.9179 - val_loss: 0.3601 - val_acc: 0.9830 Epoch 23/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5571 - acc: 0.9189 - val_loss: 0.3596 - val_acc: 0.9818 Epoch 24/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5495 - acc: 0.9200 - val_loss: 0.3568 - val_acc: 0.9817 Epoch 25/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5490 - acc: 0.9202 - val_loss: 0.3602 - val_acc: 0.9820 Epoch 26/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5529 - acc: 0.9205 - val_loss: 0.3563 - val_acc: 0.9822 Epoch 27/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5538 - acc: 0.9198 - val_loss: 0.3534 - val_acc: 0.9840 Epoch 28/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5492 - acc: 0.9204 - val_loss: 0.3573 - val_acc: 0.9812 Epoch 29/30 54000/54000 [==============================] - 9s 164us/step - loss: 0.5530 - acc: 0.9198 - val_loss: 0.3508 - val_acc: 0.9848 Epoch 30/30 54000/54000 [==============================] - 9s 163us/step - loss: 0.5521 - acc: 0.9198 - val_loss: 0.3648 - val_acc: 0.9792 모델 평가정규화 기법을 적용한 학습 결과가 어떻게 변했는지 확인 해봅시다. 1show_graph(history.history) 가중치의 급격환 변화를 L2 규제를 통해 막아두어 처음 2에폭까지는 큰 변화가 없다가 3~5 에폭이 지나서 모델이 높은 성능에 근접해지는걸 확인할 수 있습니다.그럼 조금 더 자세하게 볼 수 있도록 첫 5에폭까지의 데이터는 제외하고 이후의 데이터를 그래프로 확인해보겠습니다. 123m = map(lambda x: (x[0], x[1][5:]), history.history.items())not_noise_history = dict(list(m))show_graph(not_noise_history) 트레이닝 세트의 정확도가 낮게 측정되는건 드랍아웃 때문입니다.학습마다 몇몇의 특정 뉴런을 비활성화 시키기 때문에 정확도가 낮아지게 됩니다.훈련을 마친 이후 예측시에는 드랍아웃 시키는 뉴런의 수를 0%로 맞추고 사용합니다.(모든 뉴런을 사용) 1234model.evaluate( reshape_x_train, y_train) 60000/60000 [==============================] - 6s 103us/step [0.38104470246632893, 0.9750666666666666] 드랍아웃을 해제한 후 훈련세트의 손실값과 정확도입니다. 1234model.evaluate( reshape_x_test, y_test) 10000/10000 [==============================] - 1s 98us/step [0.3756561163902283, 0.9753] 테스트 세트의 손실과 정확도입니다.98%의 정확도를 보여주고 있습니다.모델의 네트워크를 변경하여 99% 정확도에 도전해보세요.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"},{"name":"CNN","slug":"cnn","permalink":"https://codingcrews.github.io/tags/cnn/"}]},{"title":"AWS로 GPU기반의 딥러닝 학습환경 구축하기","slug":"deeplearning-gpu","date":"2019-01-15T07:42:51.000Z","updated":"2019-01-29T18:35:26.612Z","comments":true,"path":"2019/01/15/deeplearning-gpu/","link":"","permalink":"https://codingcrews.github.io/2019/01/15/deeplearning-gpu/","excerpt":"","text":"Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기안녕하세요. 오늘은 아마존 웹 서비스(이하 AWS)를 이용하여 GPU 인스턴스를 이용한 딥러닝 학습환경 만들기에 대해 알아봅시다. 딥러닝이란 최근 핫해진 뉴럴넷 기반의 기계학습 기법을 말하는데요.학습 시 굉장히 많은 연산을 필요로 하여 학습에 소요되는 시간이 많이 필요합니다.단순한 연산을 많이 하기 때문에 병렬처리에 특화된 GPU를 사용하여 학습시간을 단축시킬 수 있는데요.당연히 좋은 GPU를 사용할수록 짧은 학습시간이 필요하겠지요.하지만 문제는 이 GPU 가격이 개인이 사서 사용하기엔 너무나도 비싸다고 느낄 수 있습니다. 위 사진은 GPU 연산에 특화되어 나온 모델들입니다.저희가 사용할 AWS EC2 P2, P3 인스턴스에 제공되는 GPU 모델들입니다. 포스팅 날짜를 기준으로 최소 200만원 이상의 가격을 보여주고 있는데요.개인이라도 사지 못할 가격은 아닙니다만, 딥러닝이 무엇인지 이제 알아가는 시점에 입문자가 구매하기엔 굉장히 벅찬 금액이죠.하지만 AWS에서 제공해주는 GPU인스턴스를 빌려서 시간제로 비용을 낼 수 있다면, 비싼 GPU를 구매하지 않고도 필요할 때에만 성능이 좋은 GPU를 이용할 수 있겠죠 ? Aws EC2EC2에 관한 설명은 생략하도록 하고, 여기에서 자세하게 확인 가능합니다. 인스턴스 종류 및 가격저희는 GPU 인스턴스를 사용할 예정이기 때문에 P, G 두가지 계열을 확인하면 됩니다.인스턴스 스펙으로 이동하여 자세한 스펙을 확인 가능합니다.각 인스턴스의 가격은 시간에 비례하여 비용이 청구되는 방식이며, 각 리전마다 사용가격의 편차가 있습니다. 서울리전은 해외리전보다 비싼편이니 (약 2배가량 비쌈), 연구목적으로 사용하실 때에는 해외리전을 이용하시는걸 추천드립니다. 위 사진은 오하이오 리전의 가격이며, 아래 사진은 서울리전의 가격입니다.같은 p2.xlarge의 가격을 보더라도 오하이오 리전은 시간당 \\$0.9, 서울리전은 시간당 \\$1.465의 가격을 보여줍니다.그리고, 서울리전은 g3계열의 인스턴스가 제공되지 않고 있습니다. 오하이오 리전의 p2.xlarge 가격을 보면 대략 시간당 1,000원 정도의 가격을 보여줍니다.요새 PC방 가격도 시간당 1,000원이 넘는걸 생각하면 그리 비싸다고 생각하진 않을 수 있습니다. 스팟 인스턴스하지만 위의 가격은 온디맨드 계약의 가격이며, AWS에서 더 할인을 받을 수 있는 방법이 존재합니다.그건 바로 R.I(예약 인스턴스) 혹은 Spot인스턴스를 이용하는 방법입니다.R.I의 경우는 최소 계약단위가 1년 단위이기 때문에 개인이 하기엔 부담스럽고, 오히려 서비스를 운영중인 회사에서 GPU인스턴스가 필요할 때 사용할 수 있겠습니다. 스팟 인스턴스의 경우는 인스턴스를 경매입찰방식으로 할당받는 개념인데, 최대 90%까지 가격이 할인될 수 있습니다. 온디맨드의 가격보다 더 저렴한 가격을 확인할 수 있습니다.이 스팟 인스턴스를 이용해보도록 하겠습니다. 스팟 인스턴스 생성 AWS 관리 콘솔에서 스팟 요청을 눌러줍니다. 자주 쓰이는 형태의 패턴들이 이미 만들어져 있으나, 명시한 시간동안 인스턴스를 사용할 수 있도록 시간을 지정하여 사용합니다. (최소 1시간에서 최대 6시간까지 사용 가능합니다.) 인스턴스 타입 변경을 눌러 사용할 인스턴스의 타입을 지정합니다. 저는 GPU compute에서 p2.xlarge를 이용하겠습니다.GPU instance에서 조금 더 저렴한 G3 계열의 인스턴스도 사용 가능합니다. AMI 검색을 눌러 사용할 기본 AMI를 지정합니다. 딥러닝 프레임워크를 사용하여 GPU를 사용하기 위해선 GPU 설정이 많이 필요한데, 이러한 사전 작업을 마친 템플릿을 AWS에서 공식 AMI로 제공합니다.꼭 소유자를 확인하여 아마존 공식 이미지가 맞는지 확인합니다.(일반 유저도 AMI를 만들어 배포할 수 있어 마이닝 프로그램을 설치해둔 템플릿들이 다수 존재하니 주의합시다.) Additional configurations를 눌러 시큐리티 그룹 등 인스턴스에 필요한 설정을 마쳐줍시다. 스팟 인스턴스가 정상적으로 생성되었습니다. 간혹 해외리전의 스팟 인스턴스 제한이 걸려있어 생성되지 않는 경우가 존재합니다.이때는 AWS에 케이스를 오픈하여 리밋제한을 상향요청 할 수 있습니다. 생성된 인스턴스 확인 생성된 인스턴스의 유형과 최대 가격을 볼 수 있습니다.최대가격이란 명시된 시간동안 다른 인스턴스들이 높은 입찰가를 제시할 때 인스턴스가 종료 될 수 있으니 최대 가격이 온디맨드 가격으로 자동적으로 조절됩니다. 영구적으로 사용할 볼륨 생성하기스팟 인스턴스가 종료될 시 75GB로 할당된 AMI 볼륨이 자동적으로 삭제됩니다.인스턴스 생성 시 해당 볼륨을 삭제하지 않을 수 있으나, 75GB의 볼륨을 갖고 있기는 부담스럽기 때문에 우리가 필요한 데이터들만 저장할 수 있는 작은 크기의 볼륨을 따로 생성하여 인스턴스의 종료와 무관하게 사용할 수 있도록 새로운 볼륨을 만들어 추가해봅시다. 데이터 영구보존 볼륨 생성하기 좌측의 볼륨 메뉴를 눌러 현재 인스턴스의 가용영역을 확인 한 이후,볼륨 생성을 눌러줍니다. 바로 이전에 확인한 가용영역을 맞춰 새로운 볼륨을 생성합니다. 가용영역이 다르다면 볼륨이 인스턴스에 사용할 수 없으니 꼭 가용영역을 맞춰 생성합니다. 생성된 볼륨을 확인할 수 있습니다. 우클릭을 이용해 볼륨 연결을 눌러줍니다. 볼륨을 연결합니다. 인스턴스를 눌러주면 자동적으로 같은 가용영역에 있는 사용중인 인스턴스 목록이 나옵니다.디바이스는 해당 인스턴스에 사용될 디바이스명입니다. 여기까지가 콘솔에서 설정 가능한 스팟 인스턴스에 대한 인스턴스 생성과 설정입니다. 이후는 AWS 인스턴스에 ssh접속을 한 이후 설정하는 부분입니다. SSH 접속SSH 로 생성한 스팟 인스턴스에 접근하면 초기 화면이 이렇게 보이는걸 확인할 수 있습니다.각각의 가상환경에 진입할 수 있는 명령어와 사용가능한 환경에 대한 설명이 보입니다. ============================================================================= __| __|_ ) _| ( / Deep Learning AMI (Amazon Linux) Version 20.0 ___|\\___|___| ============================================================================= Please use one of the following commands to start the required environment with the framework of your choice: for MXNet(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p36 for MXNet(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p27 for MXNet(+Amazon Elastic Inference) with Python3 _______________________________________ source activate amazonei_mxnet_p36 for MXNet(+Amazon Elastic Inference) with Python2 _______________________________________ source activate amazonei_mxnet_p27 for TensorFlow(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p36 for TensorFlow(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p27 for Tensorflow(+Amazon Elastic Inference) with Python2 _____________________________ source activate amazonei_tensorflow_p27 for Theano(+Keras2) with Python3 (CUDA 9.0) _____________________________________________________ source activate theano_p36 for Theano(+Keras2) with Python2 (CUDA 9.0) _____________________________________________________ source activate theano_p27 for PyTorch with Python3 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p36 for PyTorch with Python2 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p27 for CNTK(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p36 for CNTK(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p27 for Caffe2 with Python2 (CUDA 9.0) ______________________________________________________________ source activate caffe2_p27 for Caffe with Python2 (CUDA 8.0) ________________________________________________________________ source activate caffe_p27 for Caffe with Python3 (CUDA 8.0) ________________________________________________________________ source activate caffe_p35 for Chainer with Python2 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p27 for Chainer with Python3 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p36 for base Python2 (CUDA 9.0) ________________________________________________________________________ source activate python2 for base Python3 (CUDA 9.0) ________________________________________________________________________ source activate python3 Official Conda User Guide: https://conda.io/docs/user-guide/index.html AWS Deep Learning AMI Homepage: https://aws.amazon.com/machine-learning/amis/ Developer Guide and Release Notes: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html Support: https://forums.aws.amazon.com/forum.jspa?forumID=263 For a fully managed experience, check out Amazon SageMaker at https://aws.amazon.com/sagemaker ============================================================================= 파이썬 가상환경 진입저희는 Python 3.6기반의 Tensorflow 사용할 수 있는 가상환경으로 진입해보겠습니다.1234[ec2-user@ip-172-31-15-213 ~]$ source activate tensorflow_p36# 파이썬 가상환경으로 진입하면 쉘이 아래와 같이 변합니다. (tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ 연결한 볼륨 확인생성하고 나서 연결한 볼륨에 대한 정보를 확인합니다. 1(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 75G 0 disk └─xvda1 202:1 0 75G 0 part / xvdf 202:80 0 8G 0 disk 아까 연결한 8GB의 볼륨이 xvdf로 연결되어있는걸 확인할 수 있습니다. 볼륨의 포맷 확인연결된 xvdf 디바이스를 통해 볼륨의 포맷을 확인합니다.1(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf /dev/xvdf: data 갓 생성한 볼륨의 경우 data로 나오게 될 것이고, 이 볼륨은 사용하고 있는 os 파일 시스템에 맞춰 파일 포맷을 진행해줘야 합니다. 볼륨 ext4 포맷볼륨의 파일시스템을 ext4 포맷으로 변경합니다.1(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mkfs -t ext4 /dev/xvdf mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 2097152 4k blocks and 524288 inodes Filesystem UUID: 6ce22348-395a-4ac1-8df7-e180aaadaadf Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done ​ 볼륨 포맷 재확인파일 시스템 포맷이 완료 된 후 ext4 포맷으로 변경됬는지 확인합니다. 1(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf /dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf (extents) (64bit) (large files) (huge files) 결과에 나오는 UUID를 잘 기억하세요. 볼륨 마운트포맷 완료 된 볼륨을 사용하기 위해서는 마운트 과정이 필요합니다.현재 Home 디렉터리에 새로운 디렉터리를 만들어 마운트를 진행합니다. 12(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ mkdir mount_dir(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount /dev/xvdf mount_dir/ 정상적으로 마운트 되었다면 에러 메세지 없이 완료됩니다. 볼륨 자동 마운트12cp /home/ubuntu ubutnumount /dev/xvdf ~/smount 위와 같이 수동으로 마운트 시켜 사용하는 경우 인스턴스가 재시동 되는 경우 마운트를 다시 해줘야 합니다./etc/fstab 이라는 폴더는 부팅 시 마운트 해야할 목록을 가지고 있어 자동으로 마운트가 진행되도록 합니다.fstab 파일이 잘못 작성 되어 있으면 부팅이 안되는 경우도 발생하니 백업을 만들고, 신중하게 파일을 수정하도록 합시다. 아래 명령어는 복붙하지마시고 본인의 UUID와 마운트 할 디렉터리에 맞춰 변경하여 사용하세요.1234(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo cp /etc/fstab /etc/fstab.orig (tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ echo \"UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf /home/ec2-user/mount_dir ext4 defaults,nofail 0 2\" | sudo tee --apend /etc/fstab(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount -a 마지막 mount -a 옵션을 꼭 실행해보시고, 정상적이라면 아무 메시지 없이 완료됩니다.에러 메세지가 발생한 경우 fstab 파일에 문제가 있을 수 있으니 꼭 확인하세요. Jupyter Notebook 실행원격에서 접속하기 위해 IP 대역을 전부 풀어줍니다.Port 번호는 기본적으로 8888 포트를 사용하니 시큐리티 그룹에서 해당 포트를 허용시켜주시고, 다른 포트번호로 사용하셔도 무방합니다. 1(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ jupyter notebook --ip 0.0.0.0 [I 06:43:24.447 NotebookApp] Using EnvironmentKernelSpecManager... [I 06:43:24.448 NotebookApp] Started periodic updates of the kernel list (every 3 minutes). [I 06:43:24.552 NotebookApp] Writing notebook server cookie secret to /home/ec2-user/.local/share/jupyter/runtime/notebook_cookie_secret [I 06:43:27.661 NotebookApp] Loading IPython parallel extension [I 06:43:27.784 NotebookApp] JupyterLab beta preview extension loaded from /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/jupyterlab [I 06:43:27.784 NotebookApp] JupyterLab application directory is /home/ec2-user/anaconda3/envs/tensorflow_p36/share/jupyter/lab [I 06:43:28.405 NotebookApp] [nb_conda] enabled [I 06:43:28.408 NotebookApp] Serving notebooks from local directory: /home/ec2-user [I 06:43:28.408 NotebookApp] 0 active kernels [I 06:43:28.408 NotebookApp] The Jupyter Notebook is running at: [I 06:43:28.408 NotebookApp] http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d [I 06:43:28.408 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 06:43:28.408 NotebookApp] No web browser found: could not locate runnable browser. [C 06:43:28.408 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d&amp;token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d [I 06:43:28.409 NotebookApp] Starting initial scan of virtual environments... ec2 인스턴스의 public ip 주소가 만약 1.10.10.10 이라고 가정하면,http://1.10.10.10:8888 로 접속하시면 위에 출력된 해당 토큰을 요청합니다.토큰을 입력해주면 쥬피터 노트북을 사용하실 수 있습니다.","categories":[{"name":"AWS","slug":"aws","permalink":"https://codingcrews.github.io/categories/aws/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"AWS","slug":"aws","permalink":"https://codingcrews.github.io/tags/aws/"}]},{"title":"딥러닝으로 보스턴의 집값을 예측해보자.","slug":"boston_linear_regression","date":"2019-01-14T18:06:54.000Z","updated":"2019-01-29T18:39:06.890Z","comments":true,"path":"2019/01/15/boston_linear_regression/","link":"","permalink":"https://codingcrews.github.io/2019/01/15/boston_linear_regression/","excerpt":"","text":"선형 회귀 문제 : Linear Regression머신러닝으로 해결할 수 있는 문제 중 분류 문제들을 이전 포스팅에서 다뤄보았고,이번에는 연속된 데이터를 예측해야하는 회귀 문제를 보겠습니다. 이전의 포스팅은 데이터가 주어졌을 때 리뷰의 긍정/부정 분류, 뉴스 기사의 토픽 분류 등의 문제를 다뤘었고,회귀 문제란 데이터가 주어졌을 때 주택 가격을 예측하는것처럼 연속된 값을 예측하는걸 회귀 문제라고 합니다. 오늘은 선형 회귀 모델을 이용하여 1970년대 보스턴의 집값을 예측하는 모델을 만들어 보겠습니다. Dataset사용하는 데이터셋은 카네기 멜런 대학교에서 제공하는 데이터셋으로 데이터의 설명은 여기에서 확인할 수 있습니다. Download12from tensorflow.keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_targets.shape)) Train Datas Shape : (404, 13) Train Labels Shape : (404,) 데이터 확인각 컬럼별로 해당하는 데이터들을 확인할 수 있고, 레이블에는 해당 주택의 가격값이 들어가있는걸 확인할 수 있습니다. 12display(train_data[0])display(train_targets[0:10]) array([ 1.23247, 0. , 8.14 , 0. , 0.538 , 6.142 , 91.7 , 3.9769 , 4. , 307. , 21. , 396.9 , 18.72 ]) array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4]) 데이터 변환신경망 학습 시 차이가 큰 값을 신경망에 주입하면 학습이 잘 되지 않습니다.(0.01, 0.1)과 (1, 10)을 보면 비율대로만 보게되면 10배의 차이를 가지는걸 볼 수 있지만,실수 체계에서 값의 차이는 어마어마하게 큰 차이로 볼 수 있습니다.(1, 10)과 같은 데이터를 Sparse 하다라고 표현하며,(0.01, 0.1)과 같은 밀도가 높은 데이터를 Dense 데이터라고 표현합니다. 이렇게 Dense 데이터로 변경하는 과정이 필요한데, 이 부분을 Scaling 혹은 일반화 한다고 합니다. 데이터의 일반화를 위해서 각 특성별 평균값을 뺀 이후에 표준편차로 나눠주게 되면,특성의 중앙이 0으로, 표준편차는 1인 정규분포가 됩니다. 1234567mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std 모델 구성데이터셋이 작으므로 간단한 모델을 구성하면 과대적합을 피할 수 있습니다. 12345678910from tensorflow.keras import modelsfrom tensorflow.keras import layersdef build_network(input_shape=(0,)): model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=input_shape)) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='adam', loss='mse', metrics=['mae']) return model 모델 구성의 마지막 Dense layer를 보면 활성함수가 지정 되어있지 않은걸 확인할 수 있는데,우리가 필요한 데이터는 선형 데이터를 원하기 때문에 활성함수를 지정하지 않으면 스칼라 회귀를 위한 구성이 완성됩니다.반대로 sigmoid 같은 활성화 함수를 적용하면 네트워크가 0~1 사이의 값을 만들도록 만듭니다. 우리가 필요한 데이터는 주택가격을 예측하기 위함으로 활성함수를 지정하지 않고 스칼라 회귀 값을 예측하도록 구성합니다. K-Fold Validation데이터셋의 크기가 매우 작기 때문에 검증셋(Validation)의 크기도 매우 작아지게 되는데,이때 데이터셋이 적어지면서 훈련세트 중 어느 한 특정 부분이 훈련세트로 사용되는지에 따라 모델의 정확도가 크게 달라질 수 있습니다.이 이유는 훈련세트와 검증세트로 나눴을 때 각 값들의 분포도가 고르게 되지 못할 경우가 생기기 때문입니다. 이런 상황에서 가장 좋은 방법은 K-겹 교차 검증(K-Fold Cross Validation)을 실시하는것인데,데이터를 K개의 분할로 나누고 각각 K개의 모델을 만들어 K-1 개의 분할에서 훈련하고 나머지 분할된 데이터에서 평가를 하게됩니다.이 점수는 각 검증 데이터셋의 평균으로 모델을 평가하게 됩니다. 즉, 여러 폴드의 데이터셋으로 나누어 교차검증을 함으로써 데이터 분포에 신경쓰지 않고 훈련세트의 모든 부분을 사용해 모델을 평가할 수 있는 장점이 있습니다. 12345678910111213141516171819202122232425262728293031323334353637import numpy as npk = 4num_val_samples = len(train_data) // knum_epochs = 150all_scores = []all_history = []for i in range(k): print('폴드 번호 #&#123;&#125;'.format(i)) fold_start_index = i * num_val_samples fold_end_index = (i + 1) * num_val_samples val_data = train_data[fold_start_index : fold_end_index] val_targets = train_targets[fold_start_index : fold_end_index] partial_train_data = np.concatenate( [train_data[:fold_start_index], train_data[fold_end_index:]], axis=0 ) partial_train_targets = np.concatenate( [train_targets[:fold_start_index], train_targets[fold_end_index:]], axis=0 ) model = build_network((partial_train_data.shape[1], )) history = model.fit( partial_train_data, partial_train_targets, epochs=num_epochs, validation_data=(val_data, val_targets), batch_size=1, verbose=0 ) val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0) all_scores.append(val_mse) all_history.append(history.history) 폴드 번호 #0 폴드 번호 #1 폴드 번호 #2 폴드 번호 #3 12345val_mae_lst = map(lambda x: x['val_mean_absolute_error'], all_history)val_mae_lst = np.array(list(val_mae_lst))avg_mae = [ np.mean([x[i] for x in val_mae_lst]) for i in range(num_epochs)] 검증 데이터 시각화검증 데이터 시각화를 위해 history 데이터를 에폭별 평균 데이터로 치환시킵니다. 12345val_mae_lst = map(lambda x: x['val_mean_absolute_error'], all_history)val_mae_lst = np.array(list(val_mae_lst))avg_mae = [ np.mean([x[i] for x in val_mae_lst]) for i in range(num_epochs)] 그래프 변동이 심하지 않도록 이전 포인트의 지수 이동 평균값으로 변경시켜주는 함수를 만듭니다. 12345678910import matplotlib.pyplot as pltdef smooth_curve(points, factor=.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 123456def show_graph(data): smooth_data = smooth_curve(data) plt.plot(range(1, len(smooth_data) + 1), smooth_data) plt.xlabel('Epochs') plt.ylabel('Validation MAE') plt.show() 1show_graph(avg_mae[10:])","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"}]},{"title":"딥러닝으로 뉴스기사의 토픽을 분류해보자.","slug":"reuter_multi_classification","date":"2019-01-14T14:27:59.000Z","updated":"2019-01-29T18:36:28.130Z","comments":true,"path":"2019/01/14/reuter_multi_classification/","link":"","permalink":"https://codingcrews.github.io/2019/01/14/reuter_multi_classification/","excerpt":"","text":"다중 분류 문제 : Multiple class classification이전 포스팅에서 다룬 이진 분류 모델의 경우 예측할 수 있는 결과값이 긍정/부정 두가지 였습니다.하지만 뉴스의 토픽(예를들어 시사/인터넷/연예/스포츠 등)을 예측하기 위해선 이진 분류로는 토픽을 나눌 수 없습니다. 이번 포스팅에서는 2개 이상의 클래스를 가진 경우 사용할 수 있는 다중 분류 문제를 해결해보도록 하겠습니다. Dataset저희는 로이터 데이터셋을 사용하도록 합니다.해당 데이터셋은 46개의 토픽을 갖고 있으며 각 뉴스기사마다 하나의 토픽이 정해져있습니다.즉, 단일 레이블 다중분류 문제로 볼 수 있습니다. Download12from tensorflow.keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_labels.shape)) Train Datas Shape : (8982,) Train Labels Shape : (8982,) 데이터 확인확인한대로 8982개의 훈련 데이터셋이 존재하며,각 인덱스는 단어 인덱스의 리스트를 뜻 합니다. 1234display(train_data[0][:10])display(train_labels)display(test_labels) [1, 2, 2, 8, 43, 10, 447, 5, 25, 207] array([ 3, 4, 3, ..., 25, 3, 25]) array([ 3, 10, 1, ..., 3, 3, 24]) 단어를 텍스트로 디코딩1234word_index = reuters.get_word_index()reverse_word_index = dict([(val, key) for (key, val) in word_index.items()])decoded_news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])print(decoded_news) ? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3 라벨 확인샘플의 라벨은 토픽의 인덱스로써 0~45의 값을 가집니다. 1train_labels[0] 3 데이터 변환학습에 용이하도록 뉴스 기사와 라벨 데이터를 벡터로 변환시킵니다.학습에 사용되는 데이터셋의 인풋 데이터는 해당 뉴스 기사의 들어가있는 단어 인덱스를 1.0 으로 변경시킵니다. 1234567import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성 for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 12345678910111213from tensorflow.keras.utils import to_categoricalx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)display(x_train.shape)display(x_test.shape)one_hot_train_labels = to_categorical(train_labels)one_hot_test_labels = to_categorical(test_labels)display(one_hot_train_labels.shape)display(one_hot_test_labels.shape) (8982, 10000) (2246, 10000) (8982, 46) (2246, 46) 신경망 구성단일 레이블 다중 분류를 위한 모델을 작성 해보도록 하겠습니다.다만 레이어 구축 시 참고해야 할 부분이 있는데,각 레이어를 통과 할 때 유닛의 수가 레이블 보다 적다면 가지고 있어야 할 정보가 많이 사라질 수 있습니다. 학습 시 파라미터를 줄이기 위해서나 노이즈를 줄이기 위해서 유닛을 줄이는 경우도 있지만,데이터가 가지고 있어야 할 필수 데이터를 잃어버릴 수 있다는 점도 참고하여 네트워크를 구성해야 합니다. 신경망 네트워크 구축로이터 데이터셋의 단어수를 10,000개로 제한해두었으니,신경망의 입력 차원수도 10,000으로 설정합니다. 123456789from tensorflow.keras import modelsfrom tensorflow.keras import layersmodel = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 64) 640064 _________________________________________________________________ dense_4 (Dense) (None, 64) 4160 _________________________________________________________________ dense_5 (Dense) (None, 46) 2990 ================================================================= Total params: 647,214 Trainable params: 647,214 Non-trainable params: 0 _________________________________________________________________ 이진분류와 비슷하게 마지막 Dense 레이어의 아웃풋 벡터 개수는(46개) 예측하기 위한 클래스의 개수와 동일합니다.다른점은 이진 분류에서는 활성함수로 sigmoid를 사용한 반면 여기서는 softmax를 이용했는데,softmax는 각 클래스별로 해당 클래스 일 확률을 표시하도록 만들어집니다. 모델 컴파일다중 분류에서 손실함수로써는 categorical_crossentropy를 주로 사용합니다.옵티마이저는 가장 빠르고 효과가 좋다고 알려진 adam 옵티마이저를 사용하도록 설정합니다. 12345model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],) 검증 데이터셋 구성훈련용 데이터셋에서 Validation으로 사용할 데이터셋을 분리시킵니다. 12345x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = one_hot_train_labels[:1000]partial_y_train = one_hot_train_labels[1000:] 모델 학습1234567history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val)) Train on 7982 samples, validate on 1000 samples Epoch 1/20 7982/7982 [==============================] - 1s 152us/step - loss: 3.3032 - acc: 0.4328 - val_loss: 2.6156 - val_acc: 0.5320 Epoch 2/20 7982/7982 [==============================] - 1s 78us/step - loss: 2.0615 - acc: 0.6076 - val_loss: 1.6695 - val_acc: 0.6460 Epoch 3/20 7982/7982 [==============================] - 1s 78us/step - loss: 1.3844 - acc: 0.7051 - val_loss: 1.3219 - val_acc: 0.7100 Epoch 4/20 7982/7982 [==============================] - 1s 77us/step - loss: 1.0796 - acc: 0.7669 - val_loss: 1.1773 - val_acc: 0.7520 Epoch 5/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.8673 - acc: 0.8132 - val_loss: 1.0768 - val_acc: 0.7790 Epoch 6/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.6909 - acc: 0.8515 - val_loss: 0.9995 - val_acc: 0.7910 Epoch 7/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.5410 - acc: 0.8880 - val_loss: 0.9467 - val_acc: 0.8010 Epoch 8/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.4177 - acc: 0.9137 - val_loss: 0.9069 - val_acc: 0.8190 Epoch 9/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.3253 - acc: 0.9300 - val_loss: 0.8855 - val_acc: 0.8160 Epoch 10/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.2593 - acc: 0.9432 - val_loss: 0.8973 - val_acc: 0.8090 Epoch 11/20 7982/7982 [==============================] - 1s 80us/step - loss: 0.2123 - acc: 0.9503 - val_loss: 0.8912 - val_acc: 0.8210 Epoch 12/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1792 - acc: 0.9549 - val_loss: 0.9012 - val_acc: 0.8230 Epoch 13/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1594 - acc: 0.9565 - val_loss: 0.9194 - val_acc: 0.8170 Epoch 14/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1410 - acc: 0.9573 - val_loss: 0.9531 - val_acc: 0.8150 Epoch 15/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.1294 - acc: 0.9578 - val_loss: 0.9501 - val_acc: 0.8160 Epoch 16/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1192 - acc: 0.9603 - val_loss: 0.9757 - val_acc: 0.8130 Epoch 17/20 7982/7982 [==============================] - 1s 77us/step - loss: 0.1146 - acc: 0.9592 - val_loss: 0.9701 - val_acc: 0.8100 Epoch 18/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.1058 - acc: 0.9600 - val_loss: 0.9883 - val_acc: 0.8080 Epoch 19/20 7982/7982 [==============================] - 1s 80us/step - loss: 0.1055 - acc: 0.9624 - val_loss: 1.0214 - val_acc: 0.8130 Epoch 20/20 7982/7982 [==============================] - 1s 81us/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.9970 - val_acc: 0.8150 훈련의 정확도와 손실 시각화이전 이진분류 포스팅에서 사용했던 함수를 그대로 끌어와 사용하도록 합니다. 123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltdef show_graph(history): history_dict = history.history accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history) 그래프를 보면 대략 8~9번째 에폭부터 과대적합이 시작되는걸 알 수 있습니다. 새로운 데이터 예측해보기모델을 이용해 각 뉴스기사에 대한 토픽을 예측해보도록 합니다.softmax를 사용하여 각 뉴스별로 46개의 토픽에 해당하는 확률을 출력합니다.각 클래스 별 확률을 모두 더하면 1.0(100%)가 됩니다. 12predictions = model.predict(x_test)display(predictions.shape) (2246, 46) predictions는 테스트 데이터셋의 개수에 맞게 2246개의 결과가 들어있습니다.각 결과안에는 46개 클래스에 해당하는 확률값이 들어가있습니다. 1np.sum(predictions[0]) 1.0 각 46개의 모든 원소의 값을 모두 더하면 1.0(100%)가 된걸 확인할 수 있습니다. 12display(np.argmax(predictions[0]))display(predictions[0][3]) 3 0.96262544 predictions[0]의 3번째 인덱스가 가장 큰 값을 가진걸 확인하였고,해당 인덱스의 값을 확인하였더니 0.96262544(96.262544%)로 모델이 예측한걸 확인할 수 있습니다. 데이터 레이블 인코딩 방식 변경하여 학습하기one hot 인코딩이 아닌 정수형으로 토픽을 예측하도록 레이블 인코딩을 사용하도록 변경해봅니다.손실함수를 변경해주면 되는데,categorical_crossentropy는 범주형 인코딩일 시 사용하는 손실함수이고,정수형을 사용할 때에는 sparse_categorical_crossentropy를 사용합니다. 12345678y_train = np.array(train_labels)y_test = np.array(test_labels)x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:] 123456model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 64) 640064 _________________________________________________________________ dense_7 (Dense) (None, 64) 4160 _________________________________________________________________ dense_8 (Dense) (None, 46) 2990 ================================================================= Total params: 647,214 Trainable params: 647,214 Non-trainable params: 0 _________________________________________________________________ 12345model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) 1234567history = model.fit( partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val)) Train on 7982 samples, validate on 1000 samples Epoch 1/9 7982/7982 [==============================] - 1s 116us/step - loss: 3.3313 - acc: 0.3983 - val_loss: 2.5661 - val_acc: 0.5770 Epoch 2/9 7982/7982 [==============================] - 1s 78us/step - loss: 2.0006 - acc: 0.6272 - val_loss: 1.6576 - val_acc: 0.6580 Epoch 3/9 7982/7982 [==============================] - 1s 78us/step - loss: 1.3389 - acc: 0.7134 - val_loss: 1.2907 - val_acc: 0.7090 Epoch 4/9 7982/7982 [==============================] - 1s 78us/step - loss: 1.0256 - acc: 0.7762 - val_loss: 1.1484 - val_acc: 0.7630 Epoch 5/9 7982/7982 [==============================] - 1s 80us/step - loss: 0.8077 - acc: 0.8339 - val_loss: 1.0422 - val_acc: 0.7870 Epoch 6/9 7982/7982 [==============================] - 1s 76us/step - loss: 0.6357 - acc: 0.8716 - val_loss: 0.9641 - val_acc: 0.8050 Epoch 7/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.4960 - acc: 0.8990 - val_loss: 0.9275 - val_acc: 0.8050 Epoch 8/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.3909 - acc: 0.9197 - val_loss: 0.8983 - val_acc: 0.8100 Epoch 9/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.3116 - acc: 0.9346 - val_loss: 0.8843 - val_acc: 0.8190 1show_graph(history) 예측하는 방법은 아까와 동일하고,출력된 레이블 또한 softmax처럼 각 클래스 별 확률이 나오게 됩니다.9번의 에폭에서 과대적합 되던걸 확인하여 이번 학습은 최대 에폭을 9로 설정하여 학습을 진행한 내용의 그래프입니다. 추가 개선사항중간 레이어의 유닛수가 너무 작게 되면 데이터에 대한 손실이 발생할 수 있지만,데이터를 압축하여 노이즈를 줄이는 효과를 얻을 수도 있고, 반대로 유닛수를 크게 한다면,해당 레이블을 표현하기 위한 데이터를 더 넣을 수 있게 된다고 볼 수도 있습니다.이러한 내용을 잘 숙지하여 레이어 구성을 변경해가며 테스트를 진행해봅시다.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"}]},{"title":"딥러닝으로 영화 리뷰의 긍정/부정 이진 분류 모델 만들기","slug":"imdb","date":"2019-01-10T16:55:22.000Z","updated":"2019-01-29T18:37:56.098Z","comments":true,"path":"2019/01/11/imdb/","link":"","permalink":"https://codingcrews.github.io/2019/01/11/imdb/","excerpt":"","text":"IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification머신러닝은 크게 분류와 회귀 문제로 나누어 지게 됩니다.저희는 분류 문제에서도 이진분류를 진행해볼건데요.이진분류라는건 예를들어 메일을 받았을 때 이 메일이 스팸이냐 아니냐, 말을 했을 때 욕이냐 아니냐와 같이 문제에 대해 예/아니오 형태로 구분되는 문제에 적합합니다. 이번에는 IMDB 데이터셋을 이용해 해당 리뷰가 긍정적인지 부정적인지 예측하는 이진 분류 모델을 만들어 보겠습니다. DatasetIMDB 데이터셋이란 영화에 대한 평점과 리뷰들이 들어가있는 데이터인데요, (영어로 되어 있습니다.) 추후에는 한글로 된 데이터셋을 직접 구하고 만들어서 분류기를 만들어보시면 재밌지 않을까요? Download데이터셋은 텐서플로의 패키지 안에 다운로더 및 로더가 내장되어 있습니다.(포스팅 시각 기준으로 저는 r1.12 버전의 텐서플로 패키지를 사용중이며, 너무 오래된 버전의 경우 keras 패키지가 내장되어 있지 않을 수 있습니다.) 1from tensorflow.keras.datasets import imdb 1(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_labels.shape)) Train Datas Shape : (25000,) Train Labels Shape : (25000,) 데이터 확인25000개의 훈련용 데이터셋이 존재하며, 각 인덱스는 단어 인덱스의 리스트를 가지고 있습니다. 12display(train_data[0][:10])display(train_labels) [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65] array([1, 0, 0, ..., 0, 1, 0]) 단어 인덱스를 단어로 치환1234word_index = imdb.get_word_index()indexes = dict([(value, key) for (key, value) in word_index.items()])decoded_review = ' '.join(indexes.get(i - 3, '?') for i in train_data[0])print(decoded_review) ? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all 데이터 변환단어의 개수를 10,000개로 지정해두었고, 이 단어 인덱스를 원핫인코딩으로 변환하여 10,000차원의 벡터로 변경시키도록 합니다. 1234567import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성 for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 12345x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)display(x_train.shape)display(x_test.shape) (25000, 10000) (25000, 10000) 12345y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32')display(y_train)display(y_test) array([1., 0., 0., ..., 0., 1., 0.], dtype=float32) array([0., 1., 1., ..., 0., 0., 0.], dtype=float32) 신경망 구성신경망 네트워크 구축12from tensorflow.keras import modelsfrom tensorflow.keras import layers 123456model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 160016 _________________________________________________________________ dense_1 (Dense) (None, 16) 272 _________________________________________________________________ dense_2 (Dense) (None, 1) 17 ================================================================= Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 _________________________________________________________________ 모델 컴파일모델을 사용하기 위해선 네트워크를 구성한 모델을 컴파일하는 과정이 필요합니다.rmsprop 옵티마이저를 사용하고,확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택인데,이진 분류로 각 확률을 구하는 모델이니 binary crossentropy를 사용합니다. 123456789101112from tensorflow.keras import optimizersfrom tensorflow.keras import metricsfrom tensorflow.keras import lossesmodel.compile(# optimizer='rmsprop', optimizer=optimizers.RMSprop(lr=0.001),# loss='binary_crossentropy', loss=losses.binary_crossentropy,# metrics=['accuracy'] metrics=[metrics.binary_accuracy]) 검증 데이터 준비 (Validation)훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해원본 훈련 데이터에서 10,000개의 샘플을 떼내어 검증 데이터 세트를 만들겠습니다. 1234x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:] 모델 학습512의 샘플씩 미니배치를 만들어 20번의 에폭동안 훈련시키고,앞에서 떼어놓은 10,000개의 데이터를 이용해 손실과 정확도를 측정하겠습니다 1234567history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/20 15000/15000 [==============================] - 2s 139us/step - loss: 0.5883 - binary_accuracy: 0.7147 - val_loss: 0.5165 - val_binary_accuracy: 0.7686 Epoch 2/20 15000/15000 [==============================] - 1s 90us/step - loss: 0.4284 - binary_accuracy: 0.8771 - val_loss: 0.3974 - val_binary_accuracy: 0.8760 Epoch 3/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.3157 - binary_accuracy: 0.9189 - val_loss: 0.3314 - val_binary_accuracy: 0.8880 Epoch 4/20 15000/15000 [==============================] - 1s 86us/step - loss: 0.2407 - binary_accuracy: 0.9349 - val_loss: 0.2963 - val_binary_accuracy: 0.8893 Epoch 5/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1912 - binary_accuracy: 0.9473 - val_loss: 0.2806 - val_binary_accuracy: 0.8906 Epoch 6/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1587 - binary_accuracy: 0.9555 - val_loss: 0.2811 - val_binary_accuracy: 0.8880 Epoch 7/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1303 - binary_accuracy: 0.9654 - val_loss: 0.2927 - val_binary_accuracy: 0.8850 Epoch 8/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.1108 - binary_accuracy: 0.9707 - val_loss: 0.2976 - val_binary_accuracy: 0.8859 Epoch 9/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0930 - binary_accuracy: 0.9762 - val_loss: 0.3311 - val_binary_accuracy: 0.8771 Epoch 10/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0795 - binary_accuracy: 0.9802 - val_loss: 0.3353 - val_binary_accuracy: 0.8808 Epoch 11/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0652 - binary_accuracy: 0.9855 - val_loss: 0.3555 - val_binary_accuracy: 0.8786 Epoch 12/20 15000/15000 [==============================] - 1s 88us/step - loss: 0.0556 - binary_accuracy: 0.9881 - val_loss: 0.3749 - val_binary_accuracy: 0.8768 Epoch 13/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0442 - binary_accuracy: 0.9919 - val_loss: 0.4263 - val_binary_accuracy: 0.8694 Epoch 14/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0373 - binary_accuracy: 0.9934 - val_loss: 0.4213 - val_binary_accuracy: 0.8753 Epoch 15/20 15000/15000 [==============================] - 1s 90us/step - loss: 0.0305 - binary_accuracy: 0.9947 - val_loss: 0.4821 - val_binary_accuracy: 0.8657 Epoch 16/20 15000/15000 [==============================] - 1s 92us/step - loss: 0.0263 - binary_accuracy: 0.9955 - val_loss: 0.4871 - val_binary_accuracy: 0.8704 Epoch 17/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0178 - binary_accuracy: 0.9978 - val_loss: 0.5176 - val_binary_accuracy: 0.8693 Epoch 18/20 15000/15000 [==============================] - 1s 88us/step - loss: 0.0155 - binary_accuracy: 0.9982 - val_loss: 0.5890 - val_binary_accuracy: 0.8639 Epoch 19/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0136 - binary_accuracy: 0.9980 - val_loss: 0.5645 - val_binary_accuracy: 0.8679 Epoch 20/20 15000/15000 [==============================] - 1s 89us/step - loss: 0.0086 - binary_accuracy: 0.9990 - val_loss: 0.5974 - val_binary_accuracy: 0.8672 모델의 훈련 정보 그리기위에서 fit의 반환으로 받은 history는 각각의 훈련 데이터세트와 검증 데이터세트에 대한 매 에폭마다의 손실율과 정확도를 가지고 있습니다.해당 지표를 matplot를 이용해 시각화 하도록 해보겠습니다. 123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltdef show_graph(history): history_dict = history.history accuracy = history_dict['binary_accuracy'] val_accuracy = history_dict['val_binary_accuracy'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy and loss') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history) 훈련 데이터셋의 그래프를(점선) 먼저 확인해보면,각 에폭이 돌때마다 정확도가 오르고, 손실은 줄어드는 형태로 제대로 학습이 된것 처럼 보이나, 검증 데이터셋(실선)을 보게 되면 그렇지 않습니다.각 에폭마다도 정확도는 오르지 않고, 손실이 늘어나는걸 확인할 수 있는데, 이런 경우 훈련 데이터셋에 과대적합(overfitting) 되었다고 합니다.과대적합이 된 경우 모델이 새로운 데이터셋을 만났을 때 제대로 분류를 하지 못하게 됩니다. 모델 재학습하기아까와 동일한 형태의 모델을 구성하고,학습과 관련된 하이퍼파라미터만 변경하여 과대적합을 피해보겠습니다. 123456model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 16) 160016 _________________________________________________________________ dense_4 (Dense) (None, 16) 272 _________________________________________________________________ dense_5 (Dense) (None, 1) 17 ================================================================= Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 _________________________________________________________________ 12345model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 111us/step - loss: 0.5347 - binary_accuracy: 0.7908 - val_loss: 0.4127 - val_binary_accuracy: 0.8663 Epoch 2/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.3256 - binary_accuracy: 0.9005 - val_loss: 0.3316 - val_binary_accuracy: 0.8697 Epoch 3/8 15000/15000 [==============================] - 1s 88us/step - loss: 0.2393 - binary_accuracy: 0.9232 - val_loss: 0.2822 - val_binary_accuracy: 0.8906 Epoch 4/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.1881 - binary_accuracy: 0.9410 - val_loss: 0.2801 - val_binary_accuracy: 0.8875 Epoch 5/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.1525 - binary_accuracy: 0.9524 - val_loss: 0.2770 - val_binary_accuracy: 0.8885 Epoch 6/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.1263 - binary_accuracy: 0.9611 - val_loss: 0.2856 - val_binary_accuracy: 0.8880 Epoch 7/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.1035 - binary_accuracy: 0.9701 - val_loss: 0.3127 - val_binary_accuracy: 0.8846 Epoch 8/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.0863 - binary_accuracy: 0.9754 - val_loss: 0.3270 - val_binary_accuracy: 0.8835 1show_graph(history) 아까보단 모델이 상대적으로 과대적합 되지 않았습니다.손실 그래프를 확인했을 때 2에폭과 3에폭이 훈련 세트와 검증 세트가 가장 근접한 손실을 갖고있는걸 확인할 수 있고,정확도 또한 2,3에폭이 가장 근접한걸 확인할 수 있습니다.즉, 이 모델의 경우 2에폭 혹은 3에폭을 돌렸을 때 과대적합을 가장 피할 수 있는 학습상태가 된다는걸 확인할 수 있습니다.이렇게 학습에 파라미터를 조작하는 것 이외에도 과대적합을 피하는 기법이 많이 존재합니다. 모델의 평가모델의 정확도를 측정합니다. 12loss, accuracy = model.evaluate(x_test, y_test)print('accuracy : &#123;acc&#125;, loss : &#123;loss&#125;'.format(acc=accuracy, loss=loss)) 25000/25000 [==============================] - 2s 67us/step accuracy : 0.86852, loss : 0.35010315059185027 모델의 예측긍정이거나 부정일 확률 (높으면 긍정, 낮으면 부정) 1model.predict(x_test[:10]) array([[0.2251153 ], [0.9999784 ], [0.98094064], [0.94734573], [0.97099954], [0.9737046 ], [0.9995834 ], [0.01185756], [0.9645392 ], [0.99970514]], dtype=float32) 번외. 레이어 변경하여 정확도 개선해보기레이어를 한개 더 추가하여 테스트 (Deep)12345678910111213model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 16) 160016 _________________________________________________________________ dense_7 (Dense) (None, 16) 272 _________________________________________________________________ dense_8 (Dense) (None, 16) 272 _________________________________________________________________ dense_9 (Dense) (None, 1) 17 ================================================================= Total params: 160,577 Trainable params: 160,577 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 113us/step - loss: 0.5264 - binary_accuracy: 0.7816 - val_loss: 0.4348 - val_binary_accuracy: 0.8122 Epoch 2/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.2989 - binary_accuracy: 0.9006 - val_loss: 0.2936 - val_binary_accuracy: 0.8870 Epoch 3/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.2103 - binary_accuracy: 0.9263 - val_loss: 0.2941 - val_binary_accuracy: 0.8812 Epoch 4/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.1550 - binary_accuracy: 0.9481 - val_loss: 0.2963 - val_binary_accuracy: 0.8817 Epoch 5/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.1294 - binary_accuracy: 0.9543 - val_loss: 0.2956 - val_binary_accuracy: 0.8850 Epoch 6/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.0964 - binary_accuracy: 0.9709 - val_loss: 0.3357 - val_binary_accuracy: 0.8745 Epoch 7/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.0814 - binary_accuracy: 0.9739 - val_loss: 0.3678 - val_binary_accuracy: 0.8714 Epoch 8/8 15000/15000 [==============================] - 1s 84us/step - loss: 0.0598 - binary_accuracy: 0.9834 - val_loss: 0.3910 - val_binary_accuracy: 0.8714 1show_graph(history) 유닛을 추가하여 테스트 (Wide)123456789101112model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 64) 640064 _________________________________________________________________ dense_11 (Dense) (None, 64) 4160 _________________________________________________________________ dense_12 (Dense) (None, 1) 65 ================================================================= Total params: 644,289 Trainable params: 644,289 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 135us/step - loss: 0.4850 - binary_accuracy: 0.7656 - val_loss: 0.3620 - val_binary_accuracy: 0.8517 Epoch 2/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.2538 - binary_accuracy: 0.9058 - val_loss: 0.2754 - val_binary_accuracy: 0.8902 Epoch 3/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.1857 - binary_accuracy: 0.9340 - val_loss: 0.2826 - val_binary_accuracy: 0.8874 Epoch 4/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.1417 - binary_accuracy: 0.9499 - val_loss: 0.3328 - val_binary_accuracy: 0.8734 Epoch 5/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.1128 - binary_accuracy: 0.9601 - val_loss: 0.3275 - val_binary_accuracy: 0.8826 Epoch 6/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.0795 - binary_accuracy: 0.9743 - val_loss: 0.3473 - val_binary_accuracy: 0.8802 Epoch 7/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.0544 - binary_accuracy: 0.9832 - val_loss: 0.3840 - val_binary_accuracy: 0.8780 Epoch 8/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0514 - binary_accuracy: 0.9849 - val_loss: 0.4150 - val_binary_accuracy: 0.8789 1show_graph(history) 깊고 넓게 구성하기 (Deep and wide network)1234567891011121314model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_13 (Dense) (None, 64) 640064 _________________________________________________________________ dense_14 (Dense) (None, 64) 4160 _________________________________________________________________ dense_15 (Dense) (None, 32) 2080 _________________________________________________________________ dense_16 (Dense) (None, 32) 1056 _________________________________________________________________ dense_17 (Dense) (None, 1) 33 ================================================================= Total params: 647,393 Trainable params: 647,393 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 142us/step - loss: 0.5094 - binary_accuracy: 0.7512 - val_loss: 0.3875 - val_binary_accuracy: 0.8442 Epoch 2/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.2693 - binary_accuracy: 0.8983 - val_loss: 0.4223 - val_binary_accuracy: 0.8310 Epoch 3/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.1949 - binary_accuracy: 0.9275 - val_loss: 0.5629 - val_binary_accuracy: 0.7950 Epoch 4/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.1534 - binary_accuracy: 0.9434 - val_loss: 0.2965 - val_binary_accuracy: 0.8854 Epoch 5/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.1106 - binary_accuracy: 0.9613 - val_loss: 0.3647 - val_binary_accuracy: 0.8718 Epoch 6/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.0797 - binary_accuracy: 0.9733 - val_loss: 0.4042 - val_binary_accuracy: 0.8748 Epoch 7/8 15000/15000 [==============================] - 2s 110us/step - loss: 0.0802 - binary_accuracy: 0.9775 - val_loss: 0.4029 - val_binary_accuracy: 0.8815 Epoch 8/8 15000/15000 [==============================] - 2s 106us/step - loss: 0.0656 - binary_accuracy: 0.9827 - val_loss: 0.4207 - val_binary_accuracy: 0.8809 1show_graph(history) 손실함수 변경1234567891011121314model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.MSE, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 64) 640064 _________________________________________________________________ dense_19 (Dense) (None, 64) 4160 _________________________________________________________________ dense_20 (Dense) (None, 32) 2080 _________________________________________________________________ dense_21 (Dense) (None, 32) 1056 _________________________________________________________________ dense_22 (Dense) (None, 1) 33 ================================================================= Total params: 647,393 Trainable params: 647,393 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 138us/step - loss: 0.1701 - binary_accuracy: 0.7462 - val_loss: 0.1152 - val_binary_accuracy: 0.8508 Epoch 2/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.0794 - binary_accuracy: 0.8991 - val_loss: 0.0829 - val_binary_accuracy: 0.8917 Epoch 3/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0580 - binary_accuracy: 0.9281 - val_loss: 0.0892 - val_binary_accuracy: 0.8833 Epoch 4/8 15000/15000 [==============================] - 2s 111us/step - loss: 0.0375 - binary_accuracy: 0.9544 - val_loss: 0.0858 - val_binary_accuracy: 0.8876 Epoch 5/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0359 - binary_accuracy: 0.9560 - val_loss: 0.0874 - val_binary_accuracy: 0.8863 Epoch 6/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0216 - binary_accuracy: 0.9751 - val_loss: 0.0927 - val_binary_accuracy: 0.8822 Epoch 7/8 15000/15000 [==============================] - 2s 109us/step - loss: 0.0189 - binary_accuracy: 0.9782 - val_loss: 0.0946 - val_binary_accuracy: 0.8812 Epoch 8/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0068 - binary_accuracy: 0.9931 - val_loss: 0.1091 - val_binary_accuracy: 0.8678 1show_graph(history)","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"},{"name":"딥러닝으로 시리즈","slug":"딥러닝으로-시리즈","permalink":"https://codingcrews.github.io/tags/딥러닝으로-시리즈/"}]}]}