{"pages":[{"title":"About","date":"2019-01-10T06:28:13.000Z","updated":"2019-01-11T06:16:46.403Z","comments":true,"path":"about/index.html","permalink":"https://codingcrews.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-01-10T09:19:08.000Z","updated":"2019-01-10T09:19:31.122Z","comments":true,"path":"categories/index.html","permalink":"https://codingcrews.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-01-10T09:18:04.000Z","updated":"2019-01-10T09:18:25.185Z","comments":true,"path":"tags/index.html","permalink":"https://codingcrews.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"보스턴 집값 데이터를 이용한 선형 회귀","slug":"boston_linear_regression","date":"2019-01-14T18:06:54.000Z","updated":"2019-01-14T18:09:34.885Z","comments":true,"path":"2019/01/15/boston_linear_regression/","link":"","permalink":"https://codingcrews.github.io/2019/01/15/boston_linear_regression/","excerpt":"","text":"선형 회귀 문제 : Linear Regression머신러닝으로 해결할 수 있는 문제 중 분류 문제들을 이전 포스팅에서 다뤄보았고,이번에는 연속된 데이터를 예측해야하는 회귀 문제를 보겠다. 회귀 문제란 예를 들어 주택에 대한 주위 범죄율, 세율 등의 데이터가 주어졌을 때 주택 가격을 예측하는것처럼연속된 값을 예측하는걸 회귀 문제라고 한다. Dataset사용하는 데이터셋은 카네기 멜런 대학교에서 제공하는 데이터셋으로 데이터의 값에 해당하는 설명은 여기에서 확인할 수 있다. Download12from tensorflow.keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_targets.shape)) Train Datas Shape : (404, 13) Train Labels Shape : (404,) 데이터 확인각 컬럼별로 해당하는 데이터들을 확인할 수 있고, 레이블에는 해당 주택의 가격값이 들어가있는걸 확인할 수 있다. 12display(train_data[0])display(train_targets[0:10]) array([ 1.23247, 0. , 8.14 , 0. , 0.538 , 6.142 , 91.7 , 3.9769 , 4. , 307. , 21. , 396.9 , 18.72 ]) array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4]) 데이터 변환신경망 학습 시 차이가 큰 값을 신경망에 주입하면 학습이 잘 되지 않는다.(0.01, 0.1)과 (1, 10)을 보면 비율대로만 보게되면 10배의 차이를 가지는걸 볼 수 있지만,실수 체계에서 값의 차이는 어마어마하게 큰 차이로 볼 수 있다.(1, 10)과 같은 데이터를 Sparse 하다라고 표현하며,(0.01, 0.1)과 같은 밀도가 높은 데이터를 Dense 데이터라고 표현한다. 이렇게 Dense 데이터로 변경하는 과정이 필요한데, 이 부분을 Scaling 혹은 정규화 한다고 한다. 데이터의 정규화를 위해서 각 특성별 평균값을 뺀 이후에 표준편차로 나눠주게 되면,특성의 중앙이 0에 가깝게 맞춰지고, 표준편차는 1로 된다. 1234567mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std 모델 구성데이터셋이 작으므로 간단한 모델을 구성하는게 과대적합을 피하기가 손쉽다. 12345678910from tensorflow.keras import modelsfrom tensorflow.keras import layersdef build_network(input_shape=(0,)): model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=input_shape)) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='adam', loss='mse', metrics=['mae']) return model 모델 구성의 마지막 Dense layer를 보면 활성함수가 지정 되어있지 않은걸 확인할 수 있는데,우리가 필요한 데이터는 선형 데이터를 원하기 때문에 활성함수를 지정하지 않으면 스칼라 회귀를 위한 구성이 완성된다.반대로 sigmoid 같은 활성화 함수를 적용하면 네트워크가 0~1 사이의 값을 만들도록 될 것이다. 우리가 필요한 데이터는 주택가격을 예측하기 위함으로 활성함수를 지정하지 않고 스칼라 회귀 값을 예측하도록 구성한다. K-Fold Validation데이터셋의 크기가 매우 작기 때문에 검증셋(Validation)의 크기도 매우 작아지게 되는데,이때 데이터셋이 적어지면서 훈련세트 중 어느 한 특정 부분이 훈련세트로 사용되는지에 따라 모델의 정확도가 크게 달라질 수 있다.이 이유는 훈련세트와 검증세트로 나눴을 때 각 값들의 분포도가 고르게 되지 못할 경우가 생기기 때문이다. 이런 상황에서 가장 좋은 방법은 K-겹 교차 검증(K-Fold Cross Validation)을 실시하는것인데,데이터를 K개의 분할로 나누고 각각 K개의 모델을 만들어 K-1 개의 분할에서 훈련하고 나머지 분할에서 평가를 하는법이다.이 점수는 각 검증 데이터셋의 평균으로 모델을 평가하게 된다. 즉, 여러 폴드의 데이터셋으로 나누어 교차검증을 함으로써 데이터 분포에 신경쓰지 않고 훈련세트의 모든 부분을 사용해 모델을 평가할 수 있는 장점이 있다. 12345678910111213141516171819202122232425262728293031323334353637import numpy as npk = 4num_val_samples = len(train_data) // knum_epochs = 150all_scores = []all_history = []for i in range(k): print('폴드 번호 #&#123;&#125;'.format(i)) fold_start_index = i * num_val_samples fold_end_index = (i + 1) * num_val_samples val_data = train_data[fold_start_index : fold_end_index] val_targets = train_targets[fold_start_index : fold_end_index] partial_train_data = np.concatenate( [train_data[:fold_start_index], train_data[fold_end_index:]], axis=0 ) partial_train_targets = np.concatenate( [train_targets[:fold_start_index], train_targets[fold_end_index:]], axis=0 ) model = build_network((partial_train_data.shape[1], )) history = model.fit( partial_train_data, partial_train_targets, epochs=num_epochs, validation_data=(val_data, val_targets), batch_size=1, verbose=0 ) val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0) all_scores.append(val_mse) all_history.append(history.history) 폴드 번호 #0 폴드 번호 #1 폴드 번호 #2 폴드 번호 #3 12345val_mae_lst = map(lambda x: x['val_mean_absolute_error'], all_history)val_mae_lst = np.array(list(val_mae_lst))avg_mae = [ np.mean([x[i] for x in val_mae_lst]) for i in range(num_epochs)] 검증 데이터 시각화검증 데이터 시각화를 위해 history 데이터를 에폭별 평균 데이터로 치환시킨다. 12345val_mae_lst = map(lambda x: x['val_mean_absolute_error'], all_history)val_mae_lst = np.array(list(val_mae_lst))avg_mae = [ np.mean([x[i] for x in val_mae_lst]) for i in range(num_epochs)] 그래프 변동이 심하지 않도록 이전 포인트의 지수 이동 평균값으로 변경시켜주는 함수를 만든다 12345678910import matplotlib.pyplot as pltdef smooth_curve(points, factor=.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 123456def show_graph(data): smooth_data = smooth_curve(data) plt.plot(range(1, len(smooth_data) + 1), smooth_data) plt.xlabel('Epochs') plt.ylabel('Validation MAE') plt.show() 1show_graph(avg_mae[10:])","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"}]},{"title":"로이터 데이터셋을 이용한 뉴스 기사 다중 분류","slug":"reuter_multi_classification","date":"2019-01-14T14:27:59.000Z","updated":"2019-01-14T14:29:37.993Z","comments":true,"path":"2019/01/14/reuter_multi_classification/","link":"","permalink":"https://codingcrews.github.io/2019/01/14/reuter_multi_classification/","excerpt":"","text":"다중 분류 문제 : Multiple class classification이번 포스팅에서는 2개 이상의 클래스를 가진 경우 사용할 수 있는 다중 분류 문제를 해결해보도록 한다. Dataset로이터 데이터셋을 사용하도록 한다.데이터셋은 46개의 토픽을 갖고 있으며 각 뉴스기사마다 하나의 토픽이 정해져있다.즉, 단일 레이블 다중분류 문제로 볼 수 있다. Download12from tensorflow.keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_labels.shape)) Train Datas Shape : (8982,) Train Labels Shape : (8982,) 데이터 확인확인한대로 8982개의 훈련 데이터셋이 존재하며,각 인덱스는 단어 인덱스의 리스트를 뜻 한다. 1234display(train_data[0][:10])display(train_labels)display(test_labels) [1, 2, 2, 8, 43, 10, 447, 5, 25, 207] array([ 3, 4, 3, ..., 25, 3, 25]) array([ 3, 10, 1, ..., 3, 3, 24]) 단어를 텍스트로 디코딩1234word_index = reuters.get_word_index()reverse_word_index = dict([(val, key) for (key, val) in word_index.items()])decoded_news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])print(decoded_news) ? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3 라벨 확인샘플의 라벨은 토픽의 인덱스로써 0~45의 값을 가진다. 1train_labels[0] 3 데이터 변환학습에 용이하도록 뉴스 기사와 라벨 데이터를 벡터로 변환시킨다.학습에 사용되는 데이터셋의 인풋 데이터는 해당 뉴스 기사의 들어가있는 단어 인덱스를 1.0 으로 변경시킨다. 1234567import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성 for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 12345678910111213from tensorflow.keras.utils import to_categoricalx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)display(x_train.shape)display(x_test.shape)one_hot_train_labels = to_categorical(train_labels)one_hot_test_labels = to_categorical(test_labels)display(one_hot_train_labels.shape)display(one_hot_test_labels.shape) (8982, 10000) (2246, 10000) (8982, 46) (2246, 46) 신경망 구성단일 레이블 다중 분류를 위한 모델을 작성 해보도록 하겠다.다만 레이어 구축 시 참고해야 할 부분이 있는데,각 레이어를 통과 할 때 유닛의 수가 레이블 보다 적다면 가지고 있어야 할 정보가 많이 사라질 수 있다. 학습 시 파라미터를 줄이기 위해서나 노이즈를 줄이기 위해서 유닛을 줄이는 경우도 있지만,데이터가 가지고 있어야 할 필수 데이터를 잃어버릴 수 있다는 점도 참고하여 네트워크를 구성해야 한다. 신경망 네트워크 구축로이터 데이터셋의 단어수를 10,000개로 제한해두었으니,신경망의 입력 차원수도 10,000으로 설정한다. 123456789from tensorflow.keras import modelsfrom tensorflow.keras import layersmodel = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 64) 640064 _________________________________________________________________ dense_4 (Dense) (None, 64) 4160 _________________________________________________________________ dense_5 (Dense) (None, 46) 2990 ================================================================= Total params: 647,214 Trainable params: 647,214 Non-trainable params: 0 _________________________________________________________________ 이진분류와 비슷하게 마지막 Dense 레이어의 아웃풋 벡터 개수는(46개) 예측하기 위한 클래스의 개수와 동일합니다.다른점은 이진 분류에서는 활성함수로 sigmoid를 사용한 반면 여기서는 softmax를 이용했는데,softmax는 각 클래스별로 해당 클래스 일 확률을 표시하도록 만들어집니다. 모델 컴파일다중 분류에서 손실함수로써는 categorical_crossentropy를 주로 사용합니다.옵티마이저는 가장 빠르고 효과가 좋다고 알려진 adam 옵티마이저를 사용하도록 설정합니다. 12345model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],) 검증 데이터셋 구성훈련용 데이터셋에서 Validation으로 사용할 데이터셋을 분리시킨다. 12345x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = one_hot_train_labels[:1000]partial_y_train = one_hot_train_labels[1000:] 모델 학습1234567history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val)) Train on 7982 samples, validate on 1000 samples Epoch 1/20 7982/7982 [==============================] - 1s 152us/step - loss: 3.3032 - acc: 0.4328 - val_loss: 2.6156 - val_acc: 0.5320 Epoch 2/20 7982/7982 [==============================] - 1s 78us/step - loss: 2.0615 - acc: 0.6076 - val_loss: 1.6695 - val_acc: 0.6460 Epoch 3/20 7982/7982 [==============================] - 1s 78us/step - loss: 1.3844 - acc: 0.7051 - val_loss: 1.3219 - val_acc: 0.7100 Epoch 4/20 7982/7982 [==============================] - 1s 77us/step - loss: 1.0796 - acc: 0.7669 - val_loss: 1.1773 - val_acc: 0.7520 Epoch 5/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.8673 - acc: 0.8132 - val_loss: 1.0768 - val_acc: 0.7790 Epoch 6/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.6909 - acc: 0.8515 - val_loss: 0.9995 - val_acc: 0.7910 Epoch 7/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.5410 - acc: 0.8880 - val_loss: 0.9467 - val_acc: 0.8010 Epoch 8/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.4177 - acc: 0.9137 - val_loss: 0.9069 - val_acc: 0.8190 Epoch 9/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.3253 - acc: 0.9300 - val_loss: 0.8855 - val_acc: 0.8160 Epoch 10/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.2593 - acc: 0.9432 - val_loss: 0.8973 - val_acc: 0.8090 Epoch 11/20 7982/7982 [==============================] - 1s 80us/step - loss: 0.2123 - acc: 0.9503 - val_loss: 0.8912 - val_acc: 0.8210 Epoch 12/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1792 - acc: 0.9549 - val_loss: 0.9012 - val_acc: 0.8230 Epoch 13/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1594 - acc: 0.9565 - val_loss: 0.9194 - val_acc: 0.8170 Epoch 14/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1410 - acc: 0.9573 - val_loss: 0.9531 - val_acc: 0.8150 Epoch 15/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.1294 - acc: 0.9578 - val_loss: 0.9501 - val_acc: 0.8160 Epoch 16/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1192 - acc: 0.9603 - val_loss: 0.9757 - val_acc: 0.8130 Epoch 17/20 7982/7982 [==============================] - 1s 77us/step - loss: 0.1146 - acc: 0.9592 - val_loss: 0.9701 - val_acc: 0.8100 Epoch 18/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.1058 - acc: 0.9600 - val_loss: 0.9883 - val_acc: 0.8080 Epoch 19/20 7982/7982 [==============================] - 1s 80us/step - loss: 0.1055 - acc: 0.9624 - val_loss: 1.0214 - val_acc: 0.8130 Epoch 20/20 7982/7982 [==============================] - 1s 81us/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.9970 - val_acc: 0.8150 훈련의 정확도와 손실 시각화이전 이진분류 포스팅에서 사용했던 함수를 그대로 끌어와 사용하도록 한다. 123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltdef show_graph(history): history_dict = history.history accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history) 그래프를 보면 대략 8~9번째 에폭부터 과대적합이 시작되는걸 알 수 있습니다. 새로운 데이터 예측해보기모델을 이용해 각 뉴스기사에 대한 토픽을 예측해보도록 합니다.softmax를 사용하여 각 뉴스별로 46개의 토픽에 해당하는 확률을 출력합니다.각 클래스 별 확률을 모두 더하면 1.0(100%)가 됩니다. 12predictions = model.predict(x_test)display(predictions.shape) (2246, 46) predictions는 테스트 데이터셋의 개수에 맞게 2246개의 결과가 들어있습니다.각 결과안에는 46개 클래스에 해당하는 확률값이 들어가있습니다. 1np.sum(predictions[0]) 1.0 각 46개의 모든 원소의 값을 모두 더하면 1.0(100%)가 된걸 확인할 수 있습니다. 12display(np.argmax(predictions[0]))display(predictions[0][3]) 3 0.96262544 predictions[0]의 3번째 인덱스가 가장 큰 값을 가진걸 확인하였고,해당 인덱스의 값을 확인하였더니 0.96262544(96.262544%)로 모델이 예측한걸 확인할 수 있습니다. 데이터 레이블 인코딩 방식 변경하여 학습하기one hot 인코딩이 아닌 정수형으로 토픽을 예측하도록 레이블 인코딩을 사용하도록 변경해본다.손실함수를 변경해주면 되는데,categorical_crossentropy는 범주형 인코딩일 시 사용하는 손실함수이고,정수형을 사용할 때에는 sparse_categorical_crossentropy를 사용한다. 12345678y_train = np.array(train_labels)y_test = np.array(test_labels)x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:] 123456model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 64) 640064 _________________________________________________________________ dense_7 (Dense) (None, 64) 4160 _________________________________________________________________ dense_8 (Dense) (None, 46) 2990 ================================================================= Total params: 647,214 Trainable params: 647,214 Non-trainable params: 0 _________________________________________________________________ 12345model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) 1234567history = model.fit( partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val)) Train on 7982 samples, validate on 1000 samples Epoch 1/9 7982/7982 [==============================] - 1s 116us/step - loss: 3.3313 - acc: 0.3983 - val_loss: 2.5661 - val_acc: 0.5770 Epoch 2/9 7982/7982 [==============================] - 1s 78us/step - loss: 2.0006 - acc: 0.6272 - val_loss: 1.6576 - val_acc: 0.6580 Epoch 3/9 7982/7982 [==============================] - 1s 78us/step - loss: 1.3389 - acc: 0.7134 - val_loss: 1.2907 - val_acc: 0.7090 Epoch 4/9 7982/7982 [==============================] - 1s 78us/step - loss: 1.0256 - acc: 0.7762 - val_loss: 1.1484 - val_acc: 0.7630 Epoch 5/9 7982/7982 [==============================] - 1s 80us/step - loss: 0.8077 - acc: 0.8339 - val_loss: 1.0422 - val_acc: 0.7870 Epoch 6/9 7982/7982 [==============================] - 1s 76us/step - loss: 0.6357 - acc: 0.8716 - val_loss: 0.9641 - val_acc: 0.8050 Epoch 7/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.4960 - acc: 0.8990 - val_loss: 0.9275 - val_acc: 0.8050 Epoch 8/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.3909 - acc: 0.9197 - val_loss: 0.8983 - val_acc: 0.8100 Epoch 9/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.3116 - acc: 0.9346 - val_loss: 0.8843 - val_acc: 0.8190 1show_graph(history) 예측하는 방법은 아까와 동일하고,출력된 레이블 또한 softmax처럼 각 클래스 별 확률이 나오게 된다.9번의 에폭에서 과대적합 되던걸 확인하여 이번 학습은 최대 에폭을 9로 설정하여 학습을 진행한 내용의 그래프이다. 추가 개선사항중간 레이어의 유닛수가 너무 작게 되면 데이터에 대한 손실이 발생할 수 있지만,데이터를 압축하여 노이즈를 줄이는 효과를 얻을 수도 있고, 반대로 유닛수를 크게 한다면,해당 레이블을 표현하기 위한 데이터를 더 넣을 수 있게 된다고 볼 수도 있다.이러한 내용을 잘 숙지하여 레이어 구성을 변경해가며 테스트를 진행해보자.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"}]},{"title":"IMDB 데이터셋을 이용한 영화 리뷰 분류","slug":"imdb","date":"2019-01-10T16:55:22.000Z","updated":"2019-01-10T16:55:57.810Z","comments":true,"path":"2019/01/11/imdb/","link":"","permalink":"https://codingcrews.github.io/2019/01/11/imdb/","excerpt":"","text":"IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification영화 평점 데이터를 통한 이진 분류 DatasetDownload1from tensorflow.keras.datasets import imdb 1(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_labels.shape)) Train Datas Shape : (25000,) Train Labels Shape : (25000,) 데이터 확인25000개의 훈련용 데이터셋이 존재하며, 각 인덱스는 단어 인덱스의 리스트 12display(train_data[0][:10])display(train_labels) [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65] array([1, 0, 0, ..., 0, 1, 0]) 단어 인덱스를 단어로 치환1234word_index = imdb.get_word_index()indexes = dict([(value, key) for (key, value) in word_index.items()])decoded_review = ' '.join(indexes.get(i - 3, '?') for i in train_data[0])print(decoded_review) ? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all 데이터 텐서 치환단어의 개수를 10,000개로 지정해두었고, 이 단어 인덱스를 원핫인코딩으로 변환하여10,000차원의 벡터로 변경시킴 1234567import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성 for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 12345x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)display(x_train.shape)display(x_test.shape) (25000, 10000) (25000, 10000) 12345y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32')display(y_train)display(y_test) array([1., 0., 0., ..., 0., 1., 0.], dtype=float32) array([0., 1., 1., ..., 0., 0., 0.], dtype=float32) 신경망 구성신경망 네트워크 구축12from tensorflow.keras import modelsfrom tensorflow.keras import layers 123456model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 160016 _________________________________________________________________ dense_1 (Dense) (None, 16) 272 _________________________________________________________________ dense_2 (Dense) (None, 1) 17 ================================================================= Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 _________________________________________________________________ 모델 컴파일모델을 사용하기 위해선 네트워크를 구성한 모델을 컴파일하는 과정이 필요하다.rmsprop 옵티마이저를 사용하고,확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택인데,이진 분류로 각 확률을 구하는 모델이니 binary crossentropy를 사용한다. 123456789101112from tensorflow.keras import optimizersfrom tensorflow.keras import metricsfrom tensorflow.keras import lossesmodel.compile(# optimizer='rmsprop', optimizer=optimizers.RMSprop(lr=0.001),# loss='binary_crossentropy', loss=losses.binary_crossentropy,# metrics=['accuracy'] metrics=[metrics.binary_accuracy]) 검증 데이터 준비 (Validation)훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해원본 훈련 데이터에서 10,000개의 샘플을 떼내어 검증 데이터 세트를 만듬 1234x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:] 모델 학습512의 샘플씩 미니배치를 만들어 20번의 에폭동안 훈련앞에서 떼어놓은 10,000개의 데이터를 이용해 손실과 정확도를 측정 1234567history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/20 15000/15000 [==============================] - 2s 139us/step - loss: 0.5883 - binary_accuracy: 0.7147 - val_loss: 0.5165 - val_binary_accuracy: 0.7686 Epoch 2/20 15000/15000 [==============================] - 1s 90us/step - loss: 0.4284 - binary_accuracy: 0.8771 - val_loss: 0.3974 - val_binary_accuracy: 0.8760 Epoch 3/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.3157 - binary_accuracy: 0.9189 - val_loss: 0.3314 - val_binary_accuracy: 0.8880 Epoch 4/20 15000/15000 [==============================] - 1s 86us/step - loss: 0.2407 - binary_accuracy: 0.9349 - val_loss: 0.2963 - val_binary_accuracy: 0.8893 Epoch 5/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1912 - binary_accuracy: 0.9473 - val_loss: 0.2806 - val_binary_accuracy: 0.8906 Epoch 6/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1587 - binary_accuracy: 0.9555 - val_loss: 0.2811 - val_binary_accuracy: 0.8880 Epoch 7/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1303 - binary_accuracy: 0.9654 - val_loss: 0.2927 - val_binary_accuracy: 0.8850 Epoch 8/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.1108 - binary_accuracy: 0.9707 - val_loss: 0.2976 - val_binary_accuracy: 0.8859 Epoch 9/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0930 - binary_accuracy: 0.9762 - val_loss: 0.3311 - val_binary_accuracy: 0.8771 Epoch 10/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0795 - binary_accuracy: 0.9802 - val_loss: 0.3353 - val_binary_accuracy: 0.8808 Epoch 11/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0652 - binary_accuracy: 0.9855 - val_loss: 0.3555 - val_binary_accuracy: 0.8786 Epoch 12/20 15000/15000 [==============================] - 1s 88us/step - loss: 0.0556 - binary_accuracy: 0.9881 - val_loss: 0.3749 - val_binary_accuracy: 0.8768 Epoch 13/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0442 - binary_accuracy: 0.9919 - val_loss: 0.4263 - val_binary_accuracy: 0.8694 Epoch 14/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0373 - binary_accuracy: 0.9934 - val_loss: 0.4213 - val_binary_accuracy: 0.8753 Epoch 15/20 15000/15000 [==============================] - 1s 90us/step - loss: 0.0305 - binary_accuracy: 0.9947 - val_loss: 0.4821 - val_binary_accuracy: 0.8657 Epoch 16/20 15000/15000 [==============================] - 1s 92us/step - loss: 0.0263 - binary_accuracy: 0.9955 - val_loss: 0.4871 - val_binary_accuracy: 0.8704 Epoch 17/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0178 - binary_accuracy: 0.9978 - val_loss: 0.5176 - val_binary_accuracy: 0.8693 Epoch 18/20 15000/15000 [==============================] - 1s 88us/step - loss: 0.0155 - binary_accuracy: 0.9982 - val_loss: 0.5890 - val_binary_accuracy: 0.8639 Epoch 19/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0136 - binary_accuracy: 0.9980 - val_loss: 0.5645 - val_binary_accuracy: 0.8679 Epoch 20/20 15000/15000 [==============================] - 1s 89us/step - loss: 0.0086 - binary_accuracy: 0.9990 - val_loss: 0.5974 - val_binary_accuracy: 0.8672 모델의 훈련 정보 그리기위에서 fit의 반환으로 받은 history는 각각의 훈련 데이터세트와 검증 데이터세트에 대한매 에폭마다의 손실율과 정확도를 가지고 있다.해당 지표를 matplot를 이용해 시각화 하도록 한다. 123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltdef show_graph(history): history_dict = history.history accuracy = history_dict['binary_accuracy'] val_accuracy = history_dict['val_binary_accuracy'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy and loss') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history) 훈련 데이터셋의 그래프를(점선) 먼저 확인해보면,각 에폭이 돌때마다 정확도가 오르고, 손실은 줄어드는 형태로 제대로 학습이 된것 처럼 보이나,검증 데이터셋(실선)을 보게 되면 그렇지 않다.각 에폭마다도 정확도는 오르지 않고, 손실이 늘어나는걸 확인할 수 있는데이런 경우 훈련 데이터셋에 과대적합(overfitting) 되었다고 한다.과대적합이 된 경우 모델이 새로운 데이터셋을 만났을 때 제대로 분류를 하지 못하게 된다. 모델 재학습하기아까와 동일한 형태의 모델을 구성하고,학습과 관련된 하이퍼파라미터만 변경하여 과대적합을 피해보자 123456model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 16) 160016 _________________________________________________________________ dense_4 (Dense) (None, 16) 272 _________________________________________________________________ dense_5 (Dense) (None, 1) 17 ================================================================= Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 _________________________________________________________________ 12345model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 111us/step - loss: 0.5347 - binary_accuracy: 0.7908 - val_loss: 0.4127 - val_binary_accuracy: 0.8663 Epoch 2/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.3256 - binary_accuracy: 0.9005 - val_loss: 0.3316 - val_binary_accuracy: 0.8697 Epoch 3/8 15000/15000 [==============================] - 1s 88us/step - loss: 0.2393 - binary_accuracy: 0.9232 - val_loss: 0.2822 - val_binary_accuracy: 0.8906 Epoch 4/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.1881 - binary_accuracy: 0.9410 - val_loss: 0.2801 - val_binary_accuracy: 0.8875 Epoch 5/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.1525 - binary_accuracy: 0.9524 - val_loss: 0.2770 - val_binary_accuracy: 0.8885 Epoch 6/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.1263 - binary_accuracy: 0.9611 - val_loss: 0.2856 - val_binary_accuracy: 0.8880 Epoch 7/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.1035 - binary_accuracy: 0.9701 - val_loss: 0.3127 - val_binary_accuracy: 0.8846 Epoch 8/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.0863 - binary_accuracy: 0.9754 - val_loss: 0.3270 - val_binary_accuracy: 0.8835 1show_graph(history) 아까보단 과대적합을 피한 부분이 보인다.손실 그래프를 확인했을 때 2에폭과 3에폭이 훈련 세트와 검증 세트가 가장 근접한 손실을 갖고있는결 확인할 수 있고,정확도 또한 2,3에폭이 가장 근접한걸 확인할 수 있다.즉, 이 모델의 경우 2에폭 혹은 3에폭을 돌렸을 때 과대적합을 가장 피할 수 있는 학습상태가 된다는걸 확인할 수 있다.이렇게 학습에 파라미터를 조작하는 것 이외에도 과대적합을 피하는 기법이 많이 존재한다. 모델의 평가모델의 정확도를 측정한다. 12loss, accuracy = model.evaluate(x_test, y_test)print('accuracy : &#123;acc&#125;, loss : &#123;loss&#125;'.format(acc=accuracy, loss=loss)) 25000/25000 [==============================] - 2s 67us/step accuracy : 0.86852, loss : 0.35010315059185027 모델의 예측긍정이거나 부정일 확률 (높으면 긍정, 낮으면 부정) 1model.predict(x_test[:10]) array([[0.2251153 ], [0.9999784 ], [0.98094064], [0.94734573], [0.97099954], [0.9737046 ], [0.9995834 ], [0.01185756], [0.9645392 ], [0.99970514]], dtype=float32) 번외. 레이어 변경하여 정확도 개선해보기레이어를 한개 더 추가하여 테스트 (Deep)12345678910111213model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 16) 160016 _________________________________________________________________ dense_7 (Dense) (None, 16) 272 _________________________________________________________________ dense_8 (Dense) (None, 16) 272 _________________________________________________________________ dense_9 (Dense) (None, 1) 17 ================================================================= Total params: 160,577 Trainable params: 160,577 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 113us/step - loss: 0.5264 - binary_accuracy: 0.7816 - val_loss: 0.4348 - val_binary_accuracy: 0.8122 Epoch 2/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.2989 - binary_accuracy: 0.9006 - val_loss: 0.2936 - val_binary_accuracy: 0.8870 Epoch 3/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.2103 - binary_accuracy: 0.9263 - val_loss: 0.2941 - val_binary_accuracy: 0.8812 Epoch 4/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.1550 - binary_accuracy: 0.9481 - val_loss: 0.2963 - val_binary_accuracy: 0.8817 Epoch 5/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.1294 - binary_accuracy: 0.9543 - val_loss: 0.2956 - val_binary_accuracy: 0.8850 Epoch 6/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.0964 - binary_accuracy: 0.9709 - val_loss: 0.3357 - val_binary_accuracy: 0.8745 Epoch 7/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.0814 - binary_accuracy: 0.9739 - val_loss: 0.3678 - val_binary_accuracy: 0.8714 Epoch 8/8 15000/15000 [==============================] - 1s 84us/step - loss: 0.0598 - binary_accuracy: 0.9834 - val_loss: 0.3910 - val_binary_accuracy: 0.8714 1show_graph(history) 유닛을 추가하여 테스트 (Wide)123456789101112model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 64) 640064 _________________________________________________________________ dense_11 (Dense) (None, 64) 4160 _________________________________________________________________ dense_12 (Dense) (None, 1) 65 ================================================================= Total params: 644,289 Trainable params: 644,289 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 135us/step - loss: 0.4850 - binary_accuracy: 0.7656 - val_loss: 0.3620 - val_binary_accuracy: 0.8517 Epoch 2/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.2538 - binary_accuracy: 0.9058 - val_loss: 0.2754 - val_binary_accuracy: 0.8902 Epoch 3/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.1857 - binary_accuracy: 0.9340 - val_loss: 0.2826 - val_binary_accuracy: 0.8874 Epoch 4/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.1417 - binary_accuracy: 0.9499 - val_loss: 0.3328 - val_binary_accuracy: 0.8734 Epoch 5/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.1128 - binary_accuracy: 0.9601 - val_loss: 0.3275 - val_binary_accuracy: 0.8826 Epoch 6/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.0795 - binary_accuracy: 0.9743 - val_loss: 0.3473 - val_binary_accuracy: 0.8802 Epoch 7/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.0544 - binary_accuracy: 0.9832 - val_loss: 0.3840 - val_binary_accuracy: 0.8780 Epoch 8/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0514 - binary_accuracy: 0.9849 - val_loss: 0.4150 - val_binary_accuracy: 0.8789 1show_graph(history) 깊고 넓게 구성하기 (Deep and wide network)1234567891011121314model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_13 (Dense) (None, 64) 640064 _________________________________________________________________ dense_14 (Dense) (None, 64) 4160 _________________________________________________________________ dense_15 (Dense) (None, 32) 2080 _________________________________________________________________ dense_16 (Dense) (None, 32) 1056 _________________________________________________________________ dense_17 (Dense) (None, 1) 33 ================================================================= Total params: 647,393 Trainable params: 647,393 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 142us/step - loss: 0.5094 - binary_accuracy: 0.7512 - val_loss: 0.3875 - val_binary_accuracy: 0.8442 Epoch 2/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.2693 - binary_accuracy: 0.8983 - val_loss: 0.4223 - val_binary_accuracy: 0.8310 Epoch 3/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.1949 - binary_accuracy: 0.9275 - val_loss: 0.5629 - val_binary_accuracy: 0.7950 Epoch 4/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.1534 - binary_accuracy: 0.9434 - val_loss: 0.2965 - val_binary_accuracy: 0.8854 Epoch 5/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.1106 - binary_accuracy: 0.9613 - val_loss: 0.3647 - val_binary_accuracy: 0.8718 Epoch 6/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.0797 - binary_accuracy: 0.9733 - val_loss: 0.4042 - val_binary_accuracy: 0.8748 Epoch 7/8 15000/15000 [==============================] - 2s 110us/step - loss: 0.0802 - binary_accuracy: 0.9775 - val_loss: 0.4029 - val_binary_accuracy: 0.8815 Epoch 8/8 15000/15000 [==============================] - 2s 106us/step - loss: 0.0656 - binary_accuracy: 0.9827 - val_loss: 0.4207 - val_binary_accuracy: 0.8809 1show_graph(history) 손실함수 변경1234567891011121314model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.MSE, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 64) 640064 _________________________________________________________________ dense_19 (Dense) (None, 64) 4160 _________________________________________________________________ dense_20 (Dense) (None, 32) 2080 _________________________________________________________________ dense_21 (Dense) (None, 32) 1056 _________________________________________________________________ dense_22 (Dense) (None, 1) 33 ================================================================= Total params: 647,393 Trainable params: 647,393 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 138us/step - loss: 0.1701 - binary_accuracy: 0.7462 - val_loss: 0.1152 - val_binary_accuracy: 0.8508 Epoch 2/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.0794 - binary_accuracy: 0.8991 - val_loss: 0.0829 - val_binary_accuracy: 0.8917 Epoch 3/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0580 - binary_accuracy: 0.9281 - val_loss: 0.0892 - val_binary_accuracy: 0.8833 Epoch 4/8 15000/15000 [==============================] - 2s 111us/step - loss: 0.0375 - binary_accuracy: 0.9544 - val_loss: 0.0858 - val_binary_accuracy: 0.8876 Epoch 5/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0359 - binary_accuracy: 0.9560 - val_loss: 0.0874 - val_binary_accuracy: 0.8863 Epoch 6/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0216 - binary_accuracy: 0.9751 - val_loss: 0.0927 - val_binary_accuracy: 0.8822 Epoch 7/8 15000/15000 [==============================] - 2s 109us/step - loss: 0.0189 - binary_accuracy: 0.9782 - val_loss: 0.0946 - val_binary_accuracy: 0.8812 Epoch 8/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0068 - binary_accuracy: 0.9931 - val_loss: 0.1091 - val_binary_accuracy: 0.8678 1show_graph(history)","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"}]}]}