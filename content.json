{"pages":[{"title":"categories","date":"2019-01-10T09:19:08.000Z","updated":"2019-01-10T09:19:31.122Z","comments":true,"path":"categories/index.html","permalink":"https://codingcrews.github.io/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2019-01-10T06:28:13.000Z","updated":"2019-01-11T06:16:46.403Z","comments":true,"path":"about/index.html","permalink":"https://codingcrews.github.io/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-01-10T09:18:04.000Z","updated":"2019-01-10T09:18:25.185Z","comments":true,"path":"tags/index.html","permalink":"https://codingcrews.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"AWS로 GPU로 딥러닝 학습환경 구축하기","slug":"deeplearning-gpu","date":"2019-01-15T07:42:51.000Z","updated":"2019-01-15T07:43:22.150Z","comments":true,"path":"2019/01/15/deeplearning-gpu/","link":"","permalink":"https://codingcrews.github.io/2019/01/15/deeplearning-gpu/","excerpt":"","text":"Aws GPU 인스턴스를 이용해 딥러닝 환경 구축하기안녕하세요. 오늘은 아마존 웹 서비스(이하 AWS)를 이용하여 GPU 인스턴스를 이용한 딥러닝 학습환경 만들기에 대해 알아봅시다. 딥러닝이란 최근 핫해진 뉴럴넷 기반의 기계학습 기법을 말하는데요.학습 시 굉장히 많은 연산을 필요로 하여 학습에 소요되는 시간이 많이 필요합니다.단순한 연산을 많이 하기 때문에 병렬처리에 특화된 GPU를 사용하여 학습시간을 단축시킬 수 있는데요.당연히 좋은 GPU를 사용할수록 짧은 학습시간이 필요하겠지요.하지만 문제는 이 GPU 가격이 개인이 사서 사용하기엔 너무나도 비싸다고 느낄 수 있습니다. 위 사진은 GPU 연산에 특화되어 나온 모델들입니다.저희가 사용할 AWS EC2 P2, P3 인스턴스에 제공되는 GPU 모델들입니다. 포스팅 날짜를 기준으로 최소 200만원 이상의 가격을 보여주고 있는데요.개인이라도 사지 못할 가격은 아닙니다만, 딥러닝이 무엇인지 이제 알아가는 시점에 입문자가 구매하기엔 굉장히 벅찬 금액이죠.하지만 AWS에서 제공해주는 GPU인스턴스를 빌려서 시간제로 비용을 낼 수 있다면, 비싼 GPU를 구매하지 않고도 필요할 때에만 성능이 좋은 GPU를 이용할 수 있겠죠 ? Aws EC2EC2에 관한 설명은 생략하도록 하고, 여기에서 자세하게 확인 가능합니다. 인스턴스 종류 및 가격저희는 GPU 인스턴스를 사용할 예정이기 때문에 P, G 두가지 계열을 확인하면 됩니다.인스턴스 스펙으로 이동하여 자세한 스펙을 확인 가능합니다.각 인스턴스의 가격은 시간에 비례하여 비용이 청구되는 방식이며, 각 리전마다 사용가격의 편차가 있습니다. 서울리전은 해외리전보다 비싼편이니 (약 2배가량 비쌈), 연구목적으로 사용하실 때에는 해외리전을 이용하시는걸 추천드립니다. 위 사진은 오하이오 리전의 가격이며, 아래 사진은 서울리전의 가격입니다.같은 p2.xlarge의 가격을 보더라도 오하이오 리전은 시간당 $0.9, 서울리전은 시간당 $1.465의 가격을 보여줍니다.그리고, 서울리전은 g3계열의 인스턴스가 제공되지 않고 있습니다. 오하이오 리전의 p2.xlarge 가격을 보면 대략 시간당 1,000원 정도의 가격을 보여줍니다.요새 PC방 가격도 시간당 1,000원이 넘는걸 생각하면 그리 비싸다고 생각하진 않을 수 있습니다. 스팟 인스턴스하지만 위의 가격은 온디맨드 계약의 가격이며, AWS에서 더 할인을 받을 수 있는 방법이 존재합니다.그건 바로 R.I(예약 인스턴스) 혹은 Spot인스턴스를 이용하는 방법입니다.R.I의 경우는 최소 계약단위가 1년 단위이기 때문에 개인이 하기엔 부담스럽고, 오히려 서비스를 운영중인 회사에서 GPU인스턴스가 필요할 때 사용할 수 있겠습니다. 스팟 인스턴스의 경우는 인스턴스를 경매입찰방식으로 할당받는 개념인데, 최대 90%까지 가격이 할인될 수 있습니다. 온디맨드의 가격보다 더 저렴한 가격을 확인할 수 있습니다.이 스팟 인스턴스를 이용해보도록 하겠습니다. 스팟 인스턴스 생성 AWS 관리 콘솔에서 스팟 요청을 눌러줍니다. 자주 쓰이는 형태의 패턴들이 이미 만들어져 있으나, 명시한 시간동안 인스턴스를 사용할 수 있도록 시간을 지정하여 사용합니다. (최소 1시간에서 최대 6시간까지 사용 가능합니다.) 인스턴스 타입 변경을 눌러 사용할 인스턴스의 타입을 지정합니다. 저는 GPU compute에서 p2.xlarge를 이용하겠습니다.GPU instance에서 조금 더 저렴한 G3 계열의 인스턴스도 사용 가능합니다. AMI 검색을 눌러 사용할 기본 AMI를 지정합니다. 딥러닝 프레임워크를 사용하여 GPU를 사용하기 위해선 GPU 설정이 많이 필요한데, 이러한 사전 작업을 마친 템플릿을 AWS에서 공식 AMI로 제공합니다.꼭 소유자를 확인하여 아마존 공식 이미지가 맞는지 확인합니다.(일반 유저도 AMI를 만들어 배포할 수 있어 마이닝 프로그램을 설치해둔 템플릿들이 다수 존재하니 주의합시다.) Additional configurations를 눌러 시큐리티 그룹 등 인스턴스에 필요한 설정을 마쳐줍시다. 스팟 인스턴스가 정상적으로 생성되었습니다. 간혹 해외리전의 스팟 인스턴스 제한이 걸려있어 생성되지 않는 경우가 존재합니다.이때는 AWS에 케이스를 오픈하여 리밋제한을 상향요청 할 수 있습니다. 생성된 인스턴스 확인 생성된 인스턴스의 유형과 최대 가격을 볼 수 있습니다.최대가격이란 명시된 시간동안 다른 인스턴스들이 높은 입찰가를 제시할 때 인스턴스가 종료 될 수 있으니 최대 가격이 온디맨드 가격으로 자동적으로 조절됩니다. 영구적으로 사용할 볼륨 생성하기스팟 인스턴스가 종료될 시 75GB로 할당된 AMI 볼륨이 자동적으로 삭제됩니다.인스턴스 생성 시 해당 볼륨을 삭제하지 않을 수 있으나, 75GB의 볼륨을 갖고 있기는 부담스럽기 때문에 우리가 필요한 데이터들만 저장할 수 있는 작은 크기의 볼륨을 따로 생성하여 인스턴스의 종료와 무관하게 사용할 수 있도록 새로운 볼륨을 만들어 추가해봅시다. 데이터 영구보존 볼륨 생성하기 좌측의 볼륨 메뉴를 눌러 현재 인스턴스의 가용영역을 확인 한 이후,볼륨 생성을 눌러줍니다. 바로 이전에 확인한 가용영역을 맞춰 새로운 볼륨을 생성합니다. 가용영역이 다르다면 볼륨이 인스턴스에 사용할 수 없으니 꼭 가용영역을 맞춰 생성합니다. 생성된 볼륨을 확인할 수 있습니다. 우클릭을 이용해 볼륨 연결을 눌러줍니다. 볼륨을 연결합니다. 인스턴스를 눌러주면 자동적으로 같은 가용영역에 있는 사용중인 인스턴스 목록이 나옵니다.디바이스는 해당 인스턴스에 사용될 디바이스명입니다. 여기까지가 콘솔에서 설정 가능한 스팟 인스턴스에 대한 인스턴스 생성과 설정입니다. 이후는 AWS 인스턴스에 ssh접속을 한 이후 설정하는 부분입니다. SSH 접속SSH 로 생성한 스팟 인스턴스에 접근하면 초기 화면이 이렇게 보이는걸 확인할 수 있습니다.각각의 가상환경에 진입할 수 있는 명령어와 사용가능한 환경에 대한 설명이 보입니다. 123456789101112131415161718192021222324252627282930313233============================================================================= __| __|_ ) _| ( / Deep Learning AMI (Amazon Linux) Version 20.0 ___|\\___|___|=============================================================================Please use one of the following commands to start the required environment with the framework of your choice:for MXNet(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p36for MXNet(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _____________________________________ source activate mxnet_p27for MXNet(+Amazon Elastic Inference) with Python3 _______________________________________ source activate amazonei_mxnet_p36for MXNet(+Amazon Elastic Inference) with Python2 _______________________________________ source activate amazonei_mxnet_p27for TensorFlow(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p36for TensorFlow(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p27for Tensorflow(+Amazon Elastic Inference) with Python2 _____________________________ source activate amazonei_tensorflow_p27for Theano(+Keras2) with Python3 (CUDA 9.0) _____________________________________________________ source activate theano_p36for Theano(+Keras2) with Python2 (CUDA 9.0) _____________________________________________________ source activate theano_p27for PyTorch with Python3 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p36for PyTorch with Python2 (CUDA 10.0 and Intel MKL) _____________________________________________ source activate pytorch_p27for CNTK(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p36for CNTK(+Keras2) with Python2 (CUDA 9.0 and Intel MKL-DNN) _______________________________________ source activate cntk_p27for Caffe2 with Python2 (CUDA 9.0) ______________________________________________________________ source activate caffe2_p27for Caffe with Python2 (CUDA 8.0) ________________________________________________________________ source activate caffe_p27for Caffe with Python3 (CUDA 8.0) ________________________________________________________________ source activate caffe_p35for Chainer with Python2 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p27for Chainer with Python3 (CUDA 9.0 and Intel iDeep) ____________________________________________ source activate chainer_p36for base Python2 (CUDA 9.0) ________________________________________________________________________ source activate python2for base Python3 (CUDA 9.0) ________________________________________________________________________ source activate python3Official Conda User Guide: https://conda.io/docs/user-guide/index.htmlAWS Deep Learning AMI Homepage: https://aws.amazon.com/machine-learning/amis/Developer Guide and Release Notes: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.htmlSupport: https://forums.aws.amazon.com/forum.jspa?forumID=263For a fully managed experience, check out Amazon SageMaker at https://aws.amazon.com/sagemaker============================================================================= 파이썬 가상환경 진입저희는 Python 3.6기반의 Tensorflow 사용할 수 있는 가상환경으로 진입해보겠습니다.1234[ec2-user@ip-172-31-15-213 ~]$ source activate tensorflow_p36# 파이썬 가상환경으로 진입하면 쉘이 아래와 같이 변합니다. (tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ 연결한 볼륨 확인생성하고 나서 연결한 볼륨에 대한 정보를 확인합니다. 123456(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTxvda 202:0 0 75G 0 disk└─xvda1 202:1 0 75G 0 part /xvdf 202:80 0 8G 0 disk 아까 연결한 8GB의 볼륨이 xvdf로 연결되어있는걸 확인할 수 있습니다. 볼륨의 포맷 확인연결된 xvdf 디바이스를 통해 볼륨의 포맷을 확인합니다.123(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf/dev/xvdf: data 갓 생성한 볼륨의 경우 data로 나오게 될 것이고, 이 볼륨은 사용하고 있는 os 파일 시스템에 맞춰 파일 포맷을 진행해줘야 합니다. 볼륨 ext4 포맷볼륨의 파일시스템을 ext4 포맷으로 변경합니다.123456789101112(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mkfs -t ext4 /dev/xvdfmke2fs 1.43.5 (04-Aug-2017)Creating filesystem with 2097152 4k blocks and 524288 inodesFilesystem UUID: 6ce22348-395a-4ac1-8df7-e180aaadaadfSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632Allocating group tables: doneWriting inode tables: doneCreating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done 볼륨 포맷 재확인파일 시스템 포맷이 완료 된 후 ext4 포맷으로 변경됬는지 확인합니다. 123(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo file -s /dev/xvdf/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf (extents) (64bit) (large files) (huge files) 결과에 나오는 UUID를 잘 기억하세요. 볼륨 마운트포맷 완료 된 볼륨을 사용하기 위해서는 마운트 과정이 필요합니다.현재 Home 디렉터리에 새로운 디렉터리를 만들어 마운트를 진행합니다. 12(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ mkdir mount_dir(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount /dev/xvdf mount_dir/ 정상적으로 마운트 되었다면 에러 메세지 없이 완료됩니다. 볼륨 자동 마운트위와 같이 수동으로 마운트 시켜 사용하는 경우 인스턴스가 재시동 되는 경우 마운트를 다시 해줘야 합니다./etc/fstab 이라는 폴더는 부팅 시 마운트 해야할 목록을 가지고 있어 자동으로 마운트가 진행되도록 합니다.fstab 파일이 잘못 작성 되어 있으면 부팅이 안되는 경우도 발생하니 백업을 만들고, 신중하게 파일을 수정하도록 합시다. 아래 명령어는 복붙하지마시고 본인의 UUID와 마운트 할 디렉터리에 맞춰 변경하여 사용하세요.1234(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo cp /etc/fstab /etc/fstab.orig (tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ echo &quot;UUID=6ce22348-395a-4ac1-8df7-e180aaadaadf /home/ec2-user/mount_dir ext4 defaults,nofail 0 2&quot; | sudo tee --apend /etc/fstab(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ sudo mount -a 마지막 mount -a 옵션을 꼭 실행해보시고, 정상적이라면 아무 메시지 없이 완료됩니다.에러 메세지가 발생한 경우 fstab 파일에 문제가 있을 수 있으니 꼭 확인하세요. Jupyter Notebook 실행원격에서 접속하기 위해 IP 대역을 전부 풀어줍니다.Port 번호는 기본적으로 8888 포트를 사용하니 시큐리티 그룹에서 해당 포트를 허용시켜주시고, 다른 포트번호로 사용하셔도 무방합니다. 123456789101112131415161718192021(tensorflow_p36) [ec2-user@ip-172-31-15-213 ~]$ jupyter notebook --ip 0.0.0.0[I 06:43:24.447 NotebookApp] Using EnvironmentKernelSpecManager...[I 06:43:24.448 NotebookApp] Started periodic updates of the kernel list (every 3 minutes).[I 06:43:24.552 NotebookApp] Writing notebook server cookie secret to /home/ec2-user/.local/share/jupyter/runtime/notebook_cookie_secret[I 06:43:27.661 NotebookApp] Loading IPython parallel extension[I 06:43:27.784 NotebookApp] JupyterLab beta preview extension loaded from /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/jupyterlab[I 06:43:27.784 NotebookApp] JupyterLab application directory is /home/ec2-user/anaconda3/envs/tensorflow_p36/share/jupyter/lab[I 06:43:28.405 NotebookApp] [nb_conda] enabled[I 06:43:28.408 NotebookApp] Serving notebooks from local directory: /home/ec2-user[I 06:43:28.408 NotebookApp] 0 active kernels[I 06:43:28.408 NotebookApp] The Jupyter Notebook is running at:[I 06:43:28.408 NotebookApp] http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d[I 06:43:28.408 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 06:43:28.408 NotebookApp] No web browser found: could not locate runnable browser.[C 06:43:28.408 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://ip-172-31-15-213:8888/?token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d&amp;token=d591bd615294176a215c3ae4273b1b41ab12db439b7fbb8d[I 06:43:28.409 NotebookApp] Starting initial scan of virtual environments... ec2 인스턴스의 public ip 주소가 만약 1.10.10.10 이라고 가정하면,http://1.10.10.10:8888 로 접속하시면 위에 출력된 해당 토큰을 요청합니다.토큰을 입력해주면 쥬피터 노트북을 사용하실 수 있습니다.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"AWS","slug":"aws","permalink":"https://codingcrews.github.io/tags/aws/"}]},{"title":"보스턴 집값 데이터를 이용한 선형 회귀","slug":"boston_linear_regression","date":"2019-01-14T18:06:54.000Z","updated":"2019-01-14T18:09:34.885Z","comments":true,"path":"2019/01/15/boston_linear_regression/","link":"","permalink":"https://codingcrews.github.io/2019/01/15/boston_linear_regression/","excerpt":"","text":"선형 회귀 문제 : Linear Regression머신러닝으로 해결할 수 있는 문제 중 분류 문제들을 이전 포스팅에서 다뤄보았고,이번에는 연속된 데이터를 예측해야하는 회귀 문제를 보겠다. 회귀 문제란 예를 들어 주택에 대한 주위 범죄율, 세율 등의 데이터가 주어졌을 때 주택 가격을 예측하는것처럼연속된 값을 예측하는걸 회귀 문제라고 한다. Dataset사용하는 데이터셋은 카네기 멜런 대학교에서 제공하는 데이터셋으로 데이터의 값에 해당하는 설명은 여기에서 확인할 수 있다. Download12from tensorflow.keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_targets.shape)) Train Datas Shape : (404, 13) Train Labels Shape : (404,) 데이터 확인각 컬럼별로 해당하는 데이터들을 확인할 수 있고, 레이블에는 해당 주택의 가격값이 들어가있는걸 확인할 수 있다. 12display(train_data[0])display(train_targets[0:10]) array([ 1.23247, 0. , 8.14 , 0. , 0.538 , 6.142 , 91.7 , 3.9769 , 4. , 307. , 21. , 396.9 , 18.72 ]) array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4]) 데이터 변환신경망 학습 시 차이가 큰 값을 신경망에 주입하면 학습이 잘 되지 않는다.(0.01, 0.1)과 (1, 10)을 보면 비율대로만 보게되면 10배의 차이를 가지는걸 볼 수 있지만,실수 체계에서 값의 차이는 어마어마하게 큰 차이로 볼 수 있다.(1, 10)과 같은 데이터를 Sparse 하다라고 표현하며,(0.01, 0.1)과 같은 밀도가 높은 데이터를 Dense 데이터라고 표현한다. 이렇게 Dense 데이터로 변경하는 과정이 필요한데, 이 부분을 Scaling 혹은 정규화 한다고 한다. 데이터의 정규화를 위해서 각 특성별 평균값을 뺀 이후에 표준편차로 나눠주게 되면,특성의 중앙이 0에 가깝게 맞춰지고, 표준편차는 1로 된다. 1234567mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std 모델 구성데이터셋이 작으므로 간단한 모델을 구성하는게 과대적합을 피하기가 손쉽다. 12345678910from tensorflow.keras import modelsfrom tensorflow.keras import layersdef build_network(input_shape=(0,)): model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=input_shape)) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='adam', loss='mse', metrics=['mae']) return model 모델 구성의 마지막 Dense layer를 보면 활성함수가 지정 되어있지 않은걸 확인할 수 있는데,우리가 필요한 데이터는 선형 데이터를 원하기 때문에 활성함수를 지정하지 않으면 스칼라 회귀를 위한 구성이 완성된다.반대로 sigmoid 같은 활성화 함수를 적용하면 네트워크가 0~1 사이의 값을 만들도록 될 것이다. 우리가 필요한 데이터는 주택가격을 예측하기 위함으로 활성함수를 지정하지 않고 스칼라 회귀 값을 예측하도록 구성한다. K-Fold Validation데이터셋의 크기가 매우 작기 때문에 검증셋(Validation)의 크기도 매우 작아지게 되는데,이때 데이터셋이 적어지면서 훈련세트 중 어느 한 특정 부분이 훈련세트로 사용되는지에 따라 모델의 정확도가 크게 달라질 수 있다.이 이유는 훈련세트와 검증세트로 나눴을 때 각 값들의 분포도가 고르게 되지 못할 경우가 생기기 때문이다. 이런 상황에서 가장 좋은 방법은 K-겹 교차 검증(K-Fold Cross Validation)을 실시하는것인데,데이터를 K개의 분할로 나누고 각각 K개의 모델을 만들어 K-1 개의 분할에서 훈련하고 나머지 분할에서 평가를 하는법이다.이 점수는 각 검증 데이터셋의 평균으로 모델을 평가하게 된다. 즉, 여러 폴드의 데이터셋으로 나누어 교차검증을 함으로써 데이터 분포에 신경쓰지 않고 훈련세트의 모든 부분을 사용해 모델을 평가할 수 있는 장점이 있다. 12345678910111213141516171819202122232425262728293031323334353637import numpy as npk = 4num_val_samples = len(train_data) // knum_epochs = 150all_scores = []all_history = []for i in range(k): print('폴드 번호 #&#123;&#125;'.format(i)) fold_start_index = i * num_val_samples fold_end_index = (i + 1) * num_val_samples val_data = train_data[fold_start_index : fold_end_index] val_targets = train_targets[fold_start_index : fold_end_index] partial_train_data = np.concatenate( [train_data[:fold_start_index], train_data[fold_end_index:]], axis=0 ) partial_train_targets = np.concatenate( [train_targets[:fold_start_index], train_targets[fold_end_index:]], axis=0 ) model = build_network((partial_train_data.shape[1], )) history = model.fit( partial_train_data, partial_train_targets, epochs=num_epochs, validation_data=(val_data, val_targets), batch_size=1, verbose=0 ) val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0) all_scores.append(val_mse) all_history.append(history.history) 폴드 번호 #0 폴드 번호 #1 폴드 번호 #2 폴드 번호 #3 12345val_mae_lst = map(lambda x: x['val_mean_absolute_error'], all_history)val_mae_lst = np.array(list(val_mae_lst))avg_mae = [ np.mean([x[i] for x in val_mae_lst]) for i in range(num_epochs)] 검증 데이터 시각화검증 데이터 시각화를 위해 history 데이터를 에폭별 평균 데이터로 치환시킨다. 12345val_mae_lst = map(lambda x: x['val_mean_absolute_error'], all_history)val_mae_lst = np.array(list(val_mae_lst))avg_mae = [ np.mean([x[i] for x in val_mae_lst]) for i in range(num_epochs)] 그래프 변동이 심하지 않도록 이전 포인트의 지수 이동 평균값으로 변경시켜주는 함수를 만든다 12345678910import matplotlib.pyplot as pltdef smooth_curve(points, factor=.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 123456def show_graph(data): smooth_data = smooth_curve(data) plt.plot(range(1, len(smooth_data) + 1), smooth_data) plt.xlabel('Epochs') plt.ylabel('Validation MAE') plt.show() 1show_graph(avg_mae[10:])","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"}]},{"title":"로이터 데이터셋을 이용한 뉴스 기사 다중 분류","slug":"reuter_multi_classification","date":"2019-01-14T14:27:59.000Z","updated":"2019-01-14T14:29:37.993Z","comments":true,"path":"2019/01/14/reuter_multi_classification/","link":"","permalink":"https://codingcrews.github.io/2019/01/14/reuter_multi_classification/","excerpt":"","text":"다중 분류 문제 : Multiple class classification이번 포스팅에서는 2개 이상의 클래스를 가진 경우 사용할 수 있는 다중 분류 문제를 해결해보도록 한다. Dataset로이터 데이터셋을 사용하도록 한다.데이터셋은 46개의 토픽을 갖고 있으며 각 뉴스기사마다 하나의 토픽이 정해져있다.즉, 단일 레이블 다중분류 문제로 볼 수 있다. Download12from tensorflow.keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_labels.shape)) Train Datas Shape : (8982,) Train Labels Shape : (8982,) 데이터 확인확인한대로 8982개의 훈련 데이터셋이 존재하며,각 인덱스는 단어 인덱스의 리스트를 뜻 한다. 1234display(train_data[0][:10])display(train_labels)display(test_labels) [1, 2, 2, 8, 43, 10, 447, 5, 25, 207] array([ 3, 4, 3, ..., 25, 3, 25]) array([ 3, 10, 1, ..., 3, 3, 24]) 단어를 텍스트로 디코딩1234word_index = reuters.get_word_index()reverse_word_index = dict([(val, key) for (key, val) in word_index.items()])decoded_news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])print(decoded_news) ? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3 라벨 확인샘플의 라벨은 토픽의 인덱스로써 0~45의 값을 가진다. 1train_labels[0] 3 데이터 변환학습에 용이하도록 뉴스 기사와 라벨 데이터를 벡터로 변환시킨다.학습에 사용되는 데이터셋의 인풋 데이터는 해당 뉴스 기사의 들어가있는 단어 인덱스를 1.0 으로 변경시킨다. 1234567import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성 for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 12345678910111213from tensorflow.keras.utils import to_categoricalx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)display(x_train.shape)display(x_test.shape)one_hot_train_labels = to_categorical(train_labels)one_hot_test_labels = to_categorical(test_labels)display(one_hot_train_labels.shape)display(one_hot_test_labels.shape) (8982, 10000) (2246, 10000) (8982, 46) (2246, 46) 신경망 구성단일 레이블 다중 분류를 위한 모델을 작성 해보도록 하겠다.다만 레이어 구축 시 참고해야 할 부분이 있는데,각 레이어를 통과 할 때 유닛의 수가 레이블 보다 적다면 가지고 있어야 할 정보가 많이 사라질 수 있다. 학습 시 파라미터를 줄이기 위해서나 노이즈를 줄이기 위해서 유닛을 줄이는 경우도 있지만,데이터가 가지고 있어야 할 필수 데이터를 잃어버릴 수 있다는 점도 참고하여 네트워크를 구성해야 한다. 신경망 네트워크 구축로이터 데이터셋의 단어수를 10,000개로 제한해두었으니,신경망의 입력 차원수도 10,000으로 설정한다. 123456789from tensorflow.keras import modelsfrom tensorflow.keras import layersmodel = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 64) 640064 _________________________________________________________________ dense_4 (Dense) (None, 64) 4160 _________________________________________________________________ dense_5 (Dense) (None, 46) 2990 ================================================================= Total params: 647,214 Trainable params: 647,214 Non-trainable params: 0 _________________________________________________________________ 이진분류와 비슷하게 마지막 Dense 레이어의 아웃풋 벡터 개수는(46개) 예측하기 위한 클래스의 개수와 동일합니다.다른점은 이진 분류에서는 활성함수로 sigmoid를 사용한 반면 여기서는 softmax를 이용했는데,softmax는 각 클래스별로 해당 클래스 일 확률을 표시하도록 만들어집니다. 모델 컴파일다중 분류에서 손실함수로써는 categorical_crossentropy를 주로 사용합니다.옵티마이저는 가장 빠르고 효과가 좋다고 알려진 adam 옵티마이저를 사용하도록 설정합니다. 12345model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],) 검증 데이터셋 구성훈련용 데이터셋에서 Validation으로 사용할 데이터셋을 분리시킨다. 12345x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = one_hot_train_labels[:1000]partial_y_train = one_hot_train_labels[1000:] 모델 학습1234567history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val)) Train on 7982 samples, validate on 1000 samples Epoch 1/20 7982/7982 [==============================] - 1s 152us/step - loss: 3.3032 - acc: 0.4328 - val_loss: 2.6156 - val_acc: 0.5320 Epoch 2/20 7982/7982 [==============================] - 1s 78us/step - loss: 2.0615 - acc: 0.6076 - val_loss: 1.6695 - val_acc: 0.6460 Epoch 3/20 7982/7982 [==============================] - 1s 78us/step - loss: 1.3844 - acc: 0.7051 - val_loss: 1.3219 - val_acc: 0.7100 Epoch 4/20 7982/7982 [==============================] - 1s 77us/step - loss: 1.0796 - acc: 0.7669 - val_loss: 1.1773 - val_acc: 0.7520 Epoch 5/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.8673 - acc: 0.8132 - val_loss: 1.0768 - val_acc: 0.7790 Epoch 6/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.6909 - acc: 0.8515 - val_loss: 0.9995 - val_acc: 0.7910 Epoch 7/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.5410 - acc: 0.8880 - val_loss: 0.9467 - val_acc: 0.8010 Epoch 8/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.4177 - acc: 0.9137 - val_loss: 0.9069 - val_acc: 0.8190 Epoch 9/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.3253 - acc: 0.9300 - val_loss: 0.8855 - val_acc: 0.8160 Epoch 10/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.2593 - acc: 0.9432 - val_loss: 0.8973 - val_acc: 0.8090 Epoch 11/20 7982/7982 [==============================] - 1s 80us/step - loss: 0.2123 - acc: 0.9503 - val_loss: 0.8912 - val_acc: 0.8210 Epoch 12/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1792 - acc: 0.9549 - val_loss: 0.9012 - val_acc: 0.8230 Epoch 13/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1594 - acc: 0.9565 - val_loss: 0.9194 - val_acc: 0.8170 Epoch 14/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1410 - acc: 0.9573 - val_loss: 0.9531 - val_acc: 0.8150 Epoch 15/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.1294 - acc: 0.9578 - val_loss: 0.9501 - val_acc: 0.8160 Epoch 16/20 7982/7982 [==============================] - 1s 78us/step - loss: 0.1192 - acc: 0.9603 - val_loss: 0.9757 - val_acc: 0.8130 Epoch 17/20 7982/7982 [==============================] - 1s 77us/step - loss: 0.1146 - acc: 0.9592 - val_loss: 0.9701 - val_acc: 0.8100 Epoch 18/20 7982/7982 [==============================] - 1s 79us/step - loss: 0.1058 - acc: 0.9600 - val_loss: 0.9883 - val_acc: 0.8080 Epoch 19/20 7982/7982 [==============================] - 1s 80us/step - loss: 0.1055 - acc: 0.9624 - val_loss: 1.0214 - val_acc: 0.8130 Epoch 20/20 7982/7982 [==============================] - 1s 81us/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.9970 - val_acc: 0.8150 훈련의 정확도와 손실 시각화이전 이진분류 포스팅에서 사용했던 함수를 그대로 끌어와 사용하도록 한다. 123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltdef show_graph(history): history_dict = history.history accuracy = history_dict['acc'] val_accuracy = history_dict['val_acc'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history) 그래프를 보면 대략 8~9번째 에폭부터 과대적합이 시작되는걸 알 수 있습니다. 새로운 데이터 예측해보기모델을 이용해 각 뉴스기사에 대한 토픽을 예측해보도록 합니다.softmax를 사용하여 각 뉴스별로 46개의 토픽에 해당하는 확률을 출력합니다.각 클래스 별 확률을 모두 더하면 1.0(100%)가 됩니다. 12predictions = model.predict(x_test)display(predictions.shape) (2246, 46) predictions는 테스트 데이터셋의 개수에 맞게 2246개의 결과가 들어있습니다.각 결과안에는 46개 클래스에 해당하는 확률값이 들어가있습니다. 1np.sum(predictions[0]) 1.0 각 46개의 모든 원소의 값을 모두 더하면 1.0(100%)가 된걸 확인할 수 있습니다. 12display(np.argmax(predictions[0]))display(predictions[0][3]) 3 0.96262544 predictions[0]의 3번째 인덱스가 가장 큰 값을 가진걸 확인하였고,해당 인덱스의 값을 확인하였더니 0.96262544(96.262544%)로 모델이 예측한걸 확인할 수 있습니다. 데이터 레이블 인코딩 방식 변경하여 학습하기one hot 인코딩이 아닌 정수형으로 토픽을 예측하도록 레이블 인코딩을 사용하도록 변경해본다.손실함수를 변경해주면 되는데,categorical_crossentropy는 범주형 인코딩일 시 사용하는 손실함수이고,정수형을 사용할 때에는 sparse_categorical_crossentropy를 사용한다. 12345678y_train = np.array(train_labels)y_test = np.array(test_labels)x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:] 123456model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 64) 640064 _________________________________________________________________ dense_7 (Dense) (None, 64) 4160 _________________________________________________________________ dense_8 (Dense) (None, 46) 2990 ================================================================= Total params: 647,214 Trainable params: 647,214 Non-trainable params: 0 _________________________________________________________________ 12345model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) 1234567history = model.fit( partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val)) Train on 7982 samples, validate on 1000 samples Epoch 1/9 7982/7982 [==============================] - 1s 116us/step - loss: 3.3313 - acc: 0.3983 - val_loss: 2.5661 - val_acc: 0.5770 Epoch 2/9 7982/7982 [==============================] - 1s 78us/step - loss: 2.0006 - acc: 0.6272 - val_loss: 1.6576 - val_acc: 0.6580 Epoch 3/9 7982/7982 [==============================] - 1s 78us/step - loss: 1.3389 - acc: 0.7134 - val_loss: 1.2907 - val_acc: 0.7090 Epoch 4/9 7982/7982 [==============================] - 1s 78us/step - loss: 1.0256 - acc: 0.7762 - val_loss: 1.1484 - val_acc: 0.7630 Epoch 5/9 7982/7982 [==============================] - 1s 80us/step - loss: 0.8077 - acc: 0.8339 - val_loss: 1.0422 - val_acc: 0.7870 Epoch 6/9 7982/7982 [==============================] - 1s 76us/step - loss: 0.6357 - acc: 0.8716 - val_loss: 0.9641 - val_acc: 0.8050 Epoch 7/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.4960 - acc: 0.8990 - val_loss: 0.9275 - val_acc: 0.8050 Epoch 8/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.3909 - acc: 0.9197 - val_loss: 0.8983 - val_acc: 0.8100 Epoch 9/9 7982/7982 [==============================] - 1s 78us/step - loss: 0.3116 - acc: 0.9346 - val_loss: 0.8843 - val_acc: 0.8190 1show_graph(history) 예측하는 방법은 아까와 동일하고,출력된 레이블 또한 softmax처럼 각 클래스 별 확률이 나오게 된다.9번의 에폭에서 과대적합 되던걸 확인하여 이번 학습은 최대 에폭을 9로 설정하여 학습을 진행한 내용의 그래프이다. 추가 개선사항중간 레이어의 유닛수가 너무 작게 되면 데이터에 대한 손실이 발생할 수 있지만,데이터를 압축하여 노이즈를 줄이는 효과를 얻을 수도 있고, 반대로 유닛수를 크게 한다면,해당 레이블을 표현하기 위한 데이터를 더 넣을 수 있게 된다고 볼 수도 있다.이러한 내용을 잘 숙지하여 레이어 구성을 변경해가며 테스트를 진행해보자.","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"}]},{"title":"IMDB 데이터셋을 이용한 영화 리뷰 분류","slug":"imdb","date":"2019-01-10T16:55:22.000Z","updated":"2019-01-10T16:55:57.810Z","comments":true,"path":"2019/01/11/imdb/","link":"","permalink":"https://codingcrews.github.io/2019/01/11/imdb/","excerpt":"","text":"IMDB 데이터셋을 이용한 영화 리뷰 분류 : Binary Classification영화 평점 데이터를 통한 이진 분류 DatasetDownload1from tensorflow.keras.datasets import imdb 1(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) Dataset shape 확인12print(\"Train Datas Shape : &#123;&#125;\".format(train_data.shape))print(\"Train Labels Shape : &#123;&#125;\".format(train_labels.shape)) Train Datas Shape : (25000,) Train Labels Shape : (25000,) 데이터 확인25000개의 훈련용 데이터셋이 존재하며, 각 인덱스는 단어 인덱스의 리스트 12display(train_data[0][:10])display(train_labels) [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65] array([1, 0, 0, ..., 0, 1, 0]) 단어 인덱스를 단어로 치환1234word_index = imdb.get_word_index()indexes = dict([(value, key) for (key, value) in word_index.items()])decoded_review = ' '.join(indexes.get(i - 3, '?') for i in train_data[0])print(decoded_review) ? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all 데이터 텐서 치환단어의 개수를 10,000개로 지정해두었고, 이 단어 인덱스를 원핫인코딩으로 변환하여10,000차원의 벡터로 변경시킴 1234567import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 크기가 들어온 리스트 (단어개수, 전체단어개수)이고, 모든 원소가 0인 행렬을 생성 for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 12345x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)display(x_train.shape)display(x_test.shape) (25000, 10000) (25000, 10000) 12345y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32')display(y_train)display(y_test) array([1., 0., 0., ..., 0., 1., 0.], dtype=float32) array([0., 1., 1., ..., 0., 0., 0.], dtype=float32) 신경망 구성신경망 네트워크 구축12from tensorflow.keras import modelsfrom tensorflow.keras import layers 123456model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 160016 _________________________________________________________________ dense_1 (Dense) (None, 16) 272 _________________________________________________________________ dense_2 (Dense) (None, 1) 17 ================================================================= Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 _________________________________________________________________ 모델 컴파일모델을 사용하기 위해선 네트워크를 구성한 모델을 컴파일하는 과정이 필요하다.rmsprop 옵티마이저를 사용하고,확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택인데,이진 분류로 각 확률을 구하는 모델이니 binary crossentropy를 사용한다. 123456789101112from tensorflow.keras import optimizersfrom tensorflow.keras import metricsfrom tensorflow.keras import lossesmodel.compile(# optimizer='rmsprop', optimizer=optimizers.RMSprop(lr=0.001),# loss='binary_crossentropy', loss=losses.binary_crossentropy,# metrics=['accuracy'] metrics=[metrics.binary_accuracy]) 검증 데이터 준비 (Validation)훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해원본 훈련 데이터에서 10,000개의 샘플을 떼내어 검증 데이터 세트를 만듬 1234x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:] 모델 학습512의 샘플씩 미니배치를 만들어 20번의 에폭동안 훈련앞에서 떼어놓은 10,000개의 데이터를 이용해 손실과 정확도를 측정 1234567history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/20 15000/15000 [==============================] - 2s 139us/step - loss: 0.5883 - binary_accuracy: 0.7147 - val_loss: 0.5165 - val_binary_accuracy: 0.7686 Epoch 2/20 15000/15000 [==============================] - 1s 90us/step - loss: 0.4284 - binary_accuracy: 0.8771 - val_loss: 0.3974 - val_binary_accuracy: 0.8760 Epoch 3/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.3157 - binary_accuracy: 0.9189 - val_loss: 0.3314 - val_binary_accuracy: 0.8880 Epoch 4/20 15000/15000 [==============================] - 1s 86us/step - loss: 0.2407 - binary_accuracy: 0.9349 - val_loss: 0.2963 - val_binary_accuracy: 0.8893 Epoch 5/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1912 - binary_accuracy: 0.9473 - val_loss: 0.2806 - val_binary_accuracy: 0.8906 Epoch 6/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1587 - binary_accuracy: 0.9555 - val_loss: 0.2811 - val_binary_accuracy: 0.8880 Epoch 7/20 15000/15000 [==============================] - 1s 85us/step - loss: 0.1303 - binary_accuracy: 0.9654 - val_loss: 0.2927 - val_binary_accuracy: 0.8850 Epoch 8/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.1108 - binary_accuracy: 0.9707 - val_loss: 0.2976 - val_binary_accuracy: 0.8859 Epoch 9/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0930 - binary_accuracy: 0.9762 - val_loss: 0.3311 - val_binary_accuracy: 0.8771 Epoch 10/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0795 - binary_accuracy: 0.9802 - val_loss: 0.3353 - val_binary_accuracy: 0.8808 Epoch 11/20 15000/15000 [==============================] - 1s 87us/step - loss: 0.0652 - binary_accuracy: 0.9855 - val_loss: 0.3555 - val_binary_accuracy: 0.8786 Epoch 12/20 15000/15000 [==============================] - 1s 88us/step - loss: 0.0556 - binary_accuracy: 0.9881 - val_loss: 0.3749 - val_binary_accuracy: 0.8768 Epoch 13/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0442 - binary_accuracy: 0.9919 - val_loss: 0.4263 - val_binary_accuracy: 0.8694 Epoch 14/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0373 - binary_accuracy: 0.9934 - val_loss: 0.4213 - val_binary_accuracy: 0.8753 Epoch 15/20 15000/15000 [==============================] - 1s 90us/step - loss: 0.0305 - binary_accuracy: 0.9947 - val_loss: 0.4821 - val_binary_accuracy: 0.8657 Epoch 16/20 15000/15000 [==============================] - 1s 92us/step - loss: 0.0263 - binary_accuracy: 0.9955 - val_loss: 0.4871 - val_binary_accuracy: 0.8704 Epoch 17/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0178 - binary_accuracy: 0.9978 - val_loss: 0.5176 - val_binary_accuracy: 0.8693 Epoch 18/20 15000/15000 [==============================] - 1s 88us/step - loss: 0.0155 - binary_accuracy: 0.9982 - val_loss: 0.5890 - val_binary_accuracy: 0.8639 Epoch 19/20 15000/15000 [==============================] - 1s 91us/step - loss: 0.0136 - binary_accuracy: 0.9980 - val_loss: 0.5645 - val_binary_accuracy: 0.8679 Epoch 20/20 15000/15000 [==============================] - 1s 89us/step - loss: 0.0086 - binary_accuracy: 0.9990 - val_loss: 0.5974 - val_binary_accuracy: 0.8672 모델의 훈련 정보 그리기위에서 fit의 반환으로 받은 history는 각각의 훈련 데이터세트와 검증 데이터세트에 대한매 에폭마다의 손실율과 정확도를 가지고 있다.해당 지표를 matplot를 이용해 시각화 하도록 한다. 123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltdef show_graph(history): history_dict = history.history accuracy = history_dict['binary_accuracy'] val_accuracy = history_dict['val_binary_accuracy'] loss = history_dict['loss'] val_loss = history_dict['val_loss'] epochs = range(1, len(loss) + 1) plt.figure(figsize=(16, 1)) plt.subplot(121) plt.subplots_adjust(top=2) plt.plot(epochs, accuracy, 'ro', label='Training accuracy') plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy') plt.title('Trainging and validation accuracy and loss') plt.xlabel('Epochs') plt.ylabel('Accuracy and Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, -0.1)) plt.subplot(122) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)# plt.legend(bbox_to_anchor=(1, 0)) plt.show() 1show_graph(history) 훈련 데이터셋의 그래프를(점선) 먼저 확인해보면,각 에폭이 돌때마다 정확도가 오르고, 손실은 줄어드는 형태로 제대로 학습이 된것 처럼 보이나,검증 데이터셋(실선)을 보게 되면 그렇지 않다.각 에폭마다도 정확도는 오르지 않고, 손실이 늘어나는걸 확인할 수 있는데이런 경우 훈련 데이터셋에 과대적합(overfitting) 되었다고 한다.과대적합이 된 경우 모델이 새로운 데이터셋을 만났을 때 제대로 분류를 하지 못하게 된다. 모델 재학습하기아까와 동일한 형태의 모델을 구성하고,학습과 관련된 하이퍼파라미터만 변경하여 과대적합을 피해보자 123456model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 16) 160016 _________________________________________________________________ dense_4 (Dense) (None, 16) 272 _________________________________________________________________ dense_5 (Dense) (None, 1) 17 ================================================================= Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 _________________________________________________________________ 12345model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 111us/step - loss: 0.5347 - binary_accuracy: 0.7908 - val_loss: 0.4127 - val_binary_accuracy: 0.8663 Epoch 2/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.3256 - binary_accuracy: 0.9005 - val_loss: 0.3316 - val_binary_accuracy: 0.8697 Epoch 3/8 15000/15000 [==============================] - 1s 88us/step - loss: 0.2393 - binary_accuracy: 0.9232 - val_loss: 0.2822 - val_binary_accuracy: 0.8906 Epoch 4/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.1881 - binary_accuracy: 0.9410 - val_loss: 0.2801 - val_binary_accuracy: 0.8875 Epoch 5/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.1525 - binary_accuracy: 0.9524 - val_loss: 0.2770 - val_binary_accuracy: 0.8885 Epoch 6/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.1263 - binary_accuracy: 0.9611 - val_loss: 0.2856 - val_binary_accuracy: 0.8880 Epoch 7/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.1035 - binary_accuracy: 0.9701 - val_loss: 0.3127 - val_binary_accuracy: 0.8846 Epoch 8/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.0863 - binary_accuracy: 0.9754 - val_loss: 0.3270 - val_binary_accuracy: 0.8835 1show_graph(history) 아까보단 과대적합을 피한 부분이 보인다.손실 그래프를 확인했을 때 2에폭과 3에폭이 훈련 세트와 검증 세트가 가장 근접한 손실을 갖고있는결 확인할 수 있고,정확도 또한 2,3에폭이 가장 근접한걸 확인할 수 있다.즉, 이 모델의 경우 2에폭 혹은 3에폭을 돌렸을 때 과대적합을 가장 피할 수 있는 학습상태가 된다는걸 확인할 수 있다.이렇게 학습에 파라미터를 조작하는 것 이외에도 과대적합을 피하는 기법이 많이 존재한다. 모델의 평가모델의 정확도를 측정한다. 12loss, accuracy = model.evaluate(x_test, y_test)print('accuracy : &#123;acc&#125;, loss : &#123;loss&#125;'.format(acc=accuracy, loss=loss)) 25000/25000 [==============================] - 2s 67us/step accuracy : 0.86852, loss : 0.35010315059185027 모델의 예측긍정이거나 부정일 확률 (높으면 긍정, 낮으면 부정) 1model.predict(x_test[:10]) array([[0.2251153 ], [0.9999784 ], [0.98094064], [0.94734573], [0.97099954], [0.9737046 ], [0.9995834 ], [0.01185756], [0.9645392 ], [0.99970514]], dtype=float32) 번외. 레이어 변경하여 정확도 개선해보기레이어를 한개 더 추가하여 테스트 (Deep)12345678910111213model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 16) 160016 _________________________________________________________________ dense_7 (Dense) (None, 16) 272 _________________________________________________________________ dense_8 (Dense) (None, 16) 272 _________________________________________________________________ dense_9 (Dense) (None, 1) 17 ================================================================= Total params: 160,577 Trainable params: 160,577 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 113us/step - loss: 0.5264 - binary_accuracy: 0.7816 - val_loss: 0.4348 - val_binary_accuracy: 0.8122 Epoch 2/8 15000/15000 [==============================] - 1s 87us/step - loss: 0.2989 - binary_accuracy: 0.9006 - val_loss: 0.2936 - val_binary_accuracy: 0.8870 Epoch 3/8 15000/15000 [==============================] - 1s 85us/step - loss: 0.2103 - binary_accuracy: 0.9263 - val_loss: 0.2941 - val_binary_accuracy: 0.8812 Epoch 4/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.1550 - binary_accuracy: 0.9481 - val_loss: 0.2963 - val_binary_accuracy: 0.8817 Epoch 5/8 15000/15000 [==============================] - 1s 89us/step - loss: 0.1294 - binary_accuracy: 0.9543 - val_loss: 0.2956 - val_binary_accuracy: 0.8850 Epoch 6/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.0964 - binary_accuracy: 0.9709 - val_loss: 0.3357 - val_binary_accuracy: 0.8745 Epoch 7/8 15000/15000 [==============================] - 1s 86us/step - loss: 0.0814 - binary_accuracy: 0.9739 - val_loss: 0.3678 - val_binary_accuracy: 0.8714 Epoch 8/8 15000/15000 [==============================] - 1s 84us/step - loss: 0.0598 - binary_accuracy: 0.9834 - val_loss: 0.3910 - val_binary_accuracy: 0.8714 1show_graph(history) 유닛을 추가하여 테스트 (Wide)123456789101112model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 64) 640064 _________________________________________________________________ dense_11 (Dense) (None, 64) 4160 _________________________________________________________________ dense_12 (Dense) (None, 1) 65 ================================================================= Total params: 644,289 Trainable params: 644,289 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 135us/step - loss: 0.4850 - binary_accuracy: 0.7656 - val_loss: 0.3620 - val_binary_accuracy: 0.8517 Epoch 2/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.2538 - binary_accuracy: 0.9058 - val_loss: 0.2754 - val_binary_accuracy: 0.8902 Epoch 3/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.1857 - binary_accuracy: 0.9340 - val_loss: 0.2826 - val_binary_accuracy: 0.8874 Epoch 4/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.1417 - binary_accuracy: 0.9499 - val_loss: 0.3328 - val_binary_accuracy: 0.8734 Epoch 5/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.1128 - binary_accuracy: 0.9601 - val_loss: 0.3275 - val_binary_accuracy: 0.8826 Epoch 6/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.0795 - binary_accuracy: 0.9743 - val_loss: 0.3473 - val_binary_accuracy: 0.8802 Epoch 7/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.0544 - binary_accuracy: 0.9832 - val_loss: 0.3840 - val_binary_accuracy: 0.8780 Epoch 8/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0514 - binary_accuracy: 0.9849 - val_loss: 0.4150 - val_binary_accuracy: 0.8789 1show_graph(history) 깊고 넓게 구성하기 (Deep and wide network)1234567891011121314model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_13 (Dense) (None, 64) 640064 _________________________________________________________________ dense_14 (Dense) (None, 64) 4160 _________________________________________________________________ dense_15 (Dense) (None, 32) 2080 _________________________________________________________________ dense_16 (Dense) (None, 32) 1056 _________________________________________________________________ dense_17 (Dense) (None, 1) 33 ================================================================= Total params: 647,393 Trainable params: 647,393 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 142us/step - loss: 0.5094 - binary_accuracy: 0.7512 - val_loss: 0.3875 - val_binary_accuracy: 0.8442 Epoch 2/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.2693 - binary_accuracy: 0.8983 - val_loss: 0.4223 - val_binary_accuracy: 0.8310 Epoch 3/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.1949 - binary_accuracy: 0.9275 - val_loss: 0.5629 - val_binary_accuracy: 0.7950 Epoch 4/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.1534 - binary_accuracy: 0.9434 - val_loss: 0.2965 - val_binary_accuracy: 0.8854 Epoch 5/8 15000/15000 [==============================] - 2s 105us/step - loss: 0.1106 - binary_accuracy: 0.9613 - val_loss: 0.3647 - val_binary_accuracy: 0.8718 Epoch 6/8 15000/15000 [==============================] - 2s 104us/step - loss: 0.0797 - binary_accuracy: 0.9733 - val_loss: 0.4042 - val_binary_accuracy: 0.8748 Epoch 7/8 15000/15000 [==============================] - 2s 110us/step - loss: 0.0802 - binary_accuracy: 0.9775 - val_loss: 0.4029 - val_binary_accuracy: 0.8815 Epoch 8/8 15000/15000 [==============================] - 2s 106us/step - loss: 0.0656 - binary_accuracy: 0.9827 - val_loss: 0.4207 - val_binary_accuracy: 0.8809 1show_graph(history) 손실함수 변경1234567891011121314model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000, )))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(32, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.summary()model.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=losses.MSE, metrics=[metrics.binary_accuracy]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 64) 640064 _________________________________________________________________ dense_19 (Dense) (None, 64) 4160 _________________________________________________________________ dense_20 (Dense) (None, 32) 2080 _________________________________________________________________ dense_21 (Dense) (None, 32) 1056 _________________________________________________________________ dense_22 (Dense) (None, 1) 33 ================================================================= Total params: 647,393 Trainable params: 647,393 Non-trainable params: 0 _________________________________________________________________ 1234567history = model.fit( partial_x_train, partial_y_train, epochs=8, batch_size=512, validation_data=(x_val, y_val),) Train on 15000 samples, validate on 10000 samples Epoch 1/8 15000/15000 [==============================] - 2s 138us/step - loss: 0.1701 - binary_accuracy: 0.7462 - val_loss: 0.1152 - val_binary_accuracy: 0.8508 Epoch 2/8 15000/15000 [==============================] - 2s 103us/step - loss: 0.0794 - binary_accuracy: 0.8991 - val_loss: 0.0829 - val_binary_accuracy: 0.8917 Epoch 3/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0580 - binary_accuracy: 0.9281 - val_loss: 0.0892 - val_binary_accuracy: 0.8833 Epoch 4/8 15000/15000 [==============================] - 2s 111us/step - loss: 0.0375 - binary_accuracy: 0.9544 - val_loss: 0.0858 - val_binary_accuracy: 0.8876 Epoch 5/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0359 - binary_accuracy: 0.9560 - val_loss: 0.0874 - val_binary_accuracy: 0.8863 Epoch 6/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0216 - binary_accuracy: 0.9751 - val_loss: 0.0927 - val_binary_accuracy: 0.8822 Epoch 7/8 15000/15000 [==============================] - 2s 109us/step - loss: 0.0189 - binary_accuracy: 0.9782 - val_loss: 0.0946 - val_binary_accuracy: 0.8812 Epoch 8/8 15000/15000 [==============================] - 2s 107us/step - loss: 0.0068 - binary_accuracy: 0.9931 - val_loss: 0.1091 - val_binary_accuracy: 0.8678 1show_graph(history)","categories":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/categories/ml/"}],"tags":[{"name":"ML","slug":"ml","permalink":"https://codingcrews.github.io/tags/ml/"},{"name":"DNN","slug":"dnn","permalink":"https://codingcrews.github.io/tags/dnn/"},{"name":"Keras","slug":"keras","permalink":"https://codingcrews.github.io/tags/keras/"}]}]}